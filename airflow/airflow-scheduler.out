[[34m2024-05-21T23:28:10.346+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-05-21T23:28:10.348+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2024-05-21T23:28:15.421+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-05-21T23:28:15.421+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-05-21T23:28:15.429+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 73600[0m
[[34m2024-05-21T23:28:15.431+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-21T23:28:15.435+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[[34m2024-05-21T23:28:15.461+0000[0m] {[34mscheduler_job_runner.py:[0m1618} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2024-05-21T23:28:15.491+0000[0m] {[34mscheduler_job_runner.py:[0m1654} INFO[0m - Reset the following 2 orphaned TaskInstances:
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-10T00:00:00+00:00 [queued]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-09T00:00:00+00:00 [queued]>[0m
[[34m2024-05-21T23:28:15.623+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-13 00:00:00+00:00, run_after=2023-06-14 00:00:00+00:00[0m
[[34m2024-05-21T23:28:15.850+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-05 00:00:00+00:00: scheduled__2023-06-05T00:00:00+00:00, state:running, queued_at: 2024-05-21 22:25:44.984822+00:00. externally triggered: False> failed[0m
[[34m2024-05-21T23:28:15.852+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-05 00:00:00+00:00, run_id=scheduled__2023-06-05T00:00:00+00:00, run_start_date=2024-05-21 22:25:45.015970+00:00, run_end_date=2024-05-21 23:28:15.852520+00:00, run_duration=3750.83655, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-05 00:00:00+00:00, data_interval_end=2023-06-06 00:00:00+00:00, dag_hash=f18f41c57766075893c036d9a60ba4a1[0m
[[34m2024-05-21T23:28:15.859+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-06 00:00:00+00:00, run_after=2023-06-07 00:00:00+00:00[0m
[[34m2024-05-21T23:28:15.893+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 5 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.extract_311_data manual__2024-05-21T23:22:14.737669+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-09T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-21T23:28:15.895+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-21T23:28:15.895+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-21T23:28:15.896+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-21T23:28:15.896+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 3/16 running and queued tasks[0m
[[34m2024-05-21T23:28:15.896+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 4/16 running and queued tasks[0m
[[34m2024-05-21T23:28:15.900+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.extract_311_data manual__2024-05-21T23:22:14.737669+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-09T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-21T23:28:15.910+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-21T23:28:15.912+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:28:15.915+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='manual__2024-05-21T23:22:14.737669+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-21T23:28:15.915+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'manual__2024-05-21T23:22:14.737669+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:28:15.916+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-21T23:28:15.916+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:28:15.919+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-21T23:28:15.919+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:28:15.919+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-21T23:28:15.919+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:28:15.922+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:28:17.326+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:28:17.729+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:31:00.288+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'manual__2024-05-21T23:22:14.737669+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:31:01.850+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:31:02.305+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data manual__2024-05-21T23:22:14.737669+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:31:24.559+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:34:03.648+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:34:04.483+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:34:07.189+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:34:09.625+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:34:09.979+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:34:12.008+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:34:13.447+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:34:13.898+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:39:16.940+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:39:16.941+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='manual__2024-05-21T23:22:14.737669+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:39:16.941+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:39:16.941+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:39:16.941+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:39:16.954+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:34:04.609777+00:00, run_end_date=2024-05-21 23:34:06.518335+00:00, run_duration=1.908558, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=282, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-21 23:31:17.868468+00:00, queued_by_job_id=279, pid=74373[0m
[[34m2024-05-21T23:39:16.954+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:34:10.078764+00:00, run_end_date=2024-05-21 23:34:11.260836+00:00, run_duration=1.182072, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=283, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-21 23:31:17.868468+00:00, queued_by_job_id=279, pid=74395[0m
[[34m2024-05-21T23:39:16.955+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:28:17.837527+00:00, run_end_date=2024-05-21 23:30:59.377974+00:00, run_duration=161.540447, state=success, executor_state=success, try_number=1, max_tries=2, job_id=277, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-21 23:28:15.903157+00:00, queued_by_job_id=276, pid=73610[0m
[[34m2024-05-21T23:39:16.955+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=manual__2024-05-21T23:22:14.737669+00:00, map_index=-1, run_start_date=2024-05-21 23:31:02.423615+00:00, run_end_date=2024-05-21 23:31:23.441845+00:00, run_duration=21.01823, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=278, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-21 23:31:17.868468+00:00, queued_by_job_id=279, pid=74240[0m
[[34m2024-05-21T23:39:16.955+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:34:13.997244+00:00, run_end_date=2024-05-21 23:39:16.185043+00:00, run_duration=302.187799, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=284, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-21 23:31:17.868468+00:00, queued_by_job_id=279, pid=74415[0m
[[34m2024-05-21T23:39:17.009+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-21T23:39:17.012+0000[0m] {[34mscheduler_job_runner.py:[0m1618} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2024-05-21T23:39:17.016+0000[0m] {[34mscheduler_job_runner.py:[0m1654} INFO[0m - Reset the following 1 orphaned TaskInstances:
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-12T00:00:00+00:00 [queued]>[0m
[[34m2024-05-21T23:39:17.035+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-15 00:00:00+00:00, run_after=2023-06-16 00:00:00+00:00[0m
[[34m2024-05-21T23:39:17.156+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-07 00:00:00+00:00: scheduled__2023-06-07T00:00:00+00:00, state:running, queued_at: 2024-05-21 22:33:16.090531+00:00. externally triggered: False> failed[0m
[[34m2024-05-21T23:39:17.157+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-07 00:00:00+00:00, run_id=scheduled__2023-06-07T00:00:00+00:00, run_start_date=2024-05-21 22:33:16.103374+00:00, run_end_date=2024-05-21 23:39:17.157206+00:00, run_duration=3961.053832, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-07 00:00:00+00:00, data_interval_end=2023-06-08 00:00:00+00:00, dag_hash=1501b912344ab72d8f171e57f79ba68c[0m
[[34m2024-05-21T23:39:17.160+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-08 00:00:00+00:00, run_after=2023-06-09 00:00:00+00:00[0m
[[34m2024-05-21T23:39:17.173+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 6 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.extract_311_data manual__2024-05-21T23:22:14.737669+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-21T23:39:17.173+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-21T23:39:17.173+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-21T23:39:17.174+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-21T23:39:17.174+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 3/16 running and queued tasks[0m
[[34m2024-05-21T23:39:17.174+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 4/16 running and queued tasks[0m
[[34m2024-05-21T23:39:17.174+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 5/16 running and queued tasks[0m
[[34m2024-05-21T23:39:17.175+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.extract_311_data manual__2024-05-21T23:22:14.737669+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-21T23:39:17.179+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-21T23:39:17.179+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:39:17.179+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='manual__2024-05-21T23:22:14.737669+00:00', try_number=3, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-21T23:39:17.179+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'manual__2024-05-21T23:22:14.737669+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:39:17.180+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-21T23:39:17.180+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:39:17.180+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-21T23:39:17.180+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:39:17.181+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-21T23:39:17.181+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:39:17.181+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-21T23:39:17.181+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:39:17.185+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:39:18.621+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:39:19.122+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:43:06.468+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'manual__2024-05-21T23:22:14.737669+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:43:07.809+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:43:08.207+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data manual__2024-05-21T23:22:14.737669+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:44:04.295+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:44:05.827+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:44:06.349+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:44:07.974+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:44:09.159+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:44:09.594+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:44:11.465+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:44:13.305+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:44:13.676+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:44:15.606+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:44:17.136+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:44:17.575+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:44:19.760+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:44:19.760+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='manual__2024-05-21T23:22:14.737669+00:00', try_number=3, map_index=-1)[0m
[[34m2024-05-21T23:44:19.760+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-21T23:44:19.761+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-05-21T23:44:19.761+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:44:19.761+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:44:19.774+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:39:19.220636+00:00, run_end_date=2024-05-21 23:43:05.800551+00:00, run_duration=226.579915, state=success, executor_state=success, try_number=1, max_tries=2, job_id=285, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-21 23:39:17.176043+00:00, queued_by_job_id=276, pid=74629[0m
[[34m2024-05-21T23:44:19.774+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=manual__2024-05-21T23:22:14.737669+00:00, map_index=-1, run_start_date=2024-05-21 23:43:08.342465+00:00, run_end_date=2024-05-21 23:44:03.639621+00:00, run_duration=55.297156, state=success, executor_state=success, try_number=3, max_tries=2, job_id=286, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-21 23:39:17.176043+00:00, queued_by_job_id=276, pid=74783[0m
[[34m2024-05-21T23:44:19.774+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:44:06.457661+00:00, run_end_date=2024-05-21 23:44:07.378592+00:00, run_duration=0.920931, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=287, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-21 23:39:17.176043+00:00, queued_by_job_id=276, pid=74834[0m
[[34m2024-05-21T23:44:19.775+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:44:09.763949+00:00, run_end_date=2024-05-21 23:44:10.763374+00:00, run_duration=0.999425, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=288, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-21 23:39:17.176043+00:00, queued_by_job_id=276, pid=74854[0m
[[34m2024-05-21T23:44:19.775+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:44:13.778020+00:00, run_end_date=2024-05-21 23:44:14.953670+00:00, run_duration=1.17565, state=success, executor_state=success, try_number=1, max_tries=2, job_id=289, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-21 23:39:17.176043+00:00, queued_by_job_id=276, pid=74878[0m
[[34m2024-05-21T23:44:19.776+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:44:17.692944+00:00, run_end_date=2024-05-21 23:44:18.945948+00:00, run_duration=1.253004, state=success, executor_state=success, try_number=1, max_tries=2, job_id=290, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-21 23:39:17.176043+00:00, queued_by_job_id=276, pid=74902[0m
[[34m2024-05-21T23:44:19.811+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-21T23:44:19.831+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-16 00:00:00+00:00, run_after=2023-06-17 00:00:00+00:00[0m
[[34m2024-05-21T23:44:19.942+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-08 00:00:00+00:00: scheduled__2023-06-08T00:00:00+00:00, state:running, queued_at: 2024-05-21 22:35:29.040967+00:00. externally triggered: False> failed[0m
[[34m2024-05-21T23:44:19.942+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-08 00:00:00+00:00, run_id=scheduled__2023-06-08T00:00:00+00:00, run_start_date=2024-05-21 22:35:29.053539+00:00, run_end_date=2024-05-21 23:44:19.942632+00:00, run_duration=4130.889093, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-08 00:00:00+00:00, data_interval_end=2023-06-09 00:00:00+00:00, dag_hash=e0b7d46acb1a01adc0682b065d896f5c[0m
[[34m2024-05-21T23:44:19.946+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-09 00:00:00+00:00, run_after=2023-06-10 00:00:00+00:00[0m
[[34m2024-05-21T23:44:19.961+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 5 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-05-21T23:22:14.737669+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-21T23:44:19.962+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-21T23:44:19.962+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-21T23:44:19.962+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-21T23:44:19.962+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 3/16 running and queued tasks[0m
[[34m2024-05-21T23:44:19.963+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 4/16 running and queued tasks[0m
[[34m2024-05-21T23:44:19.963+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-05-21T23:22:14.737669+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-21T23:44:19.968+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-21T23:44:19.968+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:44:19.968+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-21T23:44:19.969+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:44:19.969+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='manual__2024-05-21T23:22:14.737669+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-21T23:44:19.970+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'manual__2024-05-21T23:22:14.737669+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:44:19.971+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-21T23:44:19.971+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:44:19.971+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-21T23:44:19.972+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:44:19.973+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:44:21.425+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:44:21.973+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:45:46.178+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:45:47.382+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:45:47.749+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:45:49.853+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'manual__2024-05-21T23:22:14.737669+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:45:51.363+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:45:51.823+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-05-21T23:22:14.737669+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:45:53.806+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:45:55.208+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:45:55.603+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:50:58.294+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:50:59.864+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:51:00.335+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:56:03.023+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:56:03.024+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:56:03.024+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='manual__2024-05-21T23:22:14.737669+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:56:03.024+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:56:03.024+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-21T23:56:03.031+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:45:47.848547+00:00, run_end_date=2024-05-21 23:45:48.959073+00:00, run_duration=1.110526, state=success, executor_state=success, try_number=1, max_tries=2, job_id=292, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-21 23:44:19.966187+00:00, queued_by_job_id=276, pid=74998[0m
[[34m2024-05-21T23:56:03.031+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=manual__2024-05-21T23:22:14.737669+00:00, map_index=-1, run_start_date=2024-05-21 23:45:51.942692+00:00, run_end_date=2024-05-21 23:45:53.152685+00:00, run_duration=1.209993, state=success, executor_state=success, try_number=1, max_tries=2, job_id=293, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-21 23:44:19.966187+00:00, queued_by_job_id=276, pid=75022[0m
[[34m2024-05-21T23:56:03.032+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:45:55.708383+00:00, run_end_date=2024-05-21 23:50:57.578301+00:00, run_duration=301.869918, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=294, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-21 23:44:19.966187+00:00, queued_by_job_id=276, pid=75046[0m
[[34m2024-05-21T23:56:03.032+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:44:22.111562+00:00, run_end_date=2024-05-21 23:45:45.597851+00:00, run_duration=83.486289, state=success, executor_state=success, try_number=1, max_tries=2, job_id=291, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-21 23:44:19.966187+00:00, queued_by_job_id=276, pid=74927[0m
[[34m2024-05-21T23:56:03.033+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:51:00.443926+00:00, run_end_date=2024-05-21 23:56:02.420099+00:00, run_duration=301.976173, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=295, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-21 23:44:19.966187+00:00, queued_by_job_id=276, pid=75245[0m
[[34m2024-05-21T23:56:03.065+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-21T23:56:03.084+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-17 00:00:00+00:00, run_after=2023-06-18 00:00:00+00:00[0m
[[34m2024-05-21T23:56:03.210+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 6 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available manual__2024-05-21T23:22:14.737669+00:00 [scheduled]>[0m
[[34m2024-05-21T23:56:03.211+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-21T23:56:03.211+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-21T23:56:03.212+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-21T23:56:03.212+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 3/16 running and queued tasks[0m
[[34m2024-05-21T23:56:03.212+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 4/16 running and queued tasks[0m
[[34m2024-05-21T23:56:03.213+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 5/16 running and queued tasks[0m
[[34m2024-05-21T23:56:03.214+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available manual__2024-05-21T23:22:14.737669+00:00 [scheduled]>[0m
[[34m2024-05-21T23:56:03.217+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-21T23:56:03.218+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:56:03.218+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-21T23:56:03.218+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:56:03.219+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-21T23:56:03.219+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:56:03.219+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-21T23:56:03.220+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:56:03.221+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-21T23:56:03.221+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:56:03.222+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='manual__2024-05-21T23:22:14.737669+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-21T23:56:03.222+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'manual__2024-05-21T23:22:14.737669+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:56:03.224+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:56:04.717+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:56:05.184+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:57:11.407+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:57:12.821+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:57:13.209+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:57:15.085+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:57:16.418+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:57:16.782+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:57:18.364+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:57:19.787+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:57:20.165+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-21T23:57:22.040+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-21T23:57:23.742+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-21T23:57:24.243+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:02:27.063+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'manual__2024-05-21T23:22:14.737669+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:02:28.495+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:02:28.944+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available manual__2024-05-21T23:22:14.737669+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:07:32.002+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:07:32.002+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-10T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-05-22T00:07:32.003+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-11T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-05-22T00:07:32.003+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:07:32.003+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:07:32.003+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='manual__2024-05-21T23:22:14.737669+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:07:32.034+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:57:24.363176+00:00, run_end_date=2024-05-22 00:02:26.362266+00:00, run_duration=301.99909, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=300, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-21 23:56:03.215473+00:00, queued_by_job_id=276, pid=75562[0m
[[34m2024-05-22T00:07:32.036+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=manual__2024-05-21T23:22:14.737669+00:00, map_index=-1, run_start_date=2024-05-22 00:02:29.067502+00:00, run_end_date=2024-05-22 00:07:31.373493+00:00, run_duration=302.305991, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=301, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-21 23:56:03.215473+00:00, queued_by_job_id=276, pid=75775[0m
[[34m2024-05-22T00:07:32.038+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:57:20.266046+00:00, run_end_date=2024-05-21 23:57:21.375026+00:00, run_duration=1.10898, state=success, executor_state=success, try_number=1, max_tries=2, job_id=299, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-21 23:56:03.215473+00:00, queued_by_job_id=276, pid=75538[0m
[[34m2024-05-22T00:07:32.041+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:56:05.309506+00:00, run_end_date=2024-05-21 23:57:10.709387+00:00, run_duration=65.399881, state=success, executor_state=success, try_number=1, max_tries=2, job_id=296, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-21 23:56:03.215473+00:00, queued_by_job_id=276, pid=75443[0m
[[34m2024-05-22T00:07:32.041+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:57:13.338378+00:00, run_end_date=2024-05-21 23:57:14.351290+00:00, run_duration=1.012912, state=failed, executor_state=success, try_number=3, max_tries=2, job_id=297, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-21 23:56:03.215473+00:00, queued_by_job_id=276, pid=75497[0m
[[34m2024-05-22T00:07:32.041+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-21 23:57:16.881194+00:00, run_end_date=2024-05-21 23:57:17.721589+00:00, run_duration=0.840395, state=failed, executor_state=success, try_number=3, max_tries=2, job_id=298, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-21 23:56:03.215473+00:00, queued_by_job_id=276, pid=75517[0m
[[34m2024-05-22T00:07:32.100+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T00:07:32.127+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-18 00:00:00+00:00, run_after=2023-06-19 00:00:00+00:00[0m
[[34m2024-05-22T00:07:32.270+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2024-05-21 22:47:29.496885+00:00: manual__2024-05-21T22:47:29.496885+00:00, state:running, queued_at: 2024-05-21 22:47:29.514385+00:00. externally triggered: True> failed[0m
[[34m2024-05-22T00:07:32.271+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2024-05-21 22:47:29.496885+00:00, run_id=manual__2024-05-21T22:47:29.496885+00:00, run_start_date=2024-05-21 22:48:57.270712+00:00, run_end_date=2024-05-22 00:07:32.271279+00:00, run_duration=4715.000567, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-05-20 00:00:00+00:00, data_interval_end=2024-05-21 00:00:00+00:00, dag_hash=9491e3250c99b7b3055325e1daca38c1[0m
[[34m2024-05-22T00:07:32.295+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:07:32.295+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T00:07:32.296+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T00:07:32.296+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T00:07:32.297+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:07:32.301+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T00:07:32.301+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:07:32.301+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T00:07:32.302+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:07:32.302+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T00:07:32.302+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:07:32.304+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:07:33.763+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:07:34.160+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:09:27.949+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:09:29.330+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:09:29.857+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:09:31.730+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:09:33.154+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:09:33.595+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:14:36.273+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:14:36.273+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:14:36.274+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:14:36.285+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:07:34.267396+00:00, run_end_date=2024-05-22 00:09:27.308223+00:00, run_duration=113.040827, state=success, executor_state=success, try_number=1, max_tries=2, job_id=302, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 00:07:32.298360+00:00, queued_by_job_id=276, pid=75975[0m
[[34m2024-05-22T00:14:36.285+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:09:29.961608+00:00, run_end_date=2024-05-22 00:09:31.137847+00:00, run_duration=1.176239, state=success, executor_state=success, try_number=1, max_tries=2, job_id=303, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 00:07:32.298360+00:00, queued_by_job_id=276, pid=76072[0m
[[34m2024-05-22T00:14:36.286+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:09:33.699689+00:00, run_end_date=2024-05-22 00:14:35.617397+00:00, run_duration=301.917708, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=304, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 00:07:32.298360+00:00, queued_by_job_id=276, pid=76096[0m
[[34m2024-05-22T00:14:36.321+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T00:14:36.341+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-19 00:00:00+00:00, run_after=2023-06-20 00:00:00+00:00[0m
[[34m2024-05-22T00:14:36.503+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-09 00:00:00+00:00: scheduled__2023-06-09T00:00:00+00:00, state:running, queued_at: 2024-05-21 22:42:41.925774+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T00:14:36.504+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-09 00:00:00+00:00, run_id=scheduled__2023-06-09T00:00:00+00:00, run_start_date=2024-05-21 22:42:41.940115+00:00, run_end_date=2024-05-22 00:14:36.504545+00:00, run_duration=5514.56443, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-09 00:00:00+00:00, data_interval_end=2023-06-10 00:00:00+00:00, dag_hash=d167f7133b2f36660b69fc11ba91e8fc[0m
[[34m2024-05-22T00:14:36.508+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-10 00:00:00+00:00, run_after=2023-06-11 00:00:00+00:00[0m
[[34m2024-05-22T00:14:36.518+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:14:36.519+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T00:14:36.519+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T00:14:36.520+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T00:14:36.520+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:14:36.523+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T00:14:36.523+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:14:36.524+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T00:14:36.524+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:14:36.525+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T00:14:36.525+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:14:36.526+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:14:38.103+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:14:38.579+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:17:33.811+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:17:35.062+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:17:35.418+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:17:37.358+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:17:38.745+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:17:39.120+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:22:42.192+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:22:42.193+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:22:42.193+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:22:42.199+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:14:38.698160+00:00, run_end_date=2024-05-22 00:17:33.169698+00:00, run_duration=174.471538, state=success, executor_state=success, try_number=1, max_tries=2, job_id=305, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 00:14:36.521850+00:00, queued_by_job_id=276, pid=76294[0m
[[34m2024-05-22T00:22:42.200+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:17:35.512396+00:00, run_end_date=2024-05-22 00:17:36.678380+00:00, run_duration=1.165984, state=success, executor_state=success, try_number=1, max_tries=2, job_id=306, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 00:14:36.521850+00:00, queued_by_job_id=276, pid=76421[0m
[[34m2024-05-22T00:22:42.200+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:17:39.254185+00:00, run_end_date=2024-05-22 00:22:41.438108+00:00, run_duration=302.183923, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=307, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 00:14:36.521850+00:00, queued_by_job_id=276, pid=76445[0m
[[34m2024-05-22T00:22:42.260+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T00:22:42.293+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-20 00:00:00+00:00, run_after=2023-06-21 00:00:00+00:00[0m
[[34m2024-05-22T00:22:42.465+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:22:42.466+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T00:22:42.466+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T00:22:42.466+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T00:22:42.466+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:22:42.469+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T00:22:42.470+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:22:42.470+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T00:22:42.471+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:22:42.471+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T00:22:42.471+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:22:42.475+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:22:43.746+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:22:44.120+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:26:09.739+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:26:10.987+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:26:11.389+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:26:13.666+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:26:14.982+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:26:15.427+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:31:18.240+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:31:18.240+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:31:18.241+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:31:18.247+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:26:11.504370+00:00, run_end_date=2024-05-22 00:26:12.848341+00:00, run_duration=1.343971, state=success, executor_state=success, try_number=1, max_tries=2, job_id=309, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 00:22:42.467976+00:00, queued_by_job_id=276, pid=76784[0m
[[34m2024-05-22T00:31:18.247+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:22:44.221471+00:00, run_end_date=2024-05-22 00:26:09.094012+00:00, run_duration=204.872541, state=success, executor_state=success, try_number=1, max_tries=2, job_id=308, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 00:22:42.467976+00:00, queued_by_job_id=276, pid=76643[0m
[[34m2024-05-22T00:31:18.248+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:26:15.534323+00:00, run_end_date=2024-05-22 00:31:17.541645+00:00, run_duration=302.007322, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=310, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 00:22:42.467976+00:00, queued_by_job_id=276, pid=76808[0m
[[34m2024-05-22T00:31:18.282+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T00:31:18.301+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-21 00:00:00+00:00, run_after=2023-06-22 00:00:00+00:00[0m
[[34m2024-05-22T00:31:18.399+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-13 00:00:00+00:00: scheduled__2023-06-13T00:00:00+00:00, state:running, queued_at: 2024-05-21 23:31:17.033242+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T00:31:18.400+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-13 00:00:00+00:00, run_id=scheduled__2023-06-13T00:00:00+00:00, run_start_date=2024-05-21 23:31:17.163533+00:00, run_end_date=2024-05-22 00:31:18.399845+00:00, run_duration=3601.236312, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-13 00:00:00+00:00, data_interval_end=2023-06-14 00:00:00+00:00, dag_hash=65ed8ffc156b3c5adc542f2401032a62[0m
[[34m2024-05-22T00:31:18.403+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-14 00:00:00+00:00, run_after=2023-06-15 00:00:00+00:00[0m
[[34m2024-05-22T00:31:18.410+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-12 00:00:00+00:00: scheduled__2023-06-12T00:00:00+00:00, state:running, queued_at: 2024-05-21 23:28:15.566617+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T00:31:18.411+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-12 00:00:00+00:00, run_id=scheduled__2023-06-12T00:00:00+00:00, run_start_date=2024-05-21 23:28:15.648173+00:00, run_end_date=2024-05-22 00:31:18.411406+00:00, run_duration=3782.763233, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-12 00:00:00+00:00, data_interval_end=2023-06-13 00:00:00+00:00, dag_hash=65ed8ffc156b3c5adc542f2401032a62[0m
[[34m2024-05-22T00:31:18.415+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-13 00:00:00+00:00, run_after=2023-06-14 00:00:00+00:00[0m
[[34m2024-05-22T00:31:18.446+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-18T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:31:18.446+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T00:31:18.446+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T00:31:18.446+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T00:31:18.447+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-18T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:31:18.450+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T00:31:18.451+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:31:18.451+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T00:31:18.452+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:31:18.452+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T00:31:18.453+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:31:18.454+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:31:19.886+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:31:20.301+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:35:23.719+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:35:25.338+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:35:25.786+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:35:27.859+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:35:29.402+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:35:30.009+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:40:32.847+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:40:32.848+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:40:32.848+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:40:32.871+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:35:30.164146+00:00, run_end_date=2024-05-22 00:40:32.202883+00:00, run_duration=302.038737, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=313, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 00:31:18.448929+00:00, queued_by_job_id=276, pid=77195[0m
[[34m2024-05-22T00:40:32.871+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:31:20.449165+00:00, run_end_date=2024-05-22 00:35:22.951493+00:00, run_duration=242.502328, state=success, executor_state=success, try_number=1, max_tries=2, job_id=311, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 00:31:18.448929+00:00, queued_by_job_id=276, pid=77008[0m
[[34m2024-05-22T00:40:32.872+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:35:25.897628+00:00, run_end_date=2024-05-22 00:35:27.118184+00:00, run_duration=1.220556, state=success, executor_state=success, try_number=1, max_tries=2, job_id=312, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 00:31:18.448929+00:00, queued_by_job_id=276, pid=77171[0m
[[34m2024-05-22T00:40:32.915+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T00:40:32.936+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-22 00:00:00+00:00, run_after=2023-06-23 00:00:00+00:00[0m
[[34m2024-05-22T00:40:33.030+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-14 00:00:00+00:00: scheduled__2023-06-14T00:00:00+00:00, state:running, queued_at: 2024-05-21 23:39:17.029635+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T00:40:33.031+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-14 00:00:00+00:00, run_id=scheduled__2023-06-14T00:00:00+00:00, run_start_date=2024-05-21 23:39:17.042607+00:00, run_end_date=2024-05-22 00:40:33.030955+00:00, run_duration=3675.988348, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-14 00:00:00+00:00, data_interval_end=2023-06-15 00:00:00+00:00, dag_hash=1cd3bf9f538d34c8311400c12537449f[0m
[[34m2024-05-22T00:40:33.036+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-15 00:00:00+00:00, run_after=2023-06-16 00:00:00+00:00[0m
[[34m2024-05-22T00:40:33.044+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2024-05-21 23:22:14.737669+00:00: manual__2024-05-21T23:22:14.737669+00:00, state:running, queued_at: 2024-05-21 23:22:14.746948+00:00. externally triggered: True> failed[0m
[[34m2024-05-22T00:40:33.044+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2024-05-21 23:22:14.737669+00:00, run_id=manual__2024-05-21T23:22:14.737669+00:00, run_start_date=2024-05-21 23:28:15.651415+00:00, run_end_date=2024-05-22 00:40:33.044613+00:00, run_duration=4337.393198, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-05-20 00:00:00+00:00, data_interval_end=2024-05-21 00:00:00+00:00, dag_hash=1cd3bf9f538d34c8311400c12537449f[0m
[[34m2024-05-22T00:40:33.080+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-19T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:40:33.080+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T00:40:33.081+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T00:40:33.081+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T00:40:33.081+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-19T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:40:33.085+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T00:40:33.085+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:40:33.085+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T00:40:33.086+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:40:33.086+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T00:40:33.086+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:40:33.088+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:40:34.615+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:40:35.055+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:42:26.937+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:42:28.316+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:42:28.792+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:42:30.961+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:42:32.635+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:42:33.122+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:47:36.106+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:47:36.106+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:47:36.107+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:47:36.115+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:40:35.155502+00:00, run_end_date=2024-05-22 00:42:26.257881+00:00, run_duration=111.102379, state=success, executor_state=success, try_number=1, max_tries=2, job_id=314, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 00:40:33.082877+00:00, queued_by_job_id=276, pid=77394[0m
[[34m2024-05-22T00:47:36.116+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:42:28.930832+00:00, run_end_date=2024-05-22 00:42:30.351264+00:00, run_duration=1.420432, state=success, executor_state=success, try_number=1, max_tries=2, job_id=315, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 00:40:33.082877+00:00, queued_by_job_id=276, pid=77477[0m
[[34m2024-05-22T00:47:36.116+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:42:33.238510+00:00, run_end_date=2024-05-22 00:47:35.212767+00:00, run_duration=301.974257, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=316, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 00:40:33.082877+00:00, queued_by_job_id=276, pid=77501[0m
[[34m2024-05-22T00:47:36.152+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T00:47:36.172+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-23 00:00:00+00:00, run_after=2023-06-24 00:00:00+00:00[0m
[[34m2024-05-22T00:47:36.322+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-15 00:00:00+00:00: scheduled__2023-06-15T00:00:00+00:00, state:running, queued_at: 2024-05-21 23:44:19.825325+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T00:47:36.323+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-15 00:00:00+00:00, run_id=scheduled__2023-06-15T00:00:00+00:00, run_start_date=2024-05-21 23:44:19.838287+00:00, run_end_date=2024-05-22 00:47:36.322932+00:00, run_duration=3796.484645, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-15 00:00:00+00:00, data_interval_end=2023-06-16 00:00:00+00:00, dag_hash=70641aef7c8b16c79a5de5de2cc7e8ff[0m
[[34m2024-05-22T00:47:36.328+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-16 00:00:00+00:00, run_after=2023-06-17 00:00:00+00:00[0m
[[34m2024-05-22T00:47:36.336+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-11 00:00:00+00:00: scheduled__2023-06-11T00:00:00+00:00, state:running, queued_at: 2024-05-21 23:06:24.659248+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T00:47:36.337+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-11 00:00:00+00:00, run_id=scheduled__2023-06-11T00:00:00+00:00, run_start_date=2024-05-21 23:06:24.672081+00:00, run_end_date=2024-05-22 00:47:36.337299+00:00, run_duration=6071.665218, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-11 00:00:00+00:00, data_interval_end=2023-06-12 00:00:00+00:00, dag_hash=70641aef7c8b16c79a5de5de2cc7e8ff[0m
[[34m2024-05-22T00:47:36.341+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-12 00:00:00+00:00, run_after=2023-06-13 00:00:00+00:00[0m
[[34m2024-05-22T00:47:36.350+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-10 00:00:00+00:00: scheduled__2023-06-10T00:00:00+00:00, state:running, queued_at: 2024-05-21 22:48:57.256460+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T00:47:36.350+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-10 00:00:00+00:00, run_id=scheduled__2023-06-10T00:00:00+00:00, run_start_date=2024-05-21 22:48:57.270552+00:00, run_end_date=2024-05-22 00:47:36.350695+00:00, run_duration=7119.080143, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-10 00:00:00+00:00, data_interval_end=2023-06-11 00:00:00+00:00, dag_hash=70641aef7c8b16c79a5de5de2cc7e8ff[0m
[[34m2024-05-22T00:47:36.354+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-11 00:00:00+00:00, run_after=2023-06-12 00:00:00+00:00[0m
[[34m2024-05-22T00:47:36.367+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-20T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:47:36.368+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T00:47:36.368+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T00:47:36.368+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T00:47:36.369+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-20T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:47:36.372+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T00:47:36.372+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:47:36.373+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T00:47:36.374+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:47:36.380+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T00:47:36.382+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:47:36.388+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:47:37.905+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:47:38.421+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:52:06.621+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:52:08.004+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:52:08.406+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:52:10.678+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:52:12.220+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:52:12.674+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:57:15.550+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:57:15.551+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:57:15.551+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T00:57:15.557+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:52:12.823578+00:00, run_end_date=2024-05-22 00:57:14.912013+00:00, run_duration=302.088435, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=319, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 00:47:36.370441+00:00, queued_by_job_id=276, pid=78381[0m
[[34m2024-05-22T00:57:15.558+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:47:38.521000+00:00, run_end_date=2024-05-22 00:52:05.910164+00:00, run_duration=267.389164, state=success, executor_state=success, try_number=1, max_tries=2, job_id=317, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 00:47:36.370441+00:00, queued_by_job_id=276, pid=78163[0m
[[34m2024-05-22T00:57:15.558+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:52:08.522332+00:00, run_end_date=2024-05-22 00:52:10.043374+00:00, run_duration=1.521042, state=success, executor_state=success, try_number=1, max_tries=2, job_id=318, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 00:47:36.370441+00:00, queued_by_job_id=276, pid=78357[0m
[[34m2024-05-22T00:57:15.592+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T00:57:15.610+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-24 00:00:00+00:00, run_after=2023-06-25 00:00:00+00:00[0m
[[34m2024-05-22T00:57:15.703+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-16 00:00:00+00:00: scheduled__2023-06-16T00:00:00+00:00, state:running, queued_at: 2024-05-21 23:56:03.078406+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T00:57:15.703+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-16 00:00:00+00:00, run_id=scheduled__2023-06-16T00:00:00+00:00, run_start_date=2024-05-21 23:56:03.091651+00:00, run_end_date=2024-05-22 00:57:15.703594+00:00, run_duration=3672.611943, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-16 00:00:00+00:00, data_interval_end=2023-06-17 00:00:00+00:00, dag_hash=18a457260827ca2a3098ea898564677e[0m
[[34m2024-05-22T00:57:15.707+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-17 00:00:00+00:00, run_after=2023-06-18 00:00:00+00:00[0m
[[34m2024-05-22T00:57:15.717+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:57:15.717+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T00:57:15.718+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T00:57:15.718+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T00:57:15.718+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T00:57:15.722+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T00:57:15.723+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:57:15.723+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T00:57:15.724+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:57:15.724+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T00:57:15.724+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:57:15.727+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:57:17.056+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:57:17.517+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:58:33.659+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:58:34.877+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:58:35.260+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T00:58:37.139+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T00:58:38.517+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T00:58:38.929+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:03:42.026+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:03:42.026+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:03:42.026+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:03:42.032+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:57:17.625729+00:00, run_end_date=2024-05-22 00:58:33.118428+00:00, run_duration=75.492699, state=success, executor_state=success, try_number=1, max_tries=2, job_id=320, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 00:57:15.721389+00:00, queued_by_job_id=276, pid=78583[0m
[[34m2024-05-22T01:03:42.032+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:58:35.359367+00:00, run_end_date=2024-05-22 00:58:36.503504+00:00, run_duration=1.144137, state=success, executor_state=success, try_number=1, max_tries=2, job_id=321, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 00:57:15.721389+00:00, queued_by_job_id=276, pid=78644[0m
[[34m2024-05-22T01:03:42.034+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 00:58:39.028948+00:00, run_end_date=2024-05-22 01:03:41.199771+00:00, run_duration=302.170823, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=322, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 00:57:15.721389+00:00, queued_by_job_id=276, pid=78672[0m
[[34m2024-05-22T01:03:42.070+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T01:03:42.092+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-25 00:00:00+00:00, run_after=2023-06-26 00:00:00+00:00[0m
[[34m2024-05-22T01:03:42.270+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-17 00:00:00+00:00: scheduled__2023-06-17T00:00:00+00:00, state:running, queued_at: 2024-05-22 00:07:32.118500+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T01:03:42.273+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-17 00:00:00+00:00, run_id=scheduled__2023-06-17T00:00:00+00:00, run_start_date=2024-05-22 00:07:32.136155+00:00, run_end_date=2024-05-22 01:03:42.273089+00:00, run_duration=3370.136934, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-17 00:00:00+00:00, data_interval_end=2023-06-18 00:00:00+00:00, dag_hash=0a820e509818fbe2822651155f04ba4f[0m
[[34m2024-05-22T01:03:42.285+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-18 00:00:00+00:00, run_after=2023-06-19 00:00:00+00:00[0m
[[34m2024-05-22T01:03:42.318+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-22T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:03:42.319+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T01:03:42.319+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T01:03:42.319+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T01:03:42.319+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-22T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:03:42.339+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T01:03:42.344+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:03:42.345+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T01:03:42.345+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:03:42.346+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T01:03:42.349+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:03:42.354+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:03:43.870+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:03:44.236+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:09:55.067+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:09:56.450+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:09:56.861+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:09:58.948+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:10:00.545+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:10:00.912+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:15:03.674+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:15:03.675+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:15:03.675+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:15:03.700+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:03:44.339196+00:00, run_end_date=2024-05-22 01:09:54.410902+00:00, run_duration=370.071706, state=success, executor_state=success, try_number=1, max_tries=2, job_id=323, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 01:03:42.327331+00:00, queued_by_job_id=276, pid=78888[0m
[[34m2024-05-22T01:15:03.701+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:09:56.973053+00:00, run_end_date=2024-05-22 01:09:58.350794+00:00, run_duration=1.377741, state=success, executor_state=success, try_number=1, max_tries=2, job_id=324, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 01:03:42.327331+00:00, queued_by_job_id=276, pid=79126[0m
[[34m2024-05-22T01:15:03.702+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:10:01.008949+00:00, run_end_date=2024-05-22 01:15:03.026673+00:00, run_duration=302.017724, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=325, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 01:03:42.327331+00:00, queued_by_job_id=276, pid=79150[0m
[[34m2024-05-22T01:15:03.794+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T01:15:03.848+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-26 00:00:00+00:00, run_after=2023-06-27 00:00:00+00:00[0m
[[34m2024-05-22T01:15:03.950+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-18 00:00:00+00:00: scheduled__2023-06-18T00:00:00+00:00, state:running, queued_at: 2024-05-22 00:14:36.334764+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T01:15:03.951+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-18 00:00:00+00:00, run_id=scheduled__2023-06-18T00:00:00+00:00, run_start_date=2024-05-22 00:14:36.349257+00:00, run_end_date=2024-05-22 01:15:03.951614+00:00, run_duration=3627.602357, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-18 00:00:00+00:00, data_interval_end=2023-06-19 00:00:00+00:00, dag_hash=cf514ca8e8017adcf71a6a6430de1487[0m
[[34m2024-05-22T01:15:03.955+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-19 00:00:00+00:00, run_after=2023-06-20 00:00:00+00:00[0m
[[34m2024-05-22T01:15:03.965+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-23T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:15:03.966+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T01:15:03.966+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T01:15:03.966+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T01:15:03.966+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-23T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:15:03.969+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T01:15:03.969+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:15:03.970+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T01:15:03.970+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:15:03.971+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T01:15:03.971+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:15:03.972+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:15:05.569+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:15:06.024+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:18:46.352+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:18:47.527+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:18:47.926+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:18:50.161+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:18:51.495+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:18:51.876+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:23:54.648+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:23:54.649+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:23:54.650+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:23:54.656+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:18:48.073769+00:00, run_end_date=2024-05-22 01:18:49.315865+00:00, run_duration=1.242096, state=success, executor_state=success, try_number=1, max_tries=2, job_id=327, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 01:15:03.967794+00:00, queued_by_job_id=276, pid=79501[0m
[[34m2024-05-22T01:23:54.658+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:18:51.982757+00:00, run_end_date=2024-05-22 01:23:53.886408+00:00, run_duration=301.903651, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=328, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 01:15:03.967794+00:00, queued_by_job_id=276, pid=79525[0m
[[34m2024-05-22T01:23:54.659+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:15:06.142070+00:00, run_end_date=2024-05-22 01:18:45.712346+00:00, run_duration=219.570276, state=success, executor_state=success, try_number=1, max_tries=2, job_id=326, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 01:15:03.967794+00:00, queued_by_job_id=276, pid=79353[0m
[[34m2024-05-22T01:23:54.699+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T01:23:54.726+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-27 00:00:00+00:00, run_after=2023-06-28 00:00:00+00:00[0m
[[34m2024-05-22T01:23:54.841+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-19 00:00:00+00:00: scheduled__2023-06-19T00:00:00+00:00, state:running, queued_at: 2024-05-22 00:22:42.282993+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T01:23:54.842+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-19 00:00:00+00:00, run_id=scheduled__2023-06-19T00:00:00+00:00, run_start_date=2024-05-22 00:22:42.313169+00:00, run_end_date=2024-05-22 01:23:54.842139+00:00, run_duration=3672.52897, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-19 00:00:00+00:00, data_interval_end=2023-06-20 00:00:00+00:00, dag_hash=1087a84ec0e0269ae1322c70a2f45bc1[0m
[[34m2024-05-22T01:23:54.848+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-20 00:00:00+00:00, run_after=2023-06-21 00:00:00+00:00[0m
[[34m2024-05-22T01:23:54.861+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-24T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:23:54.861+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T01:23:54.862+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T01:23:54.863+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T01:23:54.864+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-24T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:23:54.867+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T01:23:54.867+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:23:54.867+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T01:23:54.868+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:23:54.868+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T01:23:54.869+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:23:54.871+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:23:56.283+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:23:56.688+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:29:59.002+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:30:00.599+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:30:01.072+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:30:03.733+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:30:05.059+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:30:05.683+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:35:08.497+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:35:08.497+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:35:08.497+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:35:08.504+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:30:05.780961+00:00, run_end_date=2024-05-22 01:35:07.800895+00:00, run_duration=302.019934, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=331, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 01:23:54.865138+00:00, queued_by_job_id=276, pid=79988[0m
[[34m2024-05-22T01:35:08.504+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:23:56.825635+00:00, run_end_date=2024-05-22 01:29:58.388199+00:00, run_duration=361.562564, state=success, executor_state=success, try_number=1, max_tries=2, job_id=329, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 01:23:54.865138+00:00, queued_by_job_id=276, pid=79724[0m
[[34m2024-05-22T01:35:08.505+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:30:01.244615+00:00, run_end_date=2024-05-22 01:30:03.096467+00:00, run_duration=1.851852, state=success, executor_state=success, try_number=1, max_tries=2, job_id=330, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 01:23:54.865138+00:00, queued_by_job_id=276, pid=79958[0m
[[34m2024-05-22T01:35:08.580+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T01:35:08.602+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-28 00:00:00+00:00, run_after=2023-06-29 00:00:00+00:00[0m
[[34m2024-05-22T01:35:08.728+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-20 00:00:00+00:00: scheduled__2023-06-20T00:00:00+00:00, state:running, queued_at: 2024-05-22 00:31:18.295393+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T01:35:08.730+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-20 00:00:00+00:00, run_id=scheduled__2023-06-20T00:00:00+00:00, run_start_date=2024-05-22 00:31:18.308821+00:00, run_end_date=2024-05-22 01:35:08.730410+00:00, run_duration=3830.421589, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-20 00:00:00+00:00, data_interval_end=2023-06-21 00:00:00+00:00, dag_hash=9b2486e081017c7cb5d6bc36e9c47426[0m
[[34m2024-05-22T01:35:08.742+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-21 00:00:00+00:00, run_after=2023-06-22 00:00:00+00:00[0m
[[34m2024-05-22T01:35:08.753+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-25T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:35:08.753+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T01:35:08.754+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T01:35:08.754+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T01:35:08.755+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-25T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:35:08.757+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T01:35:08.758+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:35:08.758+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T01:35:08.758+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:35:08.760+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T01:35:08.760+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:35:08.762+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:35:10.581+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:35:11.039+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:36:31.968+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:36:33.427+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:36:33.917+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:36:36.205+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:36:37.796+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:36:38.334+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:41:41.316+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:41:41.316+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:41:41.316+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:41:41.322+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:35:11.140890+00:00, run_end_date=2024-05-22 01:36:31.218018+00:00, run_duration=80.077128, state=success, executor_state=success, try_number=1, max_tries=2, job_id=332, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 01:35:08.756169+00:00, queued_by_job_id=276, pid=80187[0m
[[34m2024-05-22T01:41:41.322+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:36:34.034745+00:00, run_end_date=2024-05-22 01:36:35.373871+00:00, run_duration=1.339126, state=success, executor_state=success, try_number=1, max_tries=2, job_id=333, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 01:35:08.756169+00:00, queued_by_job_id=276, pid=80249[0m
[[34m2024-05-22T01:41:41.323+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:36:38.459491+00:00, run_end_date=2024-05-22 01:41:40.699635+00:00, run_duration=302.240144, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=334, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 01:35:08.756169+00:00, queued_by_job_id=276, pid=80273[0m
[[34m2024-05-22T01:41:41.357+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T01:41:41.376+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-29 00:00:00+00:00, run_after=2023-06-30 00:00:00+00:00[0m
[[34m2024-05-22T01:41:41.471+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-21 00:00:00+00:00: scheduled__2023-06-21T00:00:00+00:00, state:running, queued_at: 2024-05-22 00:40:32.929429+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T01:41:41.472+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-21 00:00:00+00:00, run_id=scheduled__2023-06-21T00:00:00+00:00, run_start_date=2024-05-22 00:40:32.944595+00:00, run_end_date=2024-05-22 01:41:41.471932+00:00, run_duration=3668.527337, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-21 00:00:00+00:00, data_interval_end=2023-06-22 00:00:00+00:00, dag_hash=b13dcbe2fa824299184676d4de59e712[0m
[[34m2024-05-22T01:41:41.475+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-22 00:00:00+00:00, run_after=2023-06-23 00:00:00+00:00[0m
[[34m2024-05-22T01:41:41.486+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-26T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:41:41.486+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T01:41:41.486+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T01:41:41.487+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T01:41:41.487+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-26T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:41:41.490+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T01:41:41.491+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:41:41.491+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T01:41:41.491+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:41:41.492+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T01:41:41.492+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:41:41.493+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:41:42.936+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:41:43.391+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:45:16.661+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:45:18.039+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:45:18.535+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:45:20.954+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:45:22.379+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:45:22.773+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:50:25.495+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:50:25.495+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:50:25.495+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:50:25.519+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:45:18.843681+00:00, run_end_date=2024-05-22 01:45:20.291217+00:00, run_duration=1.447536, state=success, executor_state=success, try_number=1, max_tries=2, job_id=336, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 01:41:41.489008+00:00, queued_by_job_id=276, pid=80619[0m
[[34m2024-05-22T01:50:25.519+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:45:22.905475+00:00, run_end_date=2024-05-22 01:50:24.857760+00:00, run_duration=301.952285, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=337, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 01:41:41.489008+00:00, queued_by_job_id=276, pid=80644[0m
[[34m2024-05-22T01:50:25.519+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:41:43.486633+00:00, run_end_date=2024-05-22 01:45:16.008993+00:00, run_duration=212.52236, state=success, executor_state=success, try_number=1, max_tries=2, job_id=335, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 01:41:41.489008+00:00, queued_by_job_id=276, pid=80471[0m
[[34m2024-05-22T01:50:25.569+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T01:50:25.606+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-30 00:00:00+00:00, run_after=2023-07-01 00:00:00+00:00[0m
[[34m2024-05-22T01:50:25.723+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-22 00:00:00+00:00: scheduled__2023-06-22T00:00:00+00:00, state:running, queued_at: 2024-05-22 00:47:36.165986+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T01:50:25.724+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-22 00:00:00+00:00, run_id=scheduled__2023-06-22T00:00:00+00:00, run_start_date=2024-05-22 00:47:36.180153+00:00, run_end_date=2024-05-22 01:50:25.724242+00:00, run_duration=3769.544089, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-22 00:00:00+00:00, data_interval_end=2023-06-23 00:00:00+00:00, dag_hash=8af0637a10e21c0398df657daa831e74[0m
[[34m2024-05-22T01:50:25.728+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-23 00:00:00+00:00, run_after=2023-06-24 00:00:00+00:00[0m
[[34m2024-05-22T01:50:25.742+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-27T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:50:25.743+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T01:50:25.744+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T01:50:25.744+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T01:50:25.744+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-27T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:50:25.748+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T01:50:25.748+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:50:25.749+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T01:50:25.749+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:50:25.749+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T01:50:25.750+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:50:25.751+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:50:27.105+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:50:27.525+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:53:03.130+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:53:04.481+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:53:04.992+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:53:07.189+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:53:08.624+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:53:09.141+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T01:58:12.030+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:58:12.030+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:58:12.030+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T01:58:12.036+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:53:05.117964+00:00, run_end_date=2024-05-22 01:53:06.489516+00:00, run_duration=1.371552, state=success, executor_state=success, try_number=1, max_tries=2, job_id=339, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 01:50:25.745821+00:00, queued_by_job_id=276, pid=80950[0m
[[34m2024-05-22T01:58:12.037+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:53:09.254098+00:00, run_end_date=2024-05-22 01:58:11.349634+00:00, run_duration=302.095536, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=340, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 01:50:25.745821+00:00, queued_by_job_id=276, pid=80974[0m
[[34m2024-05-22T01:58:12.037+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:50:27.638165+00:00, run_end_date=2024-05-22 01:53:02.423488+00:00, run_duration=154.785323, state=success, executor_state=success, try_number=1, max_tries=2, job_id=338, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 01:50:25.745821+00:00, queued_by_job_id=276, pid=80843[0m
[[34m2024-05-22T01:58:12.072+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T01:58:12.095+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-01 00:00:00+00:00, run_after=2023-07-02 00:00:00+00:00[0m
[[34m2024-05-22T01:58:12.215+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-23 00:00:00+00:00: scheduled__2023-06-23T00:00:00+00:00, state:running, queued_at: 2024-05-22 00:57:15.605161+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T01:58:12.216+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-23 00:00:00+00:00, run_id=scheduled__2023-06-23T00:00:00+00:00, run_start_date=2024-05-22 00:57:15.619369+00:00, run_end_date=2024-05-22 01:58:12.216082+00:00, run_duration=3656.596713, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-23 00:00:00+00:00, data_interval_end=2023-06-24 00:00:00+00:00, dag_hash=0ada0bdc616154a87052409b5bf1d4a5[0m
[[34m2024-05-22T01:58:12.229+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-24 00:00:00+00:00, run_after=2023-06-25 00:00:00+00:00[0m
[[34m2024-05-22T01:58:12.251+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-28T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:58:12.252+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T01:58:12.252+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T01:58:12.252+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T01:58:12.252+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-28T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T01:58:12.256+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T01:58:12.257+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:58:12.257+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T01:58:12.258+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:58:12.258+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T01:58:12.259+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:58:12.260+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T01:58:13.698+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T01:58:14.120+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-06-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:07:26.344+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:07:27.821+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:07:28.254+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:07:30.452+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:07:31.761+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:07:32.338+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:12:35.374+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:12:35.375+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:12:35.375+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:12:35.385+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-06-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 01:58:14.226248+00:00, run_end_date=2024-05-22 02:07:25.698819+00:00, run_duration=551.472571, state=success, executor_state=success, try_number=1, max_tries=2, job_id=341, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 01:58:12.254176+00:00, queued_by_job_id=276, pid=81173[0m
[[34m2024-05-22T02:12:35.386+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:07:32.464566+00:00, run_end_date=2024-05-22 02:12:34.587008+00:00, run_duration=302.122442, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=343, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 01:58:12.254176+00:00, queued_by_job_id=276, pid=81544[0m
[[34m2024-05-22T02:12:35.387+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:07:28.371166+00:00, run_end_date=2024-05-22 02:07:29.681912+00:00, run_duration=1.310746, state=success, executor_state=success, try_number=1, max_tries=2, job_id=342, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 01:58:12.254176+00:00, queued_by_job_id=276, pid=81520[0m
[[34m2024-05-22T02:12:35.423+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T02:12:35.445+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-02 00:00:00+00:00, run_after=2023-07-03 00:00:00+00:00[0m
[[34m2024-05-22T02:12:35.700+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-24 00:00:00+00:00: scheduled__2023-06-24T00:00:00+00:00, state:running, queued_at: 2024-05-22 01:03:42.085625+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T02:12:35.701+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-24 00:00:00+00:00, run_id=scheduled__2023-06-24T00:00:00+00:00, run_start_date=2024-05-22 01:03:42.101028+00:00, run_end_date=2024-05-22 02:12:35.701407+00:00, run_duration=4133.600379, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-24 00:00:00+00:00, data_interval_end=2023-06-25 00:00:00+00:00, dag_hash=a99ba4423c03df060709f52380ff57e9[0m
[[34m2024-05-22T02:12:35.705+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-25 00:00:00+00:00, run_after=2023-06-26 00:00:00+00:00[0m
[[34m2024-05-22T02:12:35.717+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-29T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:12:35.718+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T02:12:35.718+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T02:12:35.718+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T02:12:35.719+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-29T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:12:35.723+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T02:12:35.723+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:12:35.723+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T02:12:35.724+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:12:35.724+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T02:12:35.724+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:12:35.726+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:12:37.416+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:12:37.936+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:13:48.281+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:13:49.966+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:13:50.533+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-06-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:13:52.711+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:13:54.302+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:13:54.754+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:18:57.594+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:18:57.595+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:18:57.595+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:18:57.612+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-06-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:13:50.646427+00:00, run_end_date=2024-05-22 02:13:52.027650+00:00, run_duration=1.381223, state=success, executor_state=success, try_number=1, max_tries=2, job_id=345, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 02:12:35.720968+00:00, queued_by_job_id=276, pid=81807[0m
[[34m2024-05-22T02:18:57.613+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:12:38.124399+00:00, run_end_date=2024-05-22 02:13:47.618394+00:00, run_duration=69.493995, state=success, executor_state=success, try_number=1, max_tries=2, job_id=344, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 02:12:35.720968+00:00, queued_by_job_id=276, pid=81744[0m
[[34m2024-05-22T02:18:57.614+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:13:54.872020+00:00, run_end_date=2024-05-22 02:18:56.969872+00:00, run_duration=302.097852, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=346, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 02:12:35.720968+00:00, queued_by_job_id=276, pid=81831[0m
[[34m2024-05-22T02:18:57.659+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T02:18:57.681+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-03 00:00:00+00:00, run_after=2023-07-04 00:00:00+00:00[0m
[[34m2024-05-22T02:18:57.772+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-25 00:00:00+00:00: scheduled__2023-06-25T00:00:00+00:00, state:running, queued_at: 2024-05-22 01:15:03.834820+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T02:18:57.776+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-25 00:00:00+00:00, run_id=scheduled__2023-06-25T00:00:00+00:00, run_start_date=2024-05-22 01:15:03.855892+00:00, run_end_date=2024-05-22 02:18:57.776765+00:00, run_duration=3833.920873, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-25 00:00:00+00:00, data_interval_end=2023-06-26 00:00:00+00:00, dag_hash=959949975712ff1cb059987193f31073[0m
[[34m2024-05-22T02:18:57.783+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-26 00:00:00+00:00, run_after=2023-06-27 00:00:00+00:00[0m
[[34m2024-05-22T02:18:57.794+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-30T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:18:57.795+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T02:18:57.796+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T02:18:57.797+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T02:18:57.798+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-30T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:18:57.803+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T02:18:57.803+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:18:57.804+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T02:18:57.805+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:18:57.806+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T02:18:57.807+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:18:57.809+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:18:59.208+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:18:59.602+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:22:06.388+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:22:07.838+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:22:08.262+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:22:10.672+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:22:12.035+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:22:12.448+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-06-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:27:15.625+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:27:15.626+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:27:15.626+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-06-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:27:15.648+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-06-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:22:12.556835+00:00, run_end_date=2024-05-22 02:27:14.833994+00:00, run_duration=302.277159, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=349, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 02:18:57.801324+00:00, queued_by_job_id=276, pid=82200[0m
[[34m2024-05-22T02:27:15.648+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:18:59.707349+00:00, run_end_date=2024-05-22 02:22:05.690558+00:00, run_duration=185.983209, state=success, executor_state=success, try_number=1, max_tries=2, job_id=347, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 02:18:57.801324+00:00, queued_by_job_id=276, pid=82046[0m
[[34m2024-05-22T02:27:15.648+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:22:08.381263+00:00, run_end_date=2024-05-22 02:22:09.818425+00:00, run_duration=1.437162, state=success, executor_state=success, try_number=1, max_tries=2, job_id=348, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 02:18:57.801324+00:00, queued_by_job_id=276, pid=82176[0m
[[34m2024-05-22T02:27:15.686+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T02:27:15.709+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-04 00:00:00+00:00, run_after=2023-07-05 00:00:00+00:00[0m
[[34m2024-05-22T02:27:15.831+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-26 00:00:00+00:00: scheduled__2023-06-26T00:00:00+00:00, state:running, queued_at: 2024-05-22 01:23:54.716858+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T02:27:15.831+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-26 00:00:00+00:00, run_id=scheduled__2023-06-26T00:00:00+00:00, run_start_date=2024-05-22 01:23:54.741991+00:00, run_end_date=2024-05-22 02:27:15.831785+00:00, run_duration=3801.089794, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-26 00:00:00+00:00, data_interval_end=2023-06-27 00:00:00+00:00, dag_hash=efdbc9a0d027d40ddb99d2b919e47338[0m
[[34m2024-05-22T02:27:15.835+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-27 00:00:00+00:00, run_after=2023-06-28 00:00:00+00:00[0m
[[34m2024-05-22T02:27:15.848+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:27:15.848+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T02:27:15.848+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T02:27:15.848+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T02:27:15.849+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:27:15.853+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T02:27:15.853+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:27:15.854+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T02:27:15.855+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:27:15.856+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T02:27:15.856+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:27:15.858+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:27:17.196+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:27:17.628+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:34:16.450+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:34:17.881+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:34:18.269+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:34:20.418+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:34:21.764+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:34:22.211+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:39:25.230+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:39:25.230+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:39:25.230+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:39:25.235+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:27:17.746088+00:00, run_end_date=2024-05-22 02:34:15.866370+00:00, run_duration=418.120282, state=success, executor_state=success, try_number=1, max_tries=2, job_id=350, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 02:27:15.850553+00:00, queued_by_job_id=276, pid=82402[0m
[[34m2024-05-22T02:39:25.236+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:34:18.372150+00:00, run_end_date=2024-05-22 02:34:19.591856+00:00, run_duration=1.219706, state=success, executor_state=success, try_number=1, max_tries=2, job_id=351, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 02:27:15.850553+00:00, queued_by_job_id=276, pid=82670[0m
[[34m2024-05-22T02:39:25.236+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:34:22.328135+00:00, run_end_date=2024-05-22 02:39:24.529210+00:00, run_duration=302.201075, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=352, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 02:27:15.850553+00:00, queued_by_job_id=276, pid=82694[0m
[[34m2024-05-22T02:39:25.269+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T02:39:25.288+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-05 00:00:00+00:00, run_after=2023-07-06 00:00:00+00:00[0m
[[34m2024-05-22T02:39:25.383+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-27 00:00:00+00:00: scheduled__2023-06-27T00:00:00+00:00, state:running, queued_at: 2024-05-22 01:35:08.595359+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T02:39:25.383+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-27 00:00:00+00:00, run_id=scheduled__2023-06-27T00:00:00+00:00, run_start_date=2024-05-22 01:35:08.612142+00:00, run_end_date=2024-05-22 02:39:25.383775+00:00, run_duration=3856.771633, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-27 00:00:00+00:00, data_interval_end=2023-06-28 00:00:00+00:00, dag_hash=b0f5f29125d495a57ffb7b379befb0aa[0m
[[34m2024-05-22T02:39:25.387+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-28 00:00:00+00:00, run_after=2023-06-29 00:00:00+00:00[0m
[[34m2024-05-22T02:39:25.397+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-02T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:39:25.397+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T02:39:25.398+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T02:39:25.398+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T02:39:25.398+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-02T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:39:25.400+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T02:39:25.400+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:39:25.401+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T02:39:25.401+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:39:25.401+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T02:39:25.401+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:39:25.402+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:39:26.742+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:39:27.194+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:41:57.028+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:41:58.591+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:41:59.070+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:42:01.198+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:42:02.846+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:42:03.256+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:47:06.033+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:47:06.033+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:47:06.033+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:47:06.039+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:41:59.171515+00:00, run_end_date=2024-05-22 02:42:00.479428+00:00, run_duration=1.307913, state=success, executor_state=success, try_number=1, max_tries=2, job_id=354, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 02:39:25.398976+00:00, queued_by_job_id=276, pid=83004[0m
[[34m2024-05-22T02:47:06.039+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:42:03.398625+00:00, run_end_date=2024-05-22 02:47:05.410098+00:00, run_duration=302.011473, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=355, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 02:39:25.398976+00:00, queued_by_job_id=276, pid=83029[0m
[[34m2024-05-22T02:47:06.040+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:39:27.320809+00:00, run_end_date=2024-05-22 02:41:56.371434+00:00, run_duration=149.050625, state=success, executor_state=success, try_number=1, max_tries=2, job_id=353, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 02:39:25.398976+00:00, queued_by_job_id=276, pid=82896[0m
[[34m2024-05-22T02:47:06.073+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T02:47:06.092+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-06 00:00:00+00:00, run_after=2023-07-07 00:00:00+00:00[0m
[[34m2024-05-22T02:47:06.185+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-28 00:00:00+00:00: scheduled__2023-06-28T00:00:00+00:00, state:running, queued_at: 2024-05-22 01:41:41.370200+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T02:47:06.185+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-28 00:00:00+00:00, run_id=scheduled__2023-06-28T00:00:00+00:00, run_start_date=2024-05-22 01:41:41.383932+00:00, run_end_date=2024-05-22 02:47:06.185693+00:00, run_duration=3924.801761, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-28 00:00:00+00:00, data_interval_end=2023-06-29 00:00:00+00:00, dag_hash=cc74faedfde0eb9c7d5d26792c8a3a34[0m
[[34m2024-05-22T02:47:06.189+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-29 00:00:00+00:00, run_after=2023-06-30 00:00:00+00:00[0m
[[34m2024-05-22T02:47:06.212+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:47:06.213+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T02:47:06.213+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T02:47:06.213+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T02:47:06.214+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:47:06.221+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T02:47:06.221+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:47:06.221+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T02:47:06.221+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:47:06.222+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T02:47:06.222+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:47:06.224+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:47:07.466+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:47:07.934+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:49:35.329+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:49:36.563+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:49:36.942+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:49:38.918+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:49:40.477+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:49:40.840+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:54:43.724+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:54:43.724+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:54:43.724+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T02:54:43.729+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:49:40.964913+00:00, run_end_date=2024-05-22 02:54:42.980983+00:00, run_duration=302.01607, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=358, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 02:47:06.215646+00:00, queued_by_job_id=276, pid=83356[0m
[[34m2024-05-22T02:54:43.730+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:47:08.069639+00:00, run_end_date=2024-05-22 02:49:34.553694+00:00, run_duration=146.484055, state=success, executor_state=success, try_number=1, max_tries=2, job_id=356, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 02:47:06.215646+00:00, queued_by_job_id=276, pid=83230[0m
[[34m2024-05-22T02:54:43.730+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:49:37.041679+00:00, run_end_date=2024-05-22 02:49:38.233315+00:00, run_duration=1.191636, state=success, executor_state=success, try_number=1, max_tries=2, job_id=357, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 02:47:06.215646+00:00, queued_by_job_id=276, pid=83332[0m
[[34m2024-05-22T02:54:43.767+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T02:54:43.787+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-07 00:00:00+00:00, run_after=2023-07-08 00:00:00+00:00[0m
[[34m2024-05-22T02:54:43.898+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-29 00:00:00+00:00: scheduled__2023-06-29T00:00:00+00:00, state:running, queued_at: 2024-05-22 01:50:25.599119+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T02:54:43.898+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-29 00:00:00+00:00, run_id=scheduled__2023-06-29T00:00:00+00:00, run_start_date=2024-05-22 01:50:25.614878+00:00, run_end_date=2024-05-22 02:54:43.898482+00:00, run_duration=3858.283604, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-29 00:00:00+00:00, data_interval_end=2023-06-30 00:00:00+00:00, dag_hash=b287d5680498f51f988b6cff2cec4481[0m
[[34m2024-05-22T02:54:43.901+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-06-30 00:00:00+00:00, run_after=2023-07-01 00:00:00+00:00[0m
[[34m2024-05-22T02:54:43.912+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:54:43.913+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T02:54:43.913+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T02:54:43.913+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T02:54:43.913+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T02:54:43.916+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T02:54:43.916+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:54:43.916+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T02:54:43.916+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:54:43.917+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T02:54:43.917+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:54:43.918+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:54:45.377+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:54:45.816+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:56:28.804+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:56:30.183+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:56:30.575+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T02:56:32.678+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T02:56:34.090+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T02:56:34.514+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:01:37.183+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:01:37.183+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:01:37.184+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:01:37.202+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:54:45.930819+00:00, run_end_date=2024-05-22 02:56:28.083441+00:00, run_duration=102.152622, state=success, executor_state=success, try_number=1, max_tries=2, job_id=359, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 02:54:43.914378+00:00, queued_by_job_id=276, pid=83561[0m
[[34m2024-05-22T03:01:37.203+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:56:30.677345+00:00, run_end_date=2024-05-22 02:56:32.044919+00:00, run_duration=1.367574, state=success, executor_state=success, try_number=1, max_tries=2, job_id=360, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 02:54:43.914378+00:00, queued_by_job_id=276, pid=83645[0m
[[34m2024-05-22T03:01:37.203+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 02:56:34.620993+00:00, run_end_date=2024-05-22 03:01:36.565031+00:00, run_duration=301.944038, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=361, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 02:54:43.914378+00:00, queued_by_job_id=276, pid=83669[0m
[[34m2024-05-22T03:01:37.236+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T03:01:37.256+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-08 00:00:00+00:00, run_after=2023-07-09 00:00:00+00:00[0m
[[34m2024-05-22T03:01:37.340+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-06-30 00:00:00+00:00: scheduled__2023-06-30T00:00:00+00:00, state:running, queued_at: 2024-05-22 01:58:12.088880+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T03:01:37.340+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-06-30 00:00:00+00:00, run_id=scheduled__2023-06-30T00:00:00+00:00, run_start_date=2024-05-22 01:58:12.102370+00:00, run_end_date=2024-05-22 03:01:37.340845+00:00, run_duration=3805.238475, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-06-30 00:00:00+00:00, data_interval_end=2023-07-01 00:00:00+00:00, dag_hash=1f80f759a42962a662ebeb5adf2b0b55[0m
[[34m2024-05-22T03:01:37.344+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-01 00:00:00+00:00, run_after=2023-07-02 00:00:00+00:00[0m
[[34m2024-05-22T03:01:37.354+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:01:37.354+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T03:01:37.354+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T03:01:37.355+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T03:01:37.355+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:01:37.357+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T03:01:37.357+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:01:37.357+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T03:01:37.358+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:01:37.358+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T03:01:37.358+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:01:37.360+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:01:38.579+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:01:38.924+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:06:21.375+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:06:22.757+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:06:23.146+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:06:25.061+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:06:26.322+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:06:26.701+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:11:29.446+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:11:29.446+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:11:29.447+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:11:29.452+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:01:39.019831+00:00, run_end_date=2024-05-22 03:06:20.696486+00:00, run_duration=281.676655, state=success, executor_state=success, try_number=1, max_tries=2, job_id=362, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 03:01:37.355904+00:00, queued_by_job_id=276, pid=83870[0m
[[34m2024-05-22T03:11:29.452+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:06:23.304838+00:00, run_end_date=2024-05-22 03:06:24.446475+00:00, run_duration=1.141637, state=success, executor_state=success, try_number=1, max_tries=2, job_id=363, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 03:01:37.355904+00:00, queued_by_job_id=276, pid=84055[0m
[[34m2024-05-22T03:11:29.452+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:06:26.811393+00:00, run_end_date=2024-05-22 03:11:28.849056+00:00, run_duration=302.037663, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=364, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 03:01:37.355904+00:00, queued_by_job_id=276, pid=84079[0m
[[34m2024-05-22T03:11:29.484+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T03:11:29.502+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-09 00:00:00+00:00, run_after=2023-07-10 00:00:00+00:00[0m
[[34m2024-05-22T03:11:29.598+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-01 00:00:00+00:00: scheduled__2023-07-01T00:00:00+00:00, state:running, queued_at: 2024-05-22 02:12:35.438276+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T03:11:29.598+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-01 00:00:00+00:00, run_id=scheduled__2023-07-01T00:00:00+00:00, run_start_date=2024-05-22 02:12:35.457705+00:00, run_end_date=2024-05-22 03:11:29.598762+00:00, run_duration=3534.141057, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-01 00:00:00+00:00, data_interval_end=2023-07-02 00:00:00+00:00, dag_hash=1945cfb5d5f8e3b543fa841c0f792646[0m
[[34m2024-05-22T03:11:29.602+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-02 00:00:00+00:00, run_after=2023-07-03 00:00:00+00:00[0m
[[34m2024-05-22T03:11:29.613+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-06T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:11:29.613+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T03:11:29.614+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T03:11:29.614+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T03:11:29.614+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-06T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:11:29.617+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T03:11:29.617+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:11:29.617+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T03:11:29.617+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:11:29.618+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T03:11:29.618+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:11:29.619+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:11:31.146+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:11:31.509+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:13:48.430+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:13:50.176+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:13:50.640+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:13:52.864+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:13:54.218+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:13:54.646+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:18:57.215+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:18:57.215+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:18:57.215+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:18:57.221+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:11:31.605878+00:00, run_end_date=2024-05-22 03:13:47.799823+00:00, run_duration=136.193945, state=success, executor_state=success, try_number=1, max_tries=2, job_id=365, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 03:11:29.615212+00:00, queued_by_job_id=276, pid=84276[0m
[[34m2024-05-22T03:18:57.221+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:13:50.784680+00:00, run_end_date=2024-05-22 03:13:52.168645+00:00, run_duration=1.383965, state=success, executor_state=success, try_number=1, max_tries=2, job_id=366, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 03:11:29.615212+00:00, queued_by_job_id=276, pid=84376[0m
[[34m2024-05-22T03:18:57.221+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:13:54.781346+00:00, run_end_date=2024-05-22 03:18:56.608949+00:00, run_duration=301.827603, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=367, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 03:11:29.615212+00:00, queued_by_job_id=276, pid=84400[0m
[[34m2024-05-22T03:18:57.254+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T03:18:57.272+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-10 00:00:00+00:00, run_after=2023-07-11 00:00:00+00:00[0m
[[34m2024-05-22T03:18:57.362+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-02 00:00:00+00:00: scheduled__2023-07-02T00:00:00+00:00, state:running, queued_at: 2024-05-22 02:18:57.675496+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T03:18:57.363+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-02 00:00:00+00:00, run_id=scheduled__2023-07-02T00:00:00+00:00, run_start_date=2024-05-22 02:18:57.689469+00:00, run_end_date=2024-05-22 03:18:57.363020+00:00, run_duration=3599.673551, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-02 00:00:00+00:00, data_interval_end=2023-07-03 00:00:00+00:00, dag_hash=980105246e2a241884d37e2a43978259[0m
[[34m2024-05-22T03:18:57.366+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-03 00:00:00+00:00, run_after=2023-07-04 00:00:00+00:00[0m
[[34m2024-05-22T03:18:57.375+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-07T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:18:57.375+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T03:18:57.376+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T03:18:57.376+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T03:18:57.376+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-07T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:18:57.378+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T03:18:57.378+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:18:57.379+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T03:18:57.379+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:18:57.379+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T03:18:57.379+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:18:57.381+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:18:58.705+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:18:59.067+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:22:20.873+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:22:22.350+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:22:22.804+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:22:24.820+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:22:26.148+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:22:26.587+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:27:29.154+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:27:29.154+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:27:29.154+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:27:29.159+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:22:22.899944+00:00, run_end_date=2024-05-22 03:22:24.231169+00:00, run_duration=1.331225, state=success, executor_state=success, try_number=1, max_tries=2, job_id=369, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 03:18:57.377049+00:00, queued_by_job_id=276, pid=84742[0m
[[34m2024-05-22T03:27:29.160+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:22:26.679432+00:00, run_end_date=2024-05-22 03:27:28.599761+00:00, run_duration=301.920329, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=370, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 03:18:57.377049+00:00, queued_by_job_id=276, pid=84767[0m
[[34m2024-05-22T03:27:29.160+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:18:59.171936+00:00, run_end_date=2024-05-22 03:22:20.198813+00:00, run_duration=201.026877, state=success, executor_state=success, try_number=1, max_tries=2, job_id=368, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 03:18:57.377049+00:00, queued_by_job_id=276, pid=84603[0m
[[34m2024-05-22T03:27:29.193+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T03:27:29.210+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-11 00:00:00+00:00, run_after=2023-07-12 00:00:00+00:00[0m
[[34m2024-05-22T03:27:29.294+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-03 00:00:00+00:00: scheduled__2023-07-03T00:00:00+00:00, state:running, queued_at: 2024-05-22 02:27:15.701955+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T03:27:29.294+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-03 00:00:00+00:00, run_id=scheduled__2023-07-03T00:00:00+00:00, run_start_date=2024-05-22 02:27:15.719030+00:00, run_end_date=2024-05-22 03:27:29.294354+00:00, run_duration=3613.575324, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-03 00:00:00+00:00, data_interval_end=2023-07-04 00:00:00+00:00, dag_hash=760d146bddeb4a2ebf81c8fb98f875df[0m
[[34m2024-05-22T03:27:29.297+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-04 00:00:00+00:00, run_after=2023-07-05 00:00:00+00:00[0m
[[34m2024-05-22T03:27:29.306+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-08T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:27:29.307+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T03:27:29.307+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T03:27:29.307+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T03:27:29.307+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-08T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:27:29.309+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T03:27:29.309+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:27:29.310+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T03:27:29.310+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:27:29.310+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T03:27:29.310+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:27:29.312+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:27:30.888+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:27:31.258+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:36:18.219+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:36:19.644+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:36:20.055+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:36:22.306+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:36:23.929+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:36:24.370+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:41:27.109+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:41:27.109+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:41:27.109+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:41:27.127+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:36:24.493933+00:00, run_end_date=2024-05-22 03:41:26.472696+00:00, run_duration=301.978763, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=373, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 03:27:29.308280+00:00, queued_by_job_id=276, pid=85327[0m
[[34m2024-05-22T03:41:27.128+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:27:31.361428+00:00, run_end_date=2024-05-22 03:36:17.523702+00:00, run_duration=526.162274, state=success, executor_state=success, try_number=1, max_tries=2, job_id=371, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 03:27:29.308280+00:00, queued_by_job_id=276, pid=84966[0m
[[34m2024-05-22T03:41:27.128+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:36:20.312050+00:00, run_end_date=2024-05-22 03:36:21.661755+00:00, run_duration=1.349705, state=success, executor_state=success, try_number=1, max_tries=2, job_id=372, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 03:27:29.308280+00:00, queued_by_job_id=276, pid=85298[0m
[[34m2024-05-22T03:41:27.163+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T03:41:27.182+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-12 00:00:00+00:00, run_after=2023-07-13 00:00:00+00:00[0m
[[34m2024-05-22T03:41:27.279+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-04 00:00:00+00:00: scheduled__2023-07-04T00:00:00+00:00, state:running, queued_at: 2024-05-22 02:39:25.282601+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T03:41:27.279+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-04 00:00:00+00:00, run_id=scheduled__2023-07-04T00:00:00+00:00, run_start_date=2024-05-22 02:39:25.294885+00:00, run_end_date=2024-05-22 03:41:27.279344+00:00, run_duration=3721.984459, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-04 00:00:00+00:00, data_interval_end=2023-07-05 00:00:00+00:00, dag_hash=7357a9f95c5fa9dcae40ed23406786e5[0m
[[34m2024-05-22T03:41:27.282+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-05 00:00:00+00:00, run_after=2023-07-06 00:00:00+00:00[0m
[[34m2024-05-22T03:41:27.292+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-09T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:41:27.292+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T03:41:27.293+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T03:41:27.293+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T03:41:27.293+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-09T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:41:27.295+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T03:41:27.295+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:41:27.295+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T03:41:27.296+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:41:27.296+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T03:41:27.296+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:41:27.297+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:41:28.867+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:41:29.355+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:42:04.650+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:42:05.965+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:42:06.346+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:42:08.257+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:42:09.496+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:42:09.867+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:47:12.667+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:47:12.667+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:47:12.668+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:47:12.673+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:42:06.449801+00:00, run_end_date=2024-05-22 03:42:07.657194+00:00, run_duration=1.207393, state=success, executor_state=success, try_number=1, max_tries=2, job_id=375, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 03:41:27.294051+00:00, queued_by_job_id=276, pid=85563[0m
[[34m2024-05-22T03:47:12.673+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:42:09.965602+00:00, run_end_date=2024-05-22 03:47:12.021558+00:00, run_duration=302.055956, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=376, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 03:41:27.294051+00:00, queued_by_job_id=276, pid=85587[0m
[[34m2024-05-22T03:47:12.673+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:41:29.472477+00:00, run_end_date=2024-05-22 03:42:03.955057+00:00, run_duration=34.48258, state=success, executor_state=success, try_number=1, max_tries=2, job_id=374, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 03:41:27.294051+00:00, queued_by_job_id=276, pid=85526[0m
[[34m2024-05-22T03:47:12.707+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T03:47:12.726+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-13 00:00:00+00:00, run_after=2023-07-14 00:00:00+00:00[0m
[[34m2024-05-22T03:47:12.817+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-05 00:00:00+00:00: scheduled__2023-07-05T00:00:00+00:00, state:running, queued_at: 2024-05-22 02:47:06.086332+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T03:47:12.818+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-05 00:00:00+00:00, run_id=scheduled__2023-07-05T00:00:00+00:00, run_start_date=2024-05-22 02:47:06.098901+00:00, run_end_date=2024-05-22 03:47:12.818255+00:00, run_duration=3606.719354, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-05 00:00:00+00:00, data_interval_end=2023-07-06 00:00:00+00:00, dag_hash=c3a0a08f3304f4efaa216d226cd9e31d[0m
[[34m2024-05-22T03:47:12.821+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-06 00:00:00+00:00, run_after=2023-07-07 00:00:00+00:00[0m
[[34m2024-05-22T03:47:12.832+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-10T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:47:12.832+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T03:47:12.833+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T03:47:12.833+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T03:47:12.833+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-10T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:47:12.835+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T03:47:12.835+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:47:12.836+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T03:47:12.836+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:47:12.836+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T03:47:12.836+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:47:12.838+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:47:14.297+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:47:14.690+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:54:06.805+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:54:08.318+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:54:08.952+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:54:11.217+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:54:12.450+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:54:12.814+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T03:59:15.527+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:59:15.528+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:59:15.528+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T03:59:15.533+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:54:12.914217+00:00, run_end_date=2024-05-22 03:59:14.877184+00:00, run_duration=301.962967, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=379, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 03:47:12.834009+00:00, queued_by_job_id=276, pid=86092[0m
[[34m2024-05-22T03:59:15.533+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:47:14.797015+00:00, run_end_date=2024-05-22 03:54:06.148001+00:00, run_duration=411.350986, state=success, executor_state=success, try_number=1, max_tries=2, job_id=377, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 03:47:12.834009+00:00, queued_by_job_id=276, pid=85800[0m
[[34m2024-05-22T03:59:15.534+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:54:09.069987+00:00, run_end_date=2024-05-22 03:54:10.481795+00:00, run_duration=1.411808, state=success, executor_state=success, try_number=1, max_tries=2, job_id=378, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 03:47:12.834009+00:00, queued_by_job_id=276, pid=86067[0m
[[34m2024-05-22T03:59:15.566+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T03:59:15.586+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-14 00:00:00+00:00, run_after=2023-07-15 00:00:00+00:00[0m
[[34m2024-05-22T03:59:15.676+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-06 00:00:00+00:00: scheduled__2023-07-06T00:00:00+00:00, state:running, queued_at: 2024-05-22 02:54:43.780574+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T03:59:15.676+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-06 00:00:00+00:00, run_id=scheduled__2023-07-06T00:00:00+00:00, run_start_date=2024-05-22 02:54:43.794937+00:00, run_end_date=2024-05-22 03:59:15.676909+00:00, run_duration=3871.881972, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-06 00:00:00+00:00, data_interval_end=2023-07-07 00:00:00+00:00, dag_hash=544245bd3a8edbdc76b40ff8c2ec50a3[0m
[[34m2024-05-22T03:59:15.679+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-07 00:00:00+00:00, run_after=2023-07-08 00:00:00+00:00[0m
[[34m2024-05-22T03:59:15.692+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-11T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:59:15.692+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T03:59:15.692+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T03:59:15.693+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T03:59:15.693+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-11T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T03:59:15.695+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T03:59:15.695+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:59:15.695+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T03:59:15.696+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:59:15.696+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T03:59:15.696+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:59:15.697+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T03:59:16.936+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T03:59:17.315+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:03:07.196+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:03:08.815+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:03:09.262+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:03:11.461+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:03:12.995+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:03:13.452+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:08:16.310+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:08:16.311+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:08:16.311+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:08:16.316+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 03:59:17.442894+00:00, run_end_date=2024-05-22 04:03:06.578097+00:00, run_duration=229.135203, state=success, executor_state=success, try_number=1, max_tries=2, job_id=380, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 03:59:15.693815+00:00, queued_by_job_id=276, pid=86290[0m
[[34m2024-05-22T04:08:16.317+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:03:09.389698+00:00, run_end_date=2024-05-22 04:03:10.788229+00:00, run_duration=1.398531, state=success, executor_state=success, try_number=1, max_tries=2, job_id=381, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 03:59:15.693815+00:00, queued_by_job_id=276, pid=86470[0m
[[34m2024-05-22T04:08:16.317+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:03:13.607664+00:00, run_end_date=2024-05-22 04:08:15.550836+00:00, run_duration=301.943172, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=382, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 03:59:15.693815+00:00, queued_by_job_id=276, pid=86494[0m
[[34m2024-05-22T04:08:16.353+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T04:08:16.372+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-15 00:00:00+00:00, run_after=2023-07-16 00:00:00+00:00[0m
[[34m2024-05-22T04:08:16.477+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-07 00:00:00+00:00: scheduled__2023-07-07T00:00:00+00:00, state:running, queued_at: 2024-05-22 03:01:37.249287+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T04:08:16.477+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-07 00:00:00+00:00, run_id=scheduled__2023-07-07T00:00:00+00:00, run_start_date=2024-05-22 03:01:37.262762+00:00, run_end_date=2024-05-22 04:08:16.477580+00:00, run_duration=3999.214818, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-07 00:00:00+00:00, data_interval_end=2023-07-08 00:00:00+00:00, dag_hash=6cdf639d5c9e005590e39f7bdc5d5f49[0m
[[34m2024-05-22T04:08:16.480+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-08 00:00:00+00:00, run_after=2023-07-09 00:00:00+00:00[0m
[[34m2024-05-22T04:08:16.492+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-12T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:08:16.492+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T04:08:16.493+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T04:08:16.493+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T04:08:16.493+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-12T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:08:16.495+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T04:08:16.495+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:08:16.496+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T04:08:16.496+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:08:16.496+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T04:08:16.497+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:08:16.498+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:08:17.983+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:08:18.497+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:12:47.532+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:12:48.819+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:12:49.205+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:12:51.119+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:12:52.436+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:12:52.850+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:17:55.535+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:17:55.535+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:17:55.535+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:17:55.555+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:08:18.617220+00:00, run_end_date=2024-05-22 04:12:46.908954+00:00, run_duration=268.291734, state=success, executor_state=success, try_number=1, max_tries=2, job_id=383, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 04:08:16.493944+00:00, queued_by_job_id=276, pid=86695[0m
[[34m2024-05-22T04:17:55.556+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:12:49.322667+00:00, run_end_date=2024-05-22 04:12:50.558171+00:00, run_duration=1.235504, state=success, executor_state=success, try_number=1, max_tries=2, job_id=384, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 04:08:16.493944+00:00, queued_by_job_id=276, pid=86871[0m
[[34m2024-05-22T04:17:55.556+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:12:52.962852+00:00, run_end_date=2024-05-22 04:17:54.910859+00:00, run_duration=301.948007, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=385, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 04:08:16.493944+00:00, queued_by_job_id=276, pid=86895[0m
[[34m2024-05-22T04:17:55.590+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T04:17:55.610+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-16 00:00:00+00:00, run_after=2023-07-17 00:00:00+00:00[0m
[[34m2024-05-22T04:17:55.702+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-08 00:00:00+00:00: scheduled__2023-07-08T00:00:00+00:00, state:running, queued_at: 2024-05-22 03:11:29.497471+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T04:17:55.703+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-08 00:00:00+00:00, run_id=scheduled__2023-07-08T00:00:00+00:00, run_start_date=2024-05-22 03:11:29.510139+00:00, run_end_date=2024-05-22 04:17:55.703127+00:00, run_duration=3986.192988, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-08 00:00:00+00:00, data_interval_end=2023-07-09 00:00:00+00:00, dag_hash=f381077fd42fbba3d3c97476923c3c37[0m
[[34m2024-05-22T04:17:55.706+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-09 00:00:00+00:00, run_after=2023-07-10 00:00:00+00:00[0m
[[34m2024-05-22T04:17:55.717+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:17:55.717+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T04:17:55.717+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T04:17:55.717+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T04:17:55.718+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:17:55.720+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T04:17:55.720+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:17:55.720+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T04:17:55.721+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:17:55.721+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T04:17:55.721+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:17:55.722+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:17:57.154+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:17:57.585+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:25:59.869+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:26:01.595+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:26:02.153+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:26:04.357+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:26:05.635+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:26:06.289+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:31:08.849+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:31:08.849+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:31:08.850+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:31:08.855+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:26:02.277657+00:00, run_end_date=2024-05-22 04:26:03.725228+00:00, run_duration=1.447571, state=success, executor_state=success, try_number=1, max_tries=2, job_id=387, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 04:17:55.718634+00:00, queued_by_job_id=276, pid=87401[0m
[[34m2024-05-22T04:31:08.855+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:26:06.415302+00:00, run_end_date=2024-05-22 04:31:08.263243+00:00, run_duration=301.847941, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=388, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 04:17:55.718634+00:00, queued_by_job_id=276, pid=87430[0m
[[34m2024-05-22T04:31:08.855+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:17:57.683021+00:00, run_end_date=2024-05-22 04:25:59.194389+00:00, run_duration=481.511368, state=success, executor_state=success, try_number=1, max_tries=2, job_id=386, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 04:17:55.718634+00:00, queued_by_job_id=276, pid=87098[0m
[[34m2024-05-22T04:31:08.889+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T04:31:08.910+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-17 00:00:00+00:00, run_after=2023-07-18 00:00:00+00:00[0m
[[34m2024-05-22T04:31:09.003+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-09 00:00:00+00:00: scheduled__2023-07-09T00:00:00+00:00, state:running, queued_at: 2024-05-22 03:18:57.267047+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T04:31:09.004+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-09 00:00:00+00:00, run_id=scheduled__2023-07-09T00:00:00+00:00, run_start_date=2024-05-22 03:18:57.279779+00:00, run_end_date=2024-05-22 04:31:09.004302+00:00, run_duration=4331.724523, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-09 00:00:00+00:00, data_interval_end=2023-07-10 00:00:00+00:00, dag_hash=987ab49e9cc3eaf33b4e54fadba57d4a[0m
[[34m2024-05-22T04:31:09.007+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-10 00:00:00+00:00, run_after=2023-07-11 00:00:00+00:00[0m
[[34m2024-05-22T04:31:09.018+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-14T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:31:09.018+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T04:31:09.018+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T04:31:09.018+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T04:31:09.018+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-14T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:31:09.021+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T04:31:09.021+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:31:09.021+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T04:31:09.021+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:31:09.022+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T04:31:09.022+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:31:09.023+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:31:10.932+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:31:11.330+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:34:03.635+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:34:05.196+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:34:05.636+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:34:07.843+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:34:09.238+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:34:09.629+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:39:12.292+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:39:12.293+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:39:12.293+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:39:12.298+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:34:09.725542+00:00, run_end_date=2024-05-22 04:39:11.682064+00:00, run_duration=301.956522, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=391, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 04:31:09.019479+00:00, queued_by_job_id=276, pid=87771[0m
[[34m2024-05-22T04:39:12.299+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:31:11.441172+00:00, run_end_date=2024-05-22 04:34:02.906269+00:00, run_duration=171.465097, state=success, executor_state=success, try_number=1, max_tries=2, job_id=389, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 04:31:09.019479+00:00, queued_by_job_id=276, pid=87630[0m
[[34m2024-05-22T04:39:12.299+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:34:05.777681+00:00, run_end_date=2024-05-22 04:34:07.148571+00:00, run_duration=1.37089, state=success, executor_state=success, try_number=1, max_tries=2, job_id=390, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 04:31:09.019479+00:00, queued_by_job_id=276, pid=87746[0m
[[34m2024-05-22T04:39:12.333+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T04:39:12.353+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-18 00:00:00+00:00, run_after=2023-07-19 00:00:00+00:00[0m
[[34m2024-05-22T04:39:12.447+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-10 00:00:00+00:00: scheduled__2023-07-10T00:00:00+00:00, state:running, queued_at: 2024-05-22 03:27:29.205328+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T04:39:12.448+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-10 00:00:00+00:00, run_id=scheduled__2023-07-10T00:00:00+00:00, run_start_date=2024-05-22 03:27:29.217653+00:00, run_end_date=2024-05-22 04:39:12.448223+00:00, run_duration=4303.23057, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-10 00:00:00+00:00, data_interval_end=2023-07-11 00:00:00+00:00, dag_hash=6a18151754e9c86e5a2a04a6e69cf3ee[0m
[[34m2024-05-22T04:39:12.451+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-11 00:00:00+00:00, run_after=2023-07-12 00:00:00+00:00[0m
[[34m2024-05-22T04:39:12.461+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:39:12.461+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T04:39:12.461+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T04:39:12.462+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T04:39:12.462+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:39:12.464+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T04:39:12.464+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:39:12.464+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T04:39:12.465+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:39:12.465+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T04:39:12.465+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:39:12.467+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:39:13.724+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:39:14.078+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:39:49.059+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:39:50.425+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:39:50.851+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:39:52.835+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:39:54.108+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:39:54.483+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:44:57.063+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:44:57.063+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:44:57.063+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:44:57.069+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:39:50.951779+00:00, run_end_date=2024-05-22 04:39:52.204379+00:00, run_duration=1.2526, state=success, executor_state=success, try_number=1, max_tries=2, job_id=393, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 04:39:12.462917+00:00, queued_by_job_id=276, pid=88009[0m
[[34m2024-05-22T04:44:57.070+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:39:54.585815+00:00, run_end_date=2024-05-22 04:44:56.464187+00:00, run_duration=301.878372, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=394, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 04:39:12.462917+00:00, queued_by_job_id=276, pid=88033[0m
[[34m2024-05-22T04:44:57.070+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:39:14.176617+00:00, run_end_date=2024-05-22 04:39:48.430266+00:00, run_duration=34.253649, state=success, executor_state=success, try_number=1, max_tries=2, job_id=392, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 04:39:12.462917+00:00, queued_by_job_id=276, pid=87968[0m
[[34m2024-05-22T04:44:57.112+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T04:44:57.132+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-19 00:00:00+00:00, run_after=2023-07-20 00:00:00+00:00[0m
[[34m2024-05-22T04:44:57.234+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-11 00:00:00+00:00: scheduled__2023-07-11T00:00:00+00:00, state:running, queued_at: 2024-05-22 03:41:27.175838+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T04:44:57.234+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-11 00:00:00+00:00, run_id=scheduled__2023-07-11T00:00:00+00:00, run_start_date=2024-05-22 03:41:27.189392+00:00, run_end_date=2024-05-22 04:44:57.234882+00:00, run_duration=3810.04549, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-11 00:00:00+00:00, data_interval_end=2023-07-12 00:00:00+00:00, dag_hash=b896115bd03ef3e33d4d9f1aea8602f9[0m
[[34m2024-05-22T04:44:57.238+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-12 00:00:00+00:00, run_after=2023-07-13 00:00:00+00:00[0m
[[34m2024-05-22T04:44:57.251+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:44:57.251+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T04:44:57.251+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T04:44:57.251+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T04:44:57.252+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:44:57.254+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T04:44:57.254+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:44:57.255+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T04:44:57.255+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:44:57.255+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T04:44:57.255+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:44:57.257+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:44:58.691+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:44:59.100+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:46:02.107+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:46:03.346+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:46:03.820+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:46:05.972+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:46:07.335+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:46:07.780+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:51:10.936+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:51:10.937+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:51:10.937+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:51:10.956+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:46:07.894903+00:00, run_end_date=2024-05-22 04:51:10.073072+00:00, run_duration=302.178169, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=397, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 04:44:57.252925+00:00, queued_by_job_id=276, pid=88313[0m
[[34m2024-05-22T04:51:10.957+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:44:59.201319+00:00, run_end_date=2024-05-22 04:46:01.359039+00:00, run_duration=62.15772, state=success, executor_state=success, try_number=1, max_tries=2, job_id=395, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 04:44:57.252925+00:00, queued_by_job_id=276, pid=88230[0m
[[34m2024-05-22T04:51:10.957+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:46:04.011838+00:00, run_end_date=2024-05-22 04:46:05.291487+00:00, run_duration=1.279649, state=success, executor_state=success, try_number=1, max_tries=2, job_id=396, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 04:44:57.252925+00:00, queued_by_job_id=276, pid=88288[0m
[[34m2024-05-22T04:51:10.993+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T04:51:11.014+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-20 00:00:00+00:00, run_after=2023-07-21 00:00:00+00:00[0m
[[34m2024-05-22T04:51:11.106+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-12 00:00:00+00:00: scheduled__2023-07-12T00:00:00+00:00, state:running, queued_at: 2024-05-22 03:47:12.719666+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T04:51:11.107+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-12 00:00:00+00:00, run_id=scheduled__2023-07-12T00:00:00+00:00, run_start_date=2024-05-22 03:47:12.733806+00:00, run_end_date=2024-05-22 04:51:11.107275+00:00, run_duration=3838.373469, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-12 00:00:00+00:00, data_interval_end=2023-07-13 00:00:00+00:00, dag_hash=e9cbc45baa6bd5b906edd2c71a3d1645[0m
[[34m2024-05-22T04:51:11.110+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-13 00:00:00+00:00, run_after=2023-07-14 00:00:00+00:00[0m
[[34m2024-05-22T04:51:11.123+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:51:11.123+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T04:51:11.123+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T04:51:11.123+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T04:51:11.124+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:51:11.126+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T04:51:11.126+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:51:11.127+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T04:51:11.127+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:51:11.127+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T04:51:11.127+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:51:11.129+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:51:12.426+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:51:12.808+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:51:59.929+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:52:01.536+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:52:02.094+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:52:04.387+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:52:05.609+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:52:05.966+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:57:08.999+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:57:08.999+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:57:08.999+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T04:57:09.014+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:52:02.269147+00:00, run_end_date=2024-05-22 04:52:03.686318+00:00, run_duration=1.417171, state=success, executor_state=success, try_number=1, max_tries=2, job_id=399, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 04:51:11.124803+00:00, queued_by_job_id=276, pid=88560[0m
[[34m2024-05-22T04:57:09.016+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:52:06.073444+00:00, run_end_date=2024-05-22 04:57:08.162772+00:00, run_duration=302.089328, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=400, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 04:51:11.124803+00:00, queued_by_job_id=276, pid=88585[0m
[[34m2024-05-22T04:57:09.017+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:51:12.910027+00:00, run_end_date=2024-05-22 04:51:59.299090+00:00, run_duration=46.389063, state=success, executor_state=success, try_number=1, max_tries=2, job_id=398, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 04:51:11.124803+00:00, queued_by_job_id=276, pid=88513[0m
[[34m2024-05-22T04:57:09.054+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T04:57:09.076+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-21 00:00:00+00:00, run_after=2023-07-22 00:00:00+00:00[0m
[[34m2024-05-22T04:57:09.182+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-13 00:00:00+00:00: scheduled__2023-07-13T00:00:00+00:00, state:running, queued_at: 2024-05-22 03:59:15.578924+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T04:57:09.182+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-13 00:00:00+00:00, run_id=scheduled__2023-07-13T00:00:00+00:00, run_start_date=2024-05-22 03:59:15.593863+00:00, run_end_date=2024-05-22 04:57:09.182822+00:00, run_duration=3473.588959, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-13 00:00:00+00:00, data_interval_end=2023-07-14 00:00:00+00:00, dag_hash=3f636153d63ec7e2468addeaa9349213[0m
[[34m2024-05-22T04:57:09.185+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-14 00:00:00+00:00, run_after=2023-07-15 00:00:00+00:00[0m
[[34m2024-05-22T04:57:09.200+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:57:09.200+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T04:57:09.201+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T04:57:09.201+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T04:57:09.201+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-18T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T04:57:09.204+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T04:57:09.204+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:57:09.204+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T04:57:09.204+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:57:09.205+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T04:57:09.205+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:57:09.206+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:57:10.812+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:57:11.359+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:58:43.633+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:58:45.111+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:58:45.543+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T04:58:47.360+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T04:58:48.843+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T04:58:49.288+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:03:52.239+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:03:52.240+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:03:52.240+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:03:52.246+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:57:11.473208+00:00, run_end_date=2024-05-22 04:58:42.982666+00:00, run_duration=91.509458, state=success, executor_state=success, try_number=1, max_tries=2, job_id=401, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 04:57:09.202249+00:00, queued_by_job_id=276, pid=88797[0m
[[34m2024-05-22T05:03:52.247+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:58:45.638469+00:00, run_end_date=2024-05-22 04:58:46.775815+00:00, run_duration=1.137346, state=success, executor_state=success, try_number=1, max_tries=2, job_id=402, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 04:57:09.202249+00:00, queued_by_job_id=276, pid=88874[0m
[[34m2024-05-22T05:03:52.247+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 04:58:49.396113+00:00, run_end_date=2024-05-22 05:03:51.458562+00:00, run_duration=302.062449, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=403, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 04:57:09.202249+00:00, queued_by_job_id=276, pid=88898[0m
[[34m2024-05-22T05:03:52.283+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T05:03:52.303+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-22 00:00:00+00:00, run_after=2023-07-23 00:00:00+00:00[0m
[[34m2024-05-22T05:03:52.402+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-14 00:00:00+00:00: scheduled__2023-07-14T00:00:00+00:00, state:running, queued_at: 2024-05-22 04:08:16.365903+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T05:03:52.403+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-14 00:00:00+00:00, run_id=scheduled__2023-07-14T00:00:00+00:00, run_start_date=2024-05-22 04:08:16.380422+00:00, run_end_date=2024-05-22 05:03:52.403327+00:00, run_duration=3336.022905, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-14 00:00:00+00:00, data_interval_end=2023-07-15 00:00:00+00:00, dag_hash=9a6b59c907e501b15c00fa845118b1f3[0m
[[34m2024-05-22T05:03:52.407+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-15 00:00:00+00:00, run_after=2023-07-16 00:00:00+00:00[0m
[[34m2024-05-22T05:03:52.420+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-19T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:03:52.420+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T05:03:52.420+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T05:03:52.421+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T05:03:52.421+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-19T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:03:52.423+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T05:03:52.423+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:03:52.424+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T05:03:52.424+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:03:52.424+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T05:03:52.424+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:03:52.426+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:03:53.842+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:03:54.244+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:07:17.204+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:07:18.899+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:07:19.404+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:07:21.617+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:07:23.068+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:07:23.524+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:12:26.244+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:12:26.245+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:12:26.245+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:12:26.250+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:03:54.345485+00:00, run_end_date=2024-05-22 05:07:16.520829+00:00, run_duration=202.175344, state=success, executor_state=success, try_number=1, max_tries=2, job_id=404, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 05:03:52.421884+00:00, queued_by_job_id=276, pid=89096[0m
[[34m2024-05-22T05:12:26.250+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:07:19.530991+00:00, run_end_date=2024-05-22 05:07:20.953827+00:00, run_duration=1.422836, state=success, executor_state=success, try_number=1, max_tries=2, job_id=405, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 05:03:52.421884+00:00, queued_by_job_id=276, pid=89234[0m
[[34m2024-05-22T05:12:26.250+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:07:23.625222+00:00, run_end_date=2024-05-22 05:12:25.555626+00:00, run_duration=301.930404, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=406, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 05:03:52.421884+00:00, queued_by_job_id=276, pid=89258[0m
[[34m2024-05-22T05:12:26.285+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T05:12:26.304+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-23 00:00:00+00:00, run_after=2023-07-24 00:00:00+00:00[0m
[[34m2024-05-22T05:12:26.402+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-15 00:00:00+00:00: scheduled__2023-07-15T00:00:00+00:00, state:running, queued_at: 2024-05-22 04:17:55.604167+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T05:12:26.402+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-15 00:00:00+00:00, run_id=scheduled__2023-07-15T00:00:00+00:00, run_start_date=2024-05-22 04:17:55.618108+00:00, run_end_date=2024-05-22 05:12:26.402365+00:00, run_duration=3270.784257, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-15 00:00:00+00:00, data_interval_end=2023-07-16 00:00:00+00:00, dag_hash=eb55b1a427e4a5d0368183dd6c8c5798[0m
[[34m2024-05-22T05:12:26.405+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-16 00:00:00+00:00, run_after=2023-07-17 00:00:00+00:00[0m
[[34m2024-05-22T05:12:26.416+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-20T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:12:26.417+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T05:12:26.417+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T05:12:26.417+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T05:12:26.417+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-20T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:12:26.419+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T05:12:26.419+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:12:26.420+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T05:12:26.420+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:12:26.420+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T05:12:26.420+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:12:26.421+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:12:27.660+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:12:28.127+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:13:44.920+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:13:46.139+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:13:46.502+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:13:48.651+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:13:49.884+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:13:50.253+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:18:53.321+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:18:53.322+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:18:53.322+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:18:53.329+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:12:28.238961+00:00, run_end_date=2024-05-22 05:13:44.300867+00:00, run_duration=76.061906, state=success, executor_state=success, try_number=1, max_tries=2, job_id=407, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 05:12:26.418086+00:00, queued_by_job_id=276, pid=89458[0m
[[34m2024-05-22T05:18:53.330+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:13:46.596911+00:00, run_end_date=2024-05-22 05:13:48.056161+00:00, run_duration=1.45925, state=success, executor_state=success, try_number=1, max_tries=2, job_id=408, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 05:12:26.418086+00:00, queued_by_job_id=276, pid=89518[0m
[[34m2024-05-22T05:18:53.330+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:13:50.471515+00:00, run_end_date=2024-05-22 05:18:52.555604+00:00, run_duration=302.084089, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=409, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 05:12:26.418086+00:00, queued_by_job_id=276, pid=89546[0m
[[34m2024-05-22T05:18:53.365+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T05:18:53.384+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-24 00:00:00+00:00, run_after=2023-07-25 00:00:00+00:00[0m
[[34m2024-05-22T05:18:53.515+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-16 00:00:00+00:00: scheduled__2023-07-16T00:00:00+00:00, state:running, queued_at: 2024-05-22 04:31:08.903097+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T05:18:53.516+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-16 00:00:00+00:00, run_id=scheduled__2023-07-16T00:00:00+00:00, run_start_date=2024-05-22 04:31:08.917802+00:00, run_end_date=2024-05-22 05:18:53.516253+00:00, run_duration=2864.598451, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-16 00:00:00+00:00, data_interval_end=2023-07-17 00:00:00+00:00, dag_hash=82afde47e5caec6d1bf8c825244411db[0m
[[34m2024-05-22T05:18:53.519+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-17 00:00:00+00:00, run_after=2023-07-18 00:00:00+00:00[0m
[[34m2024-05-22T05:18:53.529+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:18:53.530+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T05:18:53.530+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T05:18:53.530+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T05:18:53.530+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:18:53.532+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T05:18:53.532+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:18:53.533+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T05:18:53.533+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:18:53.534+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T05:18:53.534+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:18:53.536+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:18:54.830+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:18:55.230+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:21:34.381+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:21:35.671+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:21:36.059+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:21:38.171+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:21:39.696+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:21:40.221+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:26:43.092+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:26:43.092+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:26:43.093+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:26:43.119+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:18:55.330512+00:00, run_end_date=2024-05-22 05:21:33.669351+00:00, run_duration=158.338839, state=success, executor_state=success, try_number=1, max_tries=2, job_id=410, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 05:18:53.531266+00:00, queued_by_job_id=276, pid=89753[0m
[[34m2024-05-22T05:26:43.119+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:21:36.165119+00:00, run_end_date=2024-05-22 05:21:37.521892+00:00, run_duration=1.356773, state=success, executor_state=success, try_number=1, max_tries=2, job_id=411, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 05:18:53.531266+00:00, queued_by_job_id=276, pid=89880[0m
[[34m2024-05-22T05:26:43.119+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:21:40.377149+00:00, run_end_date=2024-05-22 05:26:42.438939+00:00, run_duration=302.06179, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=412, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 05:18:53.531266+00:00, queued_by_job_id=276, pid=89904[0m
[[34m2024-05-22T05:26:43.156+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T05:26:43.189+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-25 00:00:00+00:00, run_after=2023-07-26 00:00:00+00:00[0m
[[34m2024-05-22T05:26:43.343+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-17 00:00:00+00:00: scheduled__2023-07-17T00:00:00+00:00, state:running, queued_at: 2024-05-22 04:39:12.346099+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T05:26:43.343+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-17 00:00:00+00:00, run_id=scheduled__2023-07-17T00:00:00+00:00, run_start_date=2024-05-22 04:39:12.361517+00:00, run_end_date=2024-05-22 05:26:43.343609+00:00, run_duration=2850.982092, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-17 00:00:00+00:00, data_interval_end=2023-07-18 00:00:00+00:00, dag_hash=8f9865db25052bc213208df4799506fc[0m
[[34m2024-05-22T05:26:43.347+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-18 00:00:00+00:00, run_after=2023-07-19 00:00:00+00:00[0m
[[34m2024-05-22T05:26:43.359+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-22T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:26:43.360+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T05:26:43.360+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T05:26:43.360+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T05:26:43.360+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-22T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:26:43.362+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T05:26:43.362+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:26:43.363+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T05:26:43.363+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:26:43.363+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T05:26:43.363+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:26:43.365+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:26:44.916+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:26:45.362+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:29:31.268+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:29:32.664+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:29:33.060+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:29:35.089+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:29:36.451+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:29:36.872+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:34:39.640+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:34:39.641+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:34:39.641+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:34:39.646+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:29:33.162082+00:00, run_end_date=2024-05-22 05:29:34.434373+00:00, run_duration=1.272291, state=success, executor_state=success, try_number=1, max_tries=2, job_id=414, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 05:26:43.361140+00:00, queued_by_job_id=276, pid=90225[0m
[[34m2024-05-22T05:34:39.647+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:29:36.982973+00:00, run_end_date=2024-05-22 05:34:38.968388+00:00, run_duration=301.985415, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=415, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 05:26:43.361140+00:00, queued_by_job_id=276, pid=90249[0m
[[34m2024-05-22T05:34:39.647+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:26:45.462133+00:00, run_end_date=2024-05-22 05:29:30.608529+00:00, run_duration=165.146396, state=success, executor_state=success, try_number=1, max_tries=2, job_id=413, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 05:26:43.361140+00:00, queued_by_job_id=276, pid=90108[0m
[[34m2024-05-22T05:34:39.681+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T05:34:39.701+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-26 00:00:00+00:00, run_after=2023-07-27 00:00:00+00:00[0m
[[34m2024-05-22T05:34:39.796+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-18 00:00:00+00:00: scheduled__2023-07-18T00:00:00+00:00, state:running, queued_at: 2024-05-22 04:44:57.126079+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T05:34:39.797+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-18 00:00:00+00:00, run_id=scheduled__2023-07-18T00:00:00+00:00, run_start_date=2024-05-22 04:44:57.140423+00:00, run_end_date=2024-05-22 05:34:39.797164+00:00, run_duration=2982.656741, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-18 00:00:00+00:00, data_interval_end=2023-07-19 00:00:00+00:00, dag_hash=17084e52edd963e83fe00846b9f15785[0m
[[34m2024-05-22T05:34:39.800+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-19 00:00:00+00:00, run_after=2023-07-20 00:00:00+00:00[0m
[[34m2024-05-22T05:34:39.811+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-23T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:34:39.811+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T05:34:39.811+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T05:34:39.811+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T05:34:39.811+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-23T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:34:39.813+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T05:34:39.814+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:34:39.814+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T05:34:39.814+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:34:39.814+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T05:34:39.815+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:34:39.816+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:34:41.422+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:34:41.834+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:37:09.045+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:37:10.494+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:37:11.090+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:37:13.288+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:37:14.741+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:37:15.191+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:42:17.847+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:42:17.848+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:42:17.848+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:42:17.854+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:37:15.343443+00:00, run_end_date=2024-05-22 05:42:17.213116+00:00, run_duration=301.869673, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=418, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 05:34:39.812467+00:00, queued_by_job_id=276, pid=90580[0m
[[34m2024-05-22T05:42:17.854+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:34:41.959379+00:00, run_end_date=2024-05-22 05:37:08.327394+00:00, run_duration=146.368015, state=success, executor_state=success, try_number=1, max_tries=2, job_id=416, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 05:34:39.812467+00:00, queued_by_job_id=276, pid=90449[0m
[[34m2024-05-22T05:42:17.854+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:37:11.208197+00:00, run_end_date=2024-05-22 05:37:12.587875+00:00, run_duration=1.379678, state=success, executor_state=success, try_number=1, max_tries=2, job_id=417, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 05:34:39.812467+00:00, queued_by_job_id=276, pid=90556[0m
[[34m2024-05-22T05:42:17.889+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T05:42:17.910+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-27 00:00:00+00:00, run_after=2023-07-28 00:00:00+00:00[0m
[[34m2024-05-22T05:42:18.008+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-19 00:00:00+00:00: scheduled__2023-07-19T00:00:00+00:00, state:running, queued_at: 2024-05-22 04:51:11.006999+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T05:42:18.009+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-19 00:00:00+00:00, run_id=scheduled__2023-07-19T00:00:00+00:00, run_start_date=2024-05-22 04:51:11.021358+00:00, run_end_date=2024-05-22 05:42:18.009201+00:00, run_duration=3066.987843, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-19 00:00:00+00:00, data_interval_end=2023-07-20 00:00:00+00:00, dag_hash=e610a78050fb29b9230d7246c3e7abe9[0m
[[34m2024-05-22T05:42:18.012+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-20 00:00:00+00:00, run_after=2023-07-21 00:00:00+00:00[0m
[[34m2024-05-22T05:42:18.023+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-24T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:42:18.024+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T05:42:18.024+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T05:42:18.024+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T05:42:18.024+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-24T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:42:18.026+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T05:42:18.027+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:42:18.027+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T05:42:18.027+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:42:18.027+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T05:42:18.028+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:42:18.029+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:42:19.289+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:42:19.671+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:44:34.018+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:44:35.552+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:44:35.960+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:44:38.192+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:44:39.592+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:44:40.013+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:49:42.966+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:49:42.966+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:49:42.966+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:49:42.971+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:42:19.766156+00:00, run_end_date=2024-05-22 05:44:33.405029+00:00, run_duration=133.638873, state=success, executor_state=success, try_number=1, max_tries=2, job_id=419, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 05:42:18.025400+00:00, queued_by_job_id=276, pid=90776[0m
[[34m2024-05-22T05:49:42.971+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:44:36.102875+00:00, run_end_date=2024-05-22 05:44:37.490329+00:00, run_duration=1.387454, state=success, executor_state=success, try_number=1, max_tries=2, job_id=420, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 05:42:18.025400+00:00, queued_by_job_id=276, pid=90873[0m
[[34m2024-05-22T05:49:42.971+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:44:40.148112+00:00, run_end_date=2024-05-22 05:49:42.330370+00:00, run_duration=302.182258, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=421, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 05:42:18.025400+00:00, queued_by_job_id=276, pid=90897[0m
[[34m2024-05-22T05:49:43.007+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T05:49:43.026+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-28 00:00:00+00:00, run_after=2023-07-29 00:00:00+00:00[0m
[[34m2024-05-22T05:49:43.194+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-20 00:00:00+00:00: scheduled__2023-07-20T00:00:00+00:00, state:running, queued_at: 2024-05-22 04:57:09.068620+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T05:49:43.194+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-20 00:00:00+00:00, run_id=scheduled__2023-07-20T00:00:00+00:00, run_start_date=2024-05-22 04:57:09.084086+00:00, run_end_date=2024-05-22 05:49:43.194487+00:00, run_duration=3154.110401, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-20 00:00:00+00:00, data_interval_end=2023-07-21 00:00:00+00:00, dag_hash=75115cebefe4a0ce951b18341bfac850[0m
[[34m2024-05-22T05:49:43.198+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-21 00:00:00+00:00, run_after=2023-07-22 00:00:00+00:00[0m
[[34m2024-05-22T05:49:43.209+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-25T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:49:43.210+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T05:49:43.210+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T05:49:43.210+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T05:49:43.210+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-25T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:49:43.214+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T05:49:43.214+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:49:43.215+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T05:49:43.215+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:49:43.215+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T05:49:43.215+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:49:43.218+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:49:44.925+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:49:45.307+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:52:29.401+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:52:31.109+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:52:31.528+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:52:33.811+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:52:35.191+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:52:35.666+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:57:38.520+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:57:38.521+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:57:38.521+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T05:57:38.543+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:52:31.629917+00:00, run_end_date=2024-05-22 05:52:33.048624+00:00, run_duration=1.418707, state=success, executor_state=success, try_number=1, max_tries=2, job_id=423, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 05:49:43.212260+00:00, queued_by_job_id=276, pid=91215[0m
[[34m2024-05-22T05:57:38.544+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:49:45.407656+00:00, run_end_date=2024-05-22 05:52:28.801131+00:00, run_duration=163.393475, state=success, executor_state=success, try_number=1, max_tries=2, job_id=422, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 05:49:43.212260+00:00, queued_by_job_id=276, pid=91096[0m
[[34m2024-05-22T05:57:38.544+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:52:35.771476+00:00, run_end_date=2024-05-22 05:57:37.828124+00:00, run_duration=302.056648, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=424, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 05:49:43.212260+00:00, queued_by_job_id=276, pid=91239[0m
[[34m2024-05-22T05:57:38.582+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T05:57:38.603+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-29 00:00:00+00:00, run_after=2023-07-30 00:00:00+00:00[0m
[[34m2024-05-22T05:57:38.701+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-21 00:00:00+00:00: scheduled__2023-07-21T00:00:00+00:00, state:running, queued_at: 2024-05-22 05:03:52.297335+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T05:57:38.702+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-21 00:00:00+00:00, run_id=scheduled__2023-07-21T00:00:00+00:00, run_start_date=2024-05-22 05:03:52.311322+00:00, run_end_date=2024-05-22 05:57:38.702335+00:00, run_duration=3226.391013, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-21 00:00:00+00:00, data_interval_end=2023-07-22 00:00:00+00:00, dag_hash=55780393153765eb598988ce689d22e1[0m
[[34m2024-05-22T05:57:38.705+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-22 00:00:00+00:00, run_after=2023-07-23 00:00:00+00:00[0m
[[34m2024-05-22T05:57:38.716+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-26T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:57:38.716+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T05:57:38.717+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T05:57:38.717+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T05:57:38.717+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-26T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T05:57:38.719+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T05:57:38.719+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:57:38.720+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T05:57:38.720+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:57:38.720+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T05:57:38.720+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:57:38.722+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:57:40.108+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:57:40.612+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:58:42.659+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:58:44.116+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:58:44.491+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T05:58:46.773+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T05:58:48.063+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T05:58:48.589+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:03:51.448+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:03:51.448+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:03:51.449+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:03:51.458+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:57:40.827668+00:00, run_end_date=2024-05-22 05:58:42.011460+00:00, run_duration=61.183792, state=success, executor_state=success, try_number=1, max_tries=2, job_id=425, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 05:57:38.718271+00:00, queued_by_job_id=276, pid=91438[0m
[[34m2024-05-22T06:03:51.459+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:58:48.700698+00:00, run_end_date=2024-05-22 06:03:50.828529+00:00, run_duration=302.127831, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=427, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 05:57:38.718271+00:00, queued_by_job_id=276, pid=91518[0m
[[34m2024-05-22T06:03:51.459+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 05:58:44.641961+00:00, run_end_date=2024-05-22 05:58:46.032084+00:00, run_duration=1.390123, state=success, executor_state=success, try_number=1, max_tries=2, job_id=426, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 05:57:38.718271+00:00, queued_by_job_id=276, pid=91493[0m
[[34m2024-05-22T06:03:51.502+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T06:03:51.530+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-30 00:00:00+00:00, run_after=2023-07-31 00:00:00+00:00[0m
[[34m2024-05-22T06:03:51.664+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-22 00:00:00+00:00: scheduled__2023-07-22T00:00:00+00:00, state:running, queued_at: 2024-05-22 05:12:26.297642+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T06:03:51.664+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-22 00:00:00+00:00, run_id=scheduled__2023-07-22T00:00:00+00:00, run_start_date=2024-05-22 05:12:26.310994+00:00, run_end_date=2024-05-22 06:03:51.664840+00:00, run_duration=3085.353846, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-22 00:00:00+00:00, data_interval_end=2023-07-23 00:00:00+00:00, dag_hash=c5afedf8c9dbc680a9114662a3596319[0m
[[34m2024-05-22T06:03:51.673+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-23 00:00:00+00:00, run_after=2023-07-24 00:00:00+00:00[0m
[[34m2024-05-22T06:03:51.693+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-27T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:03:51.694+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T06:03:51.694+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T06:03:51.695+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T06:03:51.695+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-27T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:03:51.698+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T06:03:51.698+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:03:51.699+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T06:03:51.699+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:03:51.700+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T06:03:51.700+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:03:51.702+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:03:53.265+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:03:53.656+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:04:42.255+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:04:43.692+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:04:44.070+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:04:46.227+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:04:47.627+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:04:48.055+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:09:50.949+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:09:50.949+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:09:50.950+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:09:50.956+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:04:44.175518+00:00, run_end_date=2024-05-22 06:04:45.607353+00:00, run_duration=1.431835, state=success, executor_state=success, try_number=1, max_tries=2, job_id=429, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 06:03:51.695986+00:00, queued_by_job_id=276, pid=91768[0m
[[34m2024-05-22T06:09:50.956+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:03:53.755147+00:00, run_end_date=2024-05-22 06:04:41.655357+00:00, run_duration=47.90021, state=success, executor_state=success, try_number=1, max_tries=2, job_id=428, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 06:03:51.695986+00:00, queued_by_job_id=276, pid=91721[0m
[[34m2024-05-22T06:09:50.957+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:04:48.169762+00:00, run_end_date=2024-05-22 06:09:50.080289+00:00, run_duration=301.910527, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=430, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 06:03:51.695986+00:00, queued_by_job_id=276, pid=91793[0m
[[34m2024-05-22T06:09:51.010+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T06:09:51.035+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-31 00:00:00+00:00, run_after=2023-08-01 00:00:00+00:00[0m
[[34m2024-05-22T06:09:51.162+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-23 00:00:00+00:00: scheduled__2023-07-23T00:00:00+00:00, state:running, queued_at: 2024-05-22 05:18:53.378782+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T06:09:51.162+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-23 00:00:00+00:00, run_id=scheduled__2023-07-23T00:00:00+00:00, run_start_date=2024-05-22 05:18:53.392210+00:00, run_end_date=2024-05-22 06:09:51.162711+00:00, run_duration=3057.770501, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-23 00:00:00+00:00, data_interval_end=2023-07-24 00:00:00+00:00, dag_hash=e10b41070334e1f6b021e7f929663455[0m
[[34m2024-05-22T06:09:51.166+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-24 00:00:00+00:00, run_after=2023-07-25 00:00:00+00:00[0m
[[34m2024-05-22T06:09:51.179+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-28T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:09:51.179+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T06:09:51.179+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T06:09:51.180+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T06:09:51.180+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-28T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:09:51.182+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T06:09:51.183+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:09:51.183+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T06:09:51.183+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:09:51.184+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T06:09:51.184+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:09:51.185+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:09:52.524+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:09:52.896+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:10:36.051+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:10:37.440+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:10:37.891+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:10:39.923+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:10:41.396+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:10:41.775+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:15:44.537+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:15:44.537+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:15:44.537+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:15:44.543+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:10:41.879655+00:00, run_end_date=2024-05-22 06:15:43.859892+00:00, run_duration=301.980237, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=433, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 06:09:51.181123+00:00, queued_by_job_id=276, pid=92064[0m
[[34m2024-05-22T06:15:44.543+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:09:52.994998+00:00, run_end_date=2024-05-22 06:10:35.389062+00:00, run_duration=42.394064, state=success, executor_state=success, try_number=1, max_tries=2, job_id=431, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 06:09:51.181123+00:00, queued_by_job_id=276, pid=91991[0m
[[34m2024-05-22T06:15:44.544+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:10:38.018708+00:00, run_end_date=2024-05-22 06:10:39.271497+00:00, run_duration=1.252789, state=success, executor_state=success, try_number=1, max_tries=2, job_id=432, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 06:09:51.181123+00:00, queued_by_job_id=276, pid=92040[0m
[[34m2024-05-22T06:15:44.577+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T06:15:44.598+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-01 00:00:00+00:00, run_after=2023-08-02 00:00:00+00:00[0m
[[34m2024-05-22T06:15:44.696+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-24 00:00:00+00:00: scheduled__2023-07-24T00:00:00+00:00, state:running, queued_at: 2024-05-22 05:26:43.181015+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T06:15:44.696+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-24 00:00:00+00:00, run_id=scheduled__2023-07-24T00:00:00+00:00, run_start_date=2024-05-22 05:26:43.197301+00:00, run_end_date=2024-05-22 06:15:44.696779+00:00, run_duration=2941.499478, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-24 00:00:00+00:00, data_interval_end=2023-07-25 00:00:00+00:00, dag_hash=94466db901abd35f125cd506a132cfc2[0m
[[34m2024-05-22T06:15:44.700+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-25 00:00:00+00:00, run_after=2023-07-26 00:00:00+00:00[0m
[[34m2024-05-22T06:15:44.711+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-31T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-29T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:15:44.711+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T06:15:44.712+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T06:15:44.712+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T06:15:44.712+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-31T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-29T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:15:44.714+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T06:15:44.714+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:15:44.715+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T06:15:44.715+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:15:44.715+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T06:15:44.715+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:15:44.717+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:15:46.106+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:15:46.523+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-07-31T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:18:35.112+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:18:36.468+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:18:36.813+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:18:38.869+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:18:40.295+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:18:40.900+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:23:43.848+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:23:43.848+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:23:43.848+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:23:43.853+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:18:36.907496+00:00, run_end_date=2024-05-22 06:18:38.142638+00:00, run_duration=1.235142, state=success, executor_state=success, try_number=1, max_tries=2, job_id=435, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 06:15:44.712976+00:00, queued_by_job_id=276, pid=92379[0m
[[34m2024-05-22T06:23:43.854+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:18:41.036625+00:00, run_end_date=2024-05-22 06:23:43.202955+00:00, run_duration=302.16633, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=436, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 06:15:44.712976+00:00, queued_by_job_id=276, pid=92403[0m
[[34m2024-05-22T06:23:43.854+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-07-31T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:15:46.623035+00:00, run_end_date=2024-05-22 06:18:34.535156+00:00, run_duration=167.912121, state=success, executor_state=success, try_number=1, max_tries=2, job_id=434, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 06:15:44.712976+00:00, queued_by_job_id=276, pid=92263[0m
[[34m2024-05-22T06:23:43.886+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T06:23:43.905+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-02 00:00:00+00:00, run_after=2023-08-03 00:00:00+00:00[0m
[[34m2024-05-22T06:23:44.009+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-25 00:00:00+00:00: scheduled__2023-07-25T00:00:00+00:00, state:running, queued_at: 2024-05-22 05:34:39.694694+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T06:23:44.009+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-25 00:00:00+00:00, run_id=scheduled__2023-07-25T00:00:00+00:00, run_start_date=2024-05-22 05:34:39.708561+00:00, run_end_date=2024-05-22 06:23:44.009418+00:00, run_duration=2944.300857, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-25 00:00:00+00:00, data_interval_end=2023-07-26 00:00:00+00:00, dag_hash=69fb94243831b502e1afff972d917aa0[0m
[[34m2024-05-22T06:23:44.012+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-26 00:00:00+00:00, run_after=2023-07-27 00:00:00+00:00[0m
[[34m2024-05-22T06:23:44.097+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-31T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-30T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:23:44.097+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T06:23:44.097+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T06:23:44.097+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T06:23:44.098+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-31T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-30T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:23:44.100+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T06:23:44.100+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:23:44.100+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T06:23:44.100+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:23:44.101+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T06:23:44.101+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:23:44.102+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:23:45.555+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:23:46.220+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:25:09.740+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:25:11.218+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:25:11.570+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-07-31T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:25:13.434+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:25:14.860+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:25:15.300+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:30:18.191+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:30:18.191+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:30:18.191+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:30:18.222+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:25:15.415665+00:00, run_end_date=2024-05-22 06:30:17.444493+00:00, run_duration=302.028828, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=439, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 06:23:44.098746+00:00, queued_by_job_id=276, pid=92708[0m
[[34m2024-05-22T06:30:18.223+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:23:46.318511+00:00, run_end_date=2024-05-22 06:25:09.075893+00:00, run_duration=82.757382, state=success, executor_state=success, try_number=1, max_tries=2, job_id=437, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 06:23:44.098746+00:00, queued_by_job_id=276, pid=92607[0m
[[34m2024-05-22T06:30:18.223+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-07-31T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:25:11.668128+00:00, run_end_date=2024-05-22 06:25:12.800510+00:00, run_duration=1.132382, state=success, executor_state=success, try_number=1, max_tries=2, job_id=438, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 06:23:44.098746+00:00, queued_by_job_id=276, pid=92684[0m
[[34m2024-05-22T06:30:18.258+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T06:30:18.279+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-03 00:00:00+00:00, run_after=2023-08-04 00:00:00+00:00[0m
[[34m2024-05-22T06:30:18.367+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-26 00:00:00+00:00: scheduled__2023-07-26T00:00:00+00:00, state:running, queued_at: 2024-05-22 05:42:17.903711+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T06:30:18.367+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-26 00:00:00+00:00, run_id=scheduled__2023-07-26T00:00:00+00:00, run_start_date=2024-05-22 05:42:17.918398+00:00, run_end_date=2024-05-22 06:30:18.367507+00:00, run_duration=2880.449109, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-26 00:00:00+00:00, data_interval_end=2023-07-27 00:00:00+00:00, dag_hash=aa591f6e9c609c1f0313bdfbff680faa[0m
[[34m2024-05-22T06:30:18.370+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-27 00:00:00+00:00, run_after=2023-07-28 00:00:00+00:00[0m
[[34m2024-05-22T06:30:18.381+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-31T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:30:18.381+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T06:30:18.381+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T06:30:18.381+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T06:30:18.382+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-31T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:30:18.384+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T06:30:18.384+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:30:18.384+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T06:30:18.384+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:30:18.385+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T06:30:18.385+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:30:18.386+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:30:19.838+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:30:20.328+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:32:38.033+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:32:39.179+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:32:39.584+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:32:41.797+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:32:43.133+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:32:43.519+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-07-31T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:37:46.237+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:37:46.238+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:37:46.238+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-07-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:37:46.251+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:30:20.443008+00:00, run_end_date=2024-05-22 06:32:37.463533+00:00, run_duration=137.020525, state=success, executor_state=success, try_number=1, max_tries=2, job_id=440, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 06:30:18.382652+00:00, queued_by_job_id=276, pid=92907[0m
[[34m2024-05-22T06:37:46.252+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:32:39.710334+00:00, run_end_date=2024-05-22 06:32:41.164955+00:00, run_duration=1.454621, state=success, executor_state=success, try_number=1, max_tries=2, job_id=441, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 06:30:18.382652+00:00, queued_by_job_id=276, pid=93021[0m
[[34m2024-05-22T06:37:46.252+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-07-31T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:32:43.645051+00:00, run_end_date=2024-05-22 06:37:45.570581+00:00, run_duration=301.92553, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=442, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 06:30:18.382652+00:00, queued_by_job_id=276, pid=93045[0m
[[34m2024-05-22T06:37:46.286+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T06:37:46.305+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-04 00:00:00+00:00, run_after=2023-08-05 00:00:00+00:00[0m
[[34m2024-05-22T06:37:46.395+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-27 00:00:00+00:00: scheduled__2023-07-27T00:00:00+00:00, state:running, queued_at: 2024-05-22 05:49:43.020784+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T06:37:46.395+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-27 00:00:00+00:00, run_id=scheduled__2023-07-27T00:00:00+00:00, run_start_date=2024-05-22 05:49:43.039320+00:00, run_end_date=2024-05-22 06:37:46.395610+00:00, run_duration=2883.35629, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-27 00:00:00+00:00, data_interval_end=2023-07-28 00:00:00+00:00, dag_hash=5f4a633df2695fec55d87fd1c22edd24[0m
[[34m2024-05-22T06:37:46.398+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-28 00:00:00+00:00, run_after=2023-07-29 00:00:00+00:00[0m
[[34m2024-05-22T06:37:46.408+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:37:46.408+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T06:37:46.408+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T06:37:46.408+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T06:37:46.409+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:37:46.411+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T06:37:46.411+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:37:46.411+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T06:37:46.411+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:37:46.412+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T06:37:46.412+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:37:46.413+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:37:47.723+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:37:48.151+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:38:27.070+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:38:28.742+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:38:29.141+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:38:31.238+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:38:32.402+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:38:32.753+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:43:35.698+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:43:35.698+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:43:35.699+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:43:35.705+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:38:29.286526+00:00, run_end_date=2024-05-22 06:38:30.487436+00:00, run_duration=1.20091, state=success, executor_state=success, try_number=1, max_tries=2, job_id=444, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 06:37:46.409708+00:00, queued_by_job_id=276, pid=93292[0m
[[34m2024-05-22T06:43:35.705+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:38:32.908343+00:00, run_end_date=2024-05-22 06:43:34.916915+00:00, run_duration=302.008572, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=445, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 06:37:46.409708+00:00, queued_by_job_id=276, pid=93316[0m
[[34m2024-05-22T06:43:35.705+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:37:48.254629+00:00, run_end_date=2024-05-22 06:38:26.387995+00:00, run_duration=38.133366, state=success, executor_state=success, try_number=1, max_tries=2, job_id=443, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 06:37:46.409708+00:00, queued_by_job_id=276, pid=93249[0m
[[34m2024-05-22T06:43:35.738+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T06:43:35.756+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-05 00:00:00+00:00, run_after=2023-08-06 00:00:00+00:00[0m
[[34m2024-05-22T06:43:35.840+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-28 00:00:00+00:00: scheduled__2023-07-28T00:00:00+00:00, state:running, queued_at: 2024-05-22 05:57:38.595934+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T06:43:35.841+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-28 00:00:00+00:00, run_id=scheduled__2023-07-28T00:00:00+00:00, run_start_date=2024-05-22 05:57:38.611169+00:00, run_end_date=2024-05-22 06:43:35.841086+00:00, run_duration=2757.229917, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-28 00:00:00+00:00, data_interval_end=2023-07-29 00:00:00+00:00, dag_hash=ca7a3944b30b334b1e437e8490c0674f[0m
[[34m2024-05-22T06:43:35.844+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-29 00:00:00+00:00, run_after=2023-07-30 00:00:00+00:00[0m
[[34m2024-05-22T06:43:35.854+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-02T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:43:35.854+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T06:43:35.854+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T06:43:35.855+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T06:43:35.855+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-02T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:43:35.857+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T06:43:35.857+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:43:35.858+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T06:43:35.858+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:43:35.858+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T06:43:35.858+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:43:35.859+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:43:37.075+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:43:37.472+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:45:36.479+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:45:37.842+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:45:38.208+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:45:40.446+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:45:41.991+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:45:42.373+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:50:45.112+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:50:45.112+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:50:45.112+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T06:50:45.118+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:45:42.472490+00:00, run_end_date=2024-05-22 06:50:44.416078+00:00, run_duration=301.943588, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=448, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 06:43:35.855943+00:00, queued_by_job_id=276, pid=93875[0m
[[34m2024-05-22T06:50:45.118+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:43:37.568044+00:00, run_end_date=2024-05-22 06:45:35.843797+00:00, run_duration=118.275753, state=success, executor_state=success, try_number=1, max_tries=2, job_id=446, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 06:43:35.855943+00:00, queued_by_job_id=276, pid=93761[0m
[[34m2024-05-22T06:50:45.119+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:45:38.305876+00:00, run_end_date=2024-05-22 06:45:39.678882+00:00, run_duration=1.373006, state=success, executor_state=success, try_number=1, max_tries=2, job_id=447, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 06:43:35.855943+00:00, queued_by_job_id=276, pid=93851[0m
[[34m2024-05-22T06:50:45.155+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T06:50:45.182+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-06 00:00:00+00:00, run_after=2023-08-07 00:00:00+00:00[0m
[[34m2024-05-22T06:50:45.288+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-29 00:00:00+00:00: scheduled__2023-07-29T00:00:00+00:00, state:running, queued_at: 2024-05-22 06:03:51.521891+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T06:50:45.288+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-29 00:00:00+00:00, run_id=scheduled__2023-07-29T00:00:00+00:00, run_start_date=2024-05-22 06:03:51.539511+00:00, run_end_date=2024-05-22 06:50:45.288487+00:00, run_duration=2813.748976, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-29 00:00:00+00:00, data_interval_end=2023-07-30 00:00:00+00:00, dag_hash=ccc755c14ecc5b4887c934fd6fd83f47[0m
[[34m2024-05-22T06:50:45.292+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-30 00:00:00+00:00, run_after=2023-07-31 00:00:00+00:00[0m
[[34m2024-05-22T06:50:45.308+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:50:45.309+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T06:50:45.309+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T06:50:45.309+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T06:50:45.309+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T06:50:45.312+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T06:50:45.312+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:50:45.313+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T06:50:45.313+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:50:45.313+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T06:50:45.313+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:50:45.315+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:50:46.764+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:50:47.146+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:55:30.352+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:55:31.967+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:55:32.444+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T06:55:34.398+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T06:55:35.694+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T06:55:36.060+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:00:38.740+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:00:38.741+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:00:38.741+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:00:38.763+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:50:47.258530+00:00, run_end_date=2024-05-22 06:55:29.659146+00:00, run_duration=282.400616, state=success, executor_state=success, try_number=1, max_tries=2, job_id=449, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 06:50:45.310908+00:00, queued_by_job_id=276, pid=94074[0m
[[34m2024-05-22T07:00:38.764+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:55:32.596381+00:00, run_end_date=2024-05-22 06:55:33.754039+00:00, run_duration=1.157658, state=success, executor_state=success, try_number=1, max_tries=2, job_id=450, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 06:50:45.310908+00:00, queued_by_job_id=276, pid=94263[0m
[[34m2024-05-22T07:00:38.764+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 06:55:36.160419+00:00, run_end_date=2024-05-22 07:00:38.033649+00:00, run_duration=301.87323, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=451, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 06:50:45.310908+00:00, queued_by_job_id=276, pid=94287[0m
[[34m2024-05-22T07:00:38.816+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T07:00:38.852+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-07 00:00:00+00:00, run_after=2023-08-08 00:00:00+00:00[0m
[[34m2024-05-22T07:00:38.968+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-30 00:00:00+00:00: scheduled__2023-07-30T00:00:00+00:00, state:running, queued_at: 2024-05-22 06:09:51.027671+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T07:00:38.968+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-30 00:00:00+00:00, run_id=scheduled__2023-07-30T00:00:00+00:00, run_start_date=2024-05-22 06:09:51.044663+00:00, run_end_date=2024-05-22 07:00:38.968494+00:00, run_duration=3047.923831, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-30 00:00:00+00:00, data_interval_end=2023-07-31 00:00:00+00:00, dag_hash=37943905a0f0f95417c7544a46474bc6[0m
[[34m2024-05-22T07:00:38.972+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-07-31 00:00:00+00:00, run_after=2023-08-01 00:00:00+00:00[0m
[[34m2024-05-22T07:00:38.985+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:00:38.986+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T07:00:38.986+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T07:00:38.986+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T07:00:38.986+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:00:38.989+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T07:00:38.989+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:00:38.990+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T07:00:38.990+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:00:38.990+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T07:00:38.990+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:00:38.992+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:00:40.586+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:00:41.140+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:05:33.691+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:05:35.046+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:05:35.482+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:05:37.432+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:05:38.885+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:05:39.306+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:10:42.087+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:10:42.088+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:10:42.088+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:10:42.093+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:05:35.585522+00:00, run_end_date=2024-05-22 07:05:36.742678+00:00, run_duration=1.157156, state=success, executor_state=success, try_number=1, max_tries=2, job_id=453, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 07:00:38.987670+00:00, queued_by_job_id=276, pid=94681[0m
[[34m2024-05-22T07:10:42.093+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:00:41.242940+00:00, run_end_date=2024-05-22 07:05:33.030380+00:00, run_duration=291.78744, state=success, executor_state=success, try_number=1, max_tries=2, job_id=452, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 07:00:38.987670+00:00, queued_by_job_id=276, pid=94487[0m
[[34m2024-05-22T07:10:42.094+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:05:39.414088+00:00, run_end_date=2024-05-22 07:10:41.488346+00:00, run_duration=302.074258, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=454, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 07:00:38.987670+00:00, queued_by_job_id=276, pid=94705[0m
[[34m2024-05-22T07:10:42.130+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T07:10:42.152+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-08 00:00:00+00:00, run_after=2023-08-09 00:00:00+00:00[0m
[[34m2024-05-22T07:10:42.257+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-07-31 00:00:00+00:00: scheduled__2023-07-31T00:00:00+00:00, state:running, queued_at: 2024-05-22 06:15:44.591610+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T07:10:42.257+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-07-31 00:00:00+00:00, run_id=scheduled__2023-07-31T00:00:00+00:00, run_start_date=2024-05-22 06:15:44.604596+00:00, run_end_date=2024-05-22 07:10:42.257650+00:00, run_duration=3297.653054, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-07-31 00:00:00+00:00, data_interval_end=2023-08-01 00:00:00+00:00, dag_hash=e163f6de294c1631b4ce07656c55024e[0m
[[34m2024-05-22T07:10:42.261+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-01 00:00:00+00:00, run_after=2023-08-02 00:00:00+00:00[0m
[[34m2024-05-22T07:10:42.272+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:10:42.272+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T07:10:42.272+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T07:10:42.273+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T07:10:42.273+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:10:42.275+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T07:10:42.276+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:10:42.276+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T07:10:42.276+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:10:42.277+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T07:10:42.277+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:10:42.279+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:10:43.650+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:10:44.023+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:12:30.322+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:12:31.975+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:12:32.402+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:12:34.415+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:12:35.790+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:12:36.167+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:17:38.791+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:17:38.791+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:17:38.791+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:17:38.796+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:12:36.268751+00:00, run_end_date=2024-05-22 07:17:38.154485+00:00, run_duration=301.885734, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=457, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 07:10:42.274007+00:00, queued_by_job_id=276, pid=95397[0m
[[34m2024-05-22T07:17:38.796+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:10:44.123756+00:00, run_end_date=2024-05-22 07:12:29.657032+00:00, run_duration=105.533276, state=success, executor_state=success, try_number=1, max_tries=2, job_id=455, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 07:10:42.274007+00:00, queued_by_job_id=276, pid=95291[0m
[[34m2024-05-22T07:17:38.796+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:12:32.514998+00:00, run_end_date=2024-05-22 07:12:33.712739+00:00, run_duration=1.197741, state=success, executor_state=success, try_number=1, max_tries=2, job_id=456, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 07:10:42.274007+00:00, queued_by_job_id=276, pid=95373[0m
[[34m2024-05-22T07:17:38.829+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T07:17:38.849+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-09 00:00:00+00:00, run_after=2023-08-10 00:00:00+00:00[0m
[[34m2024-05-22T07:17:38.942+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-01 00:00:00+00:00: scheduled__2023-08-01T00:00:00+00:00, state:running, queued_at: 2024-05-22 06:23:43.898988+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T07:17:38.943+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-01 00:00:00+00:00, run_id=scheduled__2023-08-01T00:00:00+00:00, run_start_date=2024-05-22 06:23:43.911371+00:00, run_end_date=2024-05-22 07:17:38.943021+00:00, run_duration=3235.03165, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-01 00:00:00+00:00, data_interval_end=2023-08-02 00:00:00+00:00, dag_hash=340531841b82658bef75dfc30191a8fc[0m
[[34m2024-05-22T07:17:38.946+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-02 00:00:00+00:00, run_after=2023-08-03 00:00:00+00:00[0m
[[34m2024-05-22T07:17:38.957+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-06T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:17:38.957+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T07:17:38.958+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T07:17:38.958+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T07:17:38.958+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-06T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:17:38.960+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T07:17:38.961+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:17:38.961+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T07:17:38.961+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:17:38.962+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T07:17:38.962+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:17:38.963+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:17:40.282+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:17:40.653+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:18:32.261+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:18:33.965+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:18:34.475+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:18:36.570+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:18:37.848+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:18:38.239+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:23:40.917+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:23:40.918+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:23:40.918+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:23:40.923+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:17:40.756493+00:00, run_end_date=2024-05-22 07:18:31.618852+00:00, run_duration=50.862359, state=success, executor_state=success, try_number=1, max_tries=2, job_id=458, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 07:17:38.959421+00:00, queued_by_job_id=276, pid=95600[0m
[[34m2024-05-22T07:23:40.923+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:18:34.619054+00:00, run_end_date=2024-05-22 07:18:35.922323+00:00, run_duration=1.303269, state=success, executor_state=success, try_number=1, max_tries=2, job_id=459, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 07:17:38.959421+00:00, queued_by_job_id=276, pid=95651[0m
[[34m2024-05-22T07:23:40.924+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:18:38.343453+00:00, run_end_date=2024-05-22 07:23:40.318753+00:00, run_duration=301.9753, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=460, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 07:17:38.959421+00:00, queued_by_job_id=276, pid=95675[0m
[[34m2024-05-22T07:23:40.960+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T07:23:40.983+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-10 00:00:00+00:00, run_after=2023-08-11 00:00:00+00:00[0m
[[34m2024-05-22T07:23:41.198+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-02 00:00:00+00:00: scheduled__2023-08-02T00:00:00+00:00, state:running, queued_at: 2024-05-22 06:30:18.271789+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T07:23:41.198+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-02 00:00:00+00:00, run_id=scheduled__2023-08-02T00:00:00+00:00, run_start_date=2024-05-22 06:30:18.286127+00:00, run_end_date=2024-05-22 07:23:41.198878+00:00, run_duration=3202.912751, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-02 00:00:00+00:00, data_interval_end=2023-08-03 00:00:00+00:00, dag_hash=f6169325cb4342ec77527336d257a2bf[0m
[[34m2024-05-22T07:23:41.202+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-03 00:00:00+00:00, run_after=2023-08-04 00:00:00+00:00[0m
[[34m2024-05-22T07:23:41.213+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-07T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:23:41.213+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T07:23:41.214+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T07:23:41.214+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T07:23:41.214+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-07T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:23:41.217+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T07:23:41.217+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:23:41.218+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T07:23:41.218+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:23:41.218+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T07:23:41.219+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:23:41.220+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:23:42.412+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:23:42.766+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:24:48.765+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:24:49.933+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:24:50.290+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:24:52.199+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:24:53.401+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:24:53.889+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:29:56.586+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:29:56.586+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:29:56.586+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:29:56.591+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:23:42.862399+00:00, run_end_date=2024-05-22 07:24:48.155423+00:00, run_duration=65.293024, state=success, executor_state=success, try_number=1, max_tries=2, job_id=461, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 07:23:41.215656+00:00, queued_by_job_id=276, pid=95874[0m
[[34m2024-05-22T07:29:56.592+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:24:50.386587+00:00, run_end_date=2024-05-22 07:24:51.635888+00:00, run_duration=1.249301, state=success, executor_state=success, try_number=1, max_tries=2, job_id=462, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 07:23:41.215656+00:00, queued_by_job_id=276, pid=95930[0m
[[34m2024-05-22T07:29:56.592+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:24:54.039440+00:00, run_end_date=2024-05-22 07:29:55.913496+00:00, run_duration=301.874056, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=463, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 07:23:41.215656+00:00, queued_by_job_id=276, pid=95954[0m
[[34m2024-05-22T07:29:56.633+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T07:29:56.659+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-11 00:00:00+00:00, run_after=2023-08-12 00:00:00+00:00[0m
[[34m2024-05-22T07:29:56.759+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-03 00:00:00+00:00: scheduled__2023-08-03T00:00:00+00:00, state:running, queued_at: 2024-05-22 06:37:46.299226+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T07:29:56.759+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-03 00:00:00+00:00, run_id=scheduled__2023-08-03T00:00:00+00:00, run_start_date=2024-05-22 06:37:46.312241+00:00, run_end_date=2024-05-22 07:29:56.759741+00:00, run_duration=3130.4475, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-03 00:00:00+00:00, data_interval_end=2023-08-04 00:00:00+00:00, dag_hash=fb3b748337b9fbde7eb2929ec4687d8b[0m
[[34m2024-05-22T07:29:56.763+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-04 00:00:00+00:00, run_after=2023-08-05 00:00:00+00:00[0m
[[34m2024-05-22T07:29:56.775+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-08T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:29:56.776+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T07:29:56.776+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T07:29:56.776+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T07:29:56.776+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-08T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:29:56.779+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T07:29:56.779+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:29:56.780+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T07:29:56.780+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:29:56.780+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T07:29:56.780+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:29:56.782+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:29:58.208+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:29:58.708+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:30:49.611+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:30:50.993+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:30:51.514+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:30:53.410+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:30:54.659+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:30:55.041+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:35:57.814+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:35:57.814+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:35:57.815+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:35:57.835+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:29:58.838105+00:00, run_end_date=2024-05-22 07:30:48.940860+00:00, run_duration=50.102755, state=success, executor_state=success, try_number=1, max_tries=2, job_id=464, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 07:29:56.777525+00:00, queued_by_job_id=276, pid=96154[0m
[[34m2024-05-22T07:35:57.835+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:30:51.620730+00:00, run_end_date=2024-05-22 07:30:52.799451+00:00, run_duration=1.178721, state=success, executor_state=success, try_number=1, max_tries=2, job_id=465, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 07:29:56.777525+00:00, queued_by_job_id=276, pid=96206[0m
[[34m2024-05-22T07:35:57.835+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:30:55.147292+00:00, run_end_date=2024-05-22 07:35:57.118016+00:00, run_duration=301.970724, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=466, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 07:29:56.777525+00:00, queued_by_job_id=276, pid=96230[0m
[[34m2024-05-22T07:35:57.871+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T07:35:57.893+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-12 00:00:00+00:00, run_after=2023-08-13 00:00:00+00:00[0m
[[34m2024-05-22T07:35:57.990+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-04 00:00:00+00:00: scheduled__2023-08-04T00:00:00+00:00, state:running, queued_at: 2024-05-22 06:43:35.750888+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T07:35:57.990+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-04 00:00:00+00:00, run_id=scheduled__2023-08-04T00:00:00+00:00, run_start_date=2024-05-22 06:43:35.762711+00:00, run_end_date=2024-05-22 07:35:57.990862+00:00, run_duration=3142.228151, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-04 00:00:00+00:00, data_interval_end=2023-08-05 00:00:00+00:00, dag_hash=e8c168f81c81ca54f64c963ac5fb901c[0m
[[34m2024-05-22T07:35:57.994+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-05 00:00:00+00:00, run_after=2023-08-06 00:00:00+00:00[0m
[[34m2024-05-22T07:35:58.009+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-09T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:35:58.009+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T07:35:58.010+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T07:35:58.010+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T07:35:58.010+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-09T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:35:58.013+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T07:35:58.013+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:35:58.014+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T07:35:58.014+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:35:58.015+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T07:35:58.015+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:35:58.017+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:35:59.455+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:35:59.951+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:41:14.928+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:41:16.303+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:41:16.680+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:41:18.621+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:41:19.827+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:41:20.177+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:46:23.021+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:46:23.021+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:46:23.022+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:46:23.027+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:41:16.782294+00:00, run_end_date=2024-05-22 07:41:17.956950+00:00, run_duration=1.174656, state=success, executor_state=success, try_number=1, max_tries=2, job_id=468, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 07:35:58.011602+00:00, queued_by_job_id=276, pid=96635[0m
[[34m2024-05-22T07:46:23.027+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:41:20.273360+00:00, run_end_date=2024-05-22 07:46:22.323695+00:00, run_duration=302.050335, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=469, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 07:35:58.011602+00:00, queued_by_job_id=276, pid=96659[0m
[[34m2024-05-22T07:46:23.028+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:36:00.120027+00:00, run_end_date=2024-05-22 07:41:14.210550+00:00, run_duration=314.090523, state=success, executor_state=success, try_number=1, max_tries=2, job_id=467, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 07:35:58.011602+00:00, queued_by_job_id=276, pid=96433[0m
[[34m2024-05-22T07:46:23.064+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T07:46:23.086+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-13 00:00:00+00:00, run_after=2023-08-14 00:00:00+00:00[0m
[[34m2024-05-22T07:46:23.180+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-05 00:00:00+00:00: scheduled__2023-08-05T00:00:00+00:00, state:running, queued_at: 2024-05-22 06:50:45.173068+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T07:46:23.180+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-05 00:00:00+00:00, run_id=scheduled__2023-08-05T00:00:00+00:00, run_start_date=2024-05-22 06:50:45.190026+00:00, run_end_date=2024-05-22 07:46:23.180460+00:00, run_duration=3337.990434, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-05 00:00:00+00:00, data_interval_end=2023-08-06 00:00:00+00:00, dag_hash=c0c469fdbb2d43c91fc0b350a571a3db[0m
[[34m2024-05-22T07:46:23.184+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-06 00:00:00+00:00, run_after=2023-08-07 00:00:00+00:00[0m
[[34m2024-05-22T07:46:23.195+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-10T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:46:23.195+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T07:46:23.196+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T07:46:23.196+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T07:46:23.196+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-10T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:46:23.199+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T07:46:23.199+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:46:23.199+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T07:46:23.200+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:46:23.200+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T07:46:23.200+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:46:23.202+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:46:24.540+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:46:24.937+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:50:15.686+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:50:17.194+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:50:17.621+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:50:19.976+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:50:21.391+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:50:21.821+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:55:24.753+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:55:24.753+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:55:24.754+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T07:55:24.776+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:50:21.932481+00:00, run_end_date=2024-05-22 07:55:24.079462+00:00, run_duration=302.146981, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=472, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 07:46:23.197286+00:00, queued_by_job_id=276, pid=97041[0m
[[34m2024-05-22T07:55:24.777+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:46:25.043043+00:00, run_end_date=2024-05-22 07:50:15.073012+00:00, run_duration=230.029969, state=success, executor_state=success, try_number=1, max_tries=2, job_id=470, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 07:46:23.197286+00:00, queued_by_job_id=276, pid=96858[0m
[[34m2024-05-22T07:55:24.778+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:50:17.758664+00:00, run_end_date=2024-05-22 07:50:19.152506+00:00, run_duration=1.393842, state=success, executor_state=success, try_number=1, max_tries=2, job_id=471, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 07:46:23.197286+00:00, queued_by_job_id=276, pid=97017[0m
[[34m2024-05-22T07:55:24.819+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T07:55:24.839+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-14 00:00:00+00:00, run_after=2023-08-15 00:00:00+00:00[0m
[[34m2024-05-22T07:55:24.928+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-06 00:00:00+00:00: scheduled__2023-08-06T00:00:00+00:00, state:running, queued_at: 2024-05-22 07:00:38.841709+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T07:55:24.929+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-06 00:00:00+00:00, run_id=scheduled__2023-08-06T00:00:00+00:00, run_start_date=2024-05-22 07:00:38.859852+00:00, run_end_date=2024-05-22 07:55:24.929249+00:00, run_duration=3286.069397, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-06 00:00:00+00:00, data_interval_end=2023-08-07 00:00:00+00:00, dag_hash=394eb96a69674ed1e13d43581daa6c9b[0m
[[34m2024-05-22T07:55:24.932+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-07 00:00:00+00:00, run_after=2023-08-08 00:00:00+00:00[0m
[[34m2024-05-22T07:55:24.943+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-11T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:55:24.943+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T07:55:24.944+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T07:55:24.944+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T07:55:24.944+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-11T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T07:55:24.947+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T07:55:24.947+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:55:24.947+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T07:55:24.947+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:55:24.948+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T07:55:24.948+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:55:24.949+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:55:26.192+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:55:26.588+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:56:57.580+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:56:59.127+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:56:59.636+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T07:57:01.957+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T07:57:03.490+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T07:57:03.936+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:02:06.669+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:02:06.670+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:02:06.670+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:02:06.677+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:55:26.705672+00:00, run_end_date=2024-05-22 07:56:56.938838+00:00, run_duration=90.233166, state=success, executor_state=success, try_number=1, max_tries=2, job_id=473, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 07:55:24.945619+00:00, queued_by_job_id=276, pid=97241[0m
[[34m2024-05-22T08:02:06.678+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:56:59.735272+00:00, run_end_date=2024-05-22 07:57:01.004764+00:00, run_duration=1.269492, state=success, executor_state=success, try_number=1, max_tries=2, job_id=474, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 07:55:24.945619+00:00, queued_by_job_id=276, pid=97312[0m
[[34m2024-05-22T08:02:06.678+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 07:57:04.038355+00:00, run_end_date=2024-05-22 08:02:05.991824+00:00, run_duration=301.953469, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=475, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 07:55:24.945619+00:00, queued_by_job_id=276, pid=97337[0m
[[34m2024-05-22T08:02:06.723+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T08:02:06.746+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-15 00:00:00+00:00, run_after=2023-08-16 00:00:00+00:00[0m
[[34m2024-05-22T08:02:06.840+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-07 00:00:00+00:00: scheduled__2023-08-07T00:00:00+00:00, state:running, queued_at: 2024-05-22 07:10:42.144839+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T08:02:06.841+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-07 00:00:00+00:00, run_id=scheduled__2023-08-07T00:00:00+00:00, run_start_date=2024-05-22 07:10:42.160692+00:00, run_end_date=2024-05-22 08:02:06.841196+00:00, run_duration=3084.680504, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-07 00:00:00+00:00, data_interval_end=2023-08-08 00:00:00+00:00, dag_hash=897cd53f50573a35547a0727215e9424[0m
[[34m2024-05-22T08:02:06.844+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-08 00:00:00+00:00, run_after=2023-08-09 00:00:00+00:00[0m
[[34m2024-05-22T08:02:06.855+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-12T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:02:06.855+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T08:02:06.855+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T08:02:06.855+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T08:02:06.856+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-12T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:02:06.858+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T08:02:06.858+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:02:06.858+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T08:02:06.859+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:02:06.859+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T08:02:06.859+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:02:06.861+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:02:08.309+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:02:08.690+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:07:05.099+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:07:06.580+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:07:06.957+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:07:08.944+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:07:10.285+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:07:10.645+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:12:13.492+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:12:13.493+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:12:13.493+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:12:13.533+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:07:07.057889+00:00, run_end_date=2024-05-22 08:07:08.314313+00:00, run_duration=1.256424, state=success, executor_state=success, try_number=1, max_tries=2, job_id=477, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 08:02:06.856808+00:00, queued_by_job_id=276, pid=97735[0m
[[34m2024-05-22T08:12:13.534+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:02:08.793436+00:00, run_end_date=2024-05-22 08:07:04.340058+00:00, run_duration=295.546622, state=success, executor_state=success, try_number=1, max_tries=2, job_id=476, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 08:02:06.856808+00:00, queued_by_job_id=276, pid=97538[0m
[[34m2024-05-22T08:12:13.534+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:07:10.742925+00:00, run_end_date=2024-05-22 08:12:12.893569+00:00, run_duration=302.150644, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=478, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 08:02:06.856808+00:00, queued_by_job_id=276, pid=97760[0m
[[34m2024-05-22T08:12:13.568+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T08:12:13.588+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-16 00:00:00+00:00, run_after=2023-08-17 00:00:00+00:00[0m
[[34m2024-05-22T08:12:13.685+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-08 00:00:00+00:00: scheduled__2023-08-08T00:00:00+00:00, state:running, queued_at: 2024-05-22 07:17:38.843483+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T08:12:13.685+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-08 00:00:00+00:00, run_id=scheduled__2023-08-08T00:00:00+00:00, run_start_date=2024-05-22 07:17:38.857198+00:00, run_end_date=2024-05-22 08:12:13.685648+00:00, run_duration=3274.82845, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-08 00:00:00+00:00, data_interval_end=2023-08-09 00:00:00+00:00, dag_hash=91866e5a2c630dd28d6fd4f2bda02131[0m
[[34m2024-05-22T08:12:13.689+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-09 00:00:00+00:00, run_after=2023-08-10 00:00:00+00:00[0m
[[34m2024-05-22T08:12:13.700+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:12:13.700+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T08:12:13.700+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T08:12:13.700+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T08:12:13.701+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:12:13.703+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T08:12:13.704+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:12:13.704+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T08:12:13.704+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:12:13.705+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T08:12:13.705+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:12:13.707+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:12:15.175+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:12:15.618+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:18:04.338+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:18:05.631+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:18:06.079+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:18:08.132+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:18:09.403+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:18:09.812+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:23:12.834+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:23:12.834+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:23:12.835+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:23:12.842+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:18:09.913555+00:00, run_end_date=2024-05-22 08:23:12.029525+00:00, run_duration=302.11597, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=481, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 08:12:13.702000+00:00, queued_by_job_id=276, pid=98239[0m
[[34m2024-05-22T08:23:12.843+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:18:06.178493+00:00, run_end_date=2024-05-22 08:18:07.410565+00:00, run_duration=1.232072, state=success, executor_state=success, try_number=1, max_tries=2, job_id=480, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 08:12:13.702000+00:00, queued_by_job_id=276, pid=98214[0m
[[34m2024-05-22T08:23:12.843+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:12:15.742638+00:00, run_end_date=2024-05-22 08:18:03.732028+00:00, run_duration=347.98939, state=success, executor_state=success, try_number=1, max_tries=2, job_id=479, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 08:12:13.702000+00:00, queued_by_job_id=276, pid=97975[0m
[[34m2024-05-22T08:23:12.894+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T08:23:12.926+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-17 00:00:00+00:00, run_after=2023-08-18 00:00:00+00:00[0m
[[34m2024-05-22T08:23:13.038+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-09 00:00:00+00:00: scheduled__2023-08-09T00:00:00+00:00, state:running, queued_at: 2024-05-22 07:23:40.975121+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T08:23:13.039+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-09 00:00:00+00:00, run_id=scheduled__2023-08-09T00:00:00+00:00, run_start_date=2024-05-22 07:23:40.995508+00:00, run_end_date=2024-05-22 08:23:13.039047+00:00, run_duration=3572.043539, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-09 00:00:00+00:00, data_interval_end=2023-08-10 00:00:00+00:00, dag_hash=c70626acdd67a3a9a3b886dea89d23d8[0m
[[34m2024-05-22T08:23:13.042+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-10 00:00:00+00:00, run_after=2023-08-11 00:00:00+00:00[0m
[[34m2024-05-22T08:23:13.059+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-14T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:23:13.059+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T08:23:13.060+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T08:23:13.060+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T08:23:13.060+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-14T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:23:13.064+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T08:23:13.065+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:23:13.065+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T08:23:13.065+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:23:13.066+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T08:23:13.066+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:23:13.068+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:23:14.612+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:23:15.052+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:26:07.219+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:26:08.418+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:26:08.806+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:26:11.043+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:26:12.688+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:26:13.124+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:31:15.979+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:31:15.980+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:31:15.980+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:31:15.985+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:26:13.239327+00:00, run_end_date=2024-05-22 08:31:15.285883+00:00, run_duration=302.046556, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=484, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 08:23:13.061484+00:00, queued_by_job_id=276, pid=98584[0m
[[34m2024-05-22T08:31:15.986+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:26:08.903802+00:00, run_end_date=2024-05-22 08:26:10.310379+00:00, run_duration=1.406577, state=success, executor_state=success, try_number=1, max_tries=2, job_id=483, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 08:23:13.061484+00:00, queued_by_job_id=276, pid=98560[0m
[[34m2024-05-22T08:31:15.986+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:23:15.195996+00:00, run_end_date=2024-05-22 08:26:06.661039+00:00, run_duration=171.465043, state=success, executor_state=success, try_number=1, max_tries=2, job_id=482, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 08:23:13.061484+00:00, queued_by_job_id=276, pid=98436[0m
[[34m2024-05-22T08:31:16.051+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T08:31:16.082+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-18 00:00:00+00:00, run_after=2023-08-19 00:00:00+00:00[0m
[[34m2024-05-22T08:31:16.242+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-10 00:00:00+00:00: scheduled__2023-08-10T00:00:00+00:00, state:running, queued_at: 2024-05-22 07:29:56.652337+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T08:31:16.243+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-10 00:00:00+00:00, run_id=scheduled__2023-08-10T00:00:00+00:00, run_start_date=2024-05-22 07:29:56.666672+00:00, run_end_date=2024-05-22 08:31:16.243182+00:00, run_duration=3679.57651, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-10 00:00:00+00:00, data_interval_end=2023-08-11 00:00:00+00:00, dag_hash=b59c19f9cc500232e73754b16d60d52c[0m
[[34m2024-05-22T08:31:16.251+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-11 00:00:00+00:00, run_after=2023-08-12 00:00:00+00:00[0m
[[34m2024-05-22T08:31:16.268+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:31:16.269+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T08:31:16.269+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T08:31:16.269+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T08:31:16.269+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:31:16.272+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T08:31:16.272+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:31:16.273+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T08:31:16.273+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:31:16.274+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T08:31:16.274+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:31:16.275+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:31:17.710+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:31:18.116+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:33:21.675+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:33:22.897+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:33:23.252+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:33:25.159+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:33:26.838+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:33:27.350+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:38:30.178+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:38:30.178+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:38:30.178+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:38:30.183+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:33:27.477872+00:00, run_end_date=2024-05-22 08:38:29.574971+00:00, run_duration=302.097099, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=487, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 08:31:16.270576+00:00, queued_by_job_id=276, pid=98902[0m
[[34m2024-05-22T08:38:30.184+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:31:18.276509+00:00, run_end_date=2024-05-22 08:33:20.834531+00:00, run_duration=122.558022, state=success, executor_state=success, try_number=1, max_tries=2, job_id=485, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 08:31:16.270576+00:00, queued_by_job_id=276, pid=98785[0m
[[34m2024-05-22T08:38:30.184+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:33:23.351013+00:00, run_end_date=2024-05-22 08:33:24.476539+00:00, run_duration=1.125526, state=success, executor_state=success, try_number=1, max_tries=2, job_id=486, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 08:31:16.270576+00:00, queued_by_job_id=276, pid=98878[0m
[[34m2024-05-22T08:38:30.216+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T08:38:30.236+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-19 00:00:00+00:00, run_after=2023-08-20 00:00:00+00:00[0m
[[34m2024-05-22T08:38:30.322+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-11 00:00:00+00:00: scheduled__2023-08-11T00:00:00+00:00, state:running, queued_at: 2024-05-22 07:35:57.885321+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T08:38:30.323+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-11 00:00:00+00:00, run_id=scheduled__2023-08-11T00:00:00+00:00, run_start_date=2024-05-22 07:35:57.901963+00:00, run_end_date=2024-05-22 08:38:30.323073+00:00, run_duration=3752.42111, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-11 00:00:00+00:00, data_interval_end=2023-08-12 00:00:00+00:00, dag_hash=1133d1955f5f0534d15692421d446375[0m
[[34m2024-05-22T08:38:30.326+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-12 00:00:00+00:00, run_after=2023-08-13 00:00:00+00:00[0m
[[34m2024-05-22T08:38:30.336+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:38:30.336+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T08:38:30.336+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T08:38:30.337+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T08:38:30.337+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:38:30.339+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T08:38:30.339+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:38:30.340+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T08:38:30.340+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:38:30.340+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T08:38:30.341+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:38:30.342+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:38:31.879+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:38:32.305+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:41:35.596+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:41:36.972+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:41:37.381+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:41:39.579+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:41:40.858+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:41:41.217+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:46:44.118+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:46:44.118+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:46:44.118+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:46:44.137+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:38:32.492294+00:00, run_end_date=2024-05-22 08:41:34.870076+00:00, run_duration=182.377782, state=success, executor_state=success, try_number=1, max_tries=2, job_id=488, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 08:38:30.338130+00:00, queued_by_job_id=276, pid=99101[0m
[[34m2024-05-22T08:46:44.138+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:41:37.526277+00:00, run_end_date=2024-05-22 08:41:38.946670+00:00, run_duration=1.420393, state=success, executor_state=success, try_number=1, max_tries=2, job_id=489, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 08:38:30.338130+00:00, queued_by_job_id=276, pid=99228[0m
[[34m2024-05-22T08:46:44.138+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:41:41.363168+00:00, run_end_date=2024-05-22 08:46:43.483085+00:00, run_duration=302.119917, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=490, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 08:38:30.338130+00:00, queued_by_job_id=276, pid=99252[0m
[[34m2024-05-22T08:46:44.173+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T08:46:44.194+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-20 00:00:00+00:00, run_after=2023-08-21 00:00:00+00:00[0m
[[34m2024-05-22T08:46:44.287+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-12 00:00:00+00:00: scheduled__2023-08-12T00:00:00+00:00, state:running, queued_at: 2024-05-22 07:46:23.078133+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T08:46:44.287+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-12 00:00:00+00:00, run_id=scheduled__2023-08-12T00:00:00+00:00, run_start_date=2024-05-22 07:46:23.094144+00:00, run_end_date=2024-05-22 08:46:44.287604+00:00, run_duration=3621.19346, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-12 00:00:00+00:00, data_interval_end=2023-08-13 00:00:00+00:00, dag_hash=03c42bb97a99829c09db93266c816af5[0m
[[34m2024-05-22T08:46:44.291+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-13 00:00:00+00:00, run_after=2023-08-14 00:00:00+00:00[0m
[[34m2024-05-22T08:46:44.301+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:46:44.301+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T08:46:44.301+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T08:46:44.301+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T08:46:44.302+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:46:44.304+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T08:46:44.304+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:46:44.305+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T08:46:44.305+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:46:44.305+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T08:46:44.305+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:46:44.307+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:46:45.492+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:46:45.836+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:51:35.168+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:51:36.549+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:51:36.942+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:51:38.891+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:51:40.267+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:51:40.633+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:56:43.648+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:56:43.649+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:56:43.649+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T08:56:43.660+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:51:37.050262+00:00, run_end_date=2024-05-22 08:51:38.242632+00:00, run_duration=1.19237, state=success, executor_state=success, try_number=1, max_tries=2, job_id=492, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 08:46:44.302877+00:00, queued_by_job_id=276, pid=99643[0m
[[34m2024-05-22T08:56:43.661+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:46:45.928697+00:00, run_end_date=2024-05-22 08:51:34.564121+00:00, run_duration=288.635424, state=success, executor_state=success, try_number=1, max_tries=2, job_id=491, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 08:46:44.302877+00:00, queued_by_job_id=276, pid=99453[0m
[[34m2024-05-22T08:56:43.662+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:51:40.735616+00:00, run_end_date=2024-05-22 08:56:42.981302+00:00, run_duration=302.245686, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=493, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 08:46:44.302877+00:00, queued_by_job_id=276, pid=99667[0m
[[34m2024-05-22T08:56:43.698+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T08:56:43.719+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-21 00:00:00+00:00, run_after=2023-08-22 00:00:00+00:00[0m
[[34m2024-05-22T08:56:43.815+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-13 00:00:00+00:00: scheduled__2023-08-13T00:00:00+00:00, state:running, queued_at: 2024-05-22 07:55:24.832888+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T08:56:43.815+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-13 00:00:00+00:00, run_id=scheduled__2023-08-13T00:00:00+00:00, run_start_date=2024-05-22 07:55:24.846425+00:00, run_end_date=2024-05-22 08:56:43.815853+00:00, run_duration=3678.969428, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-13 00:00:00+00:00, data_interval_end=2023-08-14 00:00:00+00:00, dag_hash=5c059052d6ff1cdca4aa768722986197[0m
[[34m2024-05-22T08:56:43.819+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-14 00:00:00+00:00, run_after=2023-08-15 00:00:00+00:00[0m
[[34m2024-05-22T08:56:43.829+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-18T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:56:43.829+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T08:56:43.829+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T08:56:43.830+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T08:56:43.830+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-18T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T08:56:43.832+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T08:56:43.832+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:56:43.833+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T08:56:43.833+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:56:43.834+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T08:56:43.834+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:56:43.835+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:56:45.070+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:56:45.500+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:57:35.865+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:57:37.224+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:57:37.716+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T08:57:39.693+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T08:57:40.996+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T08:57:41.378+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:02:44.251+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:02:44.251+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:02:44.251+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:02:44.256+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:56:45.605307+00:00, run_end_date=2024-05-22 08:57:35.208900+00:00, run_duration=49.603593, state=success, executor_state=success, try_number=1, max_tries=2, job_id=494, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 08:56:43.831285+00:00, queued_by_job_id=276, pid=99864[0m
[[34m2024-05-22T09:02:44.257+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:57:37.837082+00:00, run_end_date=2024-05-22 08:57:39.032499+00:00, run_duration=1.195417, state=success, executor_state=success, try_number=1, max_tries=2, job_id=495, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 08:56:43.831285+00:00, queued_by_job_id=276, pid=99912[0m
[[34m2024-05-22T09:02:44.257+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 08:57:41.568854+00:00, run_end_date=2024-05-22 09:02:43.647435+00:00, run_duration=302.078581, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=496, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 08:56:43.831285+00:00, queued_by_job_id=276, pid=99936[0m
[[34m2024-05-22T09:02:44.292+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T09:02:44.312+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-22 00:00:00+00:00, run_after=2023-08-23 00:00:00+00:00[0m
[[34m2024-05-22T09:02:44.404+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-14 00:00:00+00:00: scheduled__2023-08-14T00:00:00+00:00, state:running, queued_at: 2024-05-22 08:02:06.738848+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T09:02:44.405+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-14 00:00:00+00:00, run_id=scheduled__2023-08-14T00:00:00+00:00, run_start_date=2024-05-22 08:02:06.754200+00:00, run_end_date=2024-05-22 09:02:44.405177+00:00, run_duration=3637.650977, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-14 00:00:00+00:00, data_interval_end=2023-08-15 00:00:00+00:00, dag_hash=b5a2c697f635eef85b985be5e7596c6e[0m
[[34m2024-05-22T09:02:44.408+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-15 00:00:00+00:00, run_after=2023-08-16 00:00:00+00:00[0m
[[34m2024-05-22T09:02:44.419+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-19T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:02:44.419+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T09:02:44.419+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T09:02:44.420+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T09:02:44.420+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-19T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:02:44.423+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T09:02:44.423+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:02:44.423+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T09:02:44.423+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:02:44.424+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T09:02:44.424+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:02:44.426+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:02:45.616+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:02:46.020+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:03:36.475+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:03:37.944+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:03:38.305+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:03:40.545+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:03:42.034+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:03:42.406+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:08:45.048+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:08:45.048+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:08:45.049+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:08:45.062+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:03:38.425620+00:00, run_end_date=2024-05-22 09:03:39.854158+00:00, run_duration=1.428538, state=success, executor_state=success, try_number=1, max_tries=2, job_id=498, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 09:02:44.421066+00:00, queued_by_job_id=276, pid=100185[0m
[[34m2024-05-22T09:08:45.062+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:02:46.139932+00:00, run_end_date=2024-05-22 09:03:35.813894+00:00, run_duration=49.673962, state=success, executor_state=success, try_number=1, max_tries=2, job_id=497, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 09:02:44.421066+00:00, queued_by_job_id=276, pid=100136[0m
[[34m2024-05-22T09:08:45.062+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:03:42.505062+00:00, run_end_date=2024-05-22 09:08:44.424899+00:00, run_duration=301.919837, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=499, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 09:02:44.421066+00:00, queued_by_job_id=276, pid=100209[0m
[[34m2024-05-22T09:08:45.109+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T09:08:45.142+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-23 00:00:00+00:00, run_after=2023-08-24 00:00:00+00:00[0m
[[34m2024-05-22T09:08:45.239+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-15 00:00:00+00:00: scheduled__2023-08-15T00:00:00+00:00, state:running, queued_at: 2024-05-22 08:12:13.581329+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T09:08:45.240+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-15 00:00:00+00:00, run_id=scheduled__2023-08-15T00:00:00+00:00, run_start_date=2024-05-22 08:12:13.595711+00:00, run_end_date=2024-05-22 09:08:45.240263+00:00, run_duration=3391.644552, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-15 00:00:00+00:00, data_interval_end=2023-08-16 00:00:00+00:00, dag_hash=6230703ec3bce833ce744df0721e0f37[0m
[[34m2024-05-22T09:08:45.243+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-16 00:00:00+00:00, run_after=2023-08-17 00:00:00+00:00[0m
[[34m2024-05-22T09:08:45.264+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-20T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:08:45.265+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T09:08:45.265+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T09:08:45.266+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T09:08:45.266+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-20T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:08:45.270+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T09:08:45.271+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:08:45.272+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T09:08:45.272+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:08:45.273+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T09:08:45.274+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:08:45.276+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:08:46.721+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:08:47.226+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:10:01.889+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:10:03.510+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:10:03.876+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:10:06.165+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:10:07.584+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:10:07.993+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:15:10.834+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:15:10.835+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:15:10.835+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:15:10.842+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:10:08.093327+00:00, run_end_date=2024-05-22 09:15:10.124832+00:00, run_duration=302.031505, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=502, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 09:08:45.268171+00:00, queued_by_job_id=276, pid=100494[0m
[[34m2024-05-22T09:15:10.842+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:08:47.343856+00:00, run_end_date=2024-05-22 09:10:00.826581+00:00, run_duration=73.482725, state=success, executor_state=success, try_number=1, max_tries=2, job_id=500, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 09:08:45.268171+00:00, queued_by_job_id=276, pid=100406[0m
[[34m2024-05-22T09:15:10.843+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:10:03.974236+00:00, run_end_date=2024-05-22 09:10:05.462650+00:00, run_duration=1.488414, state=success, executor_state=success, try_number=1, max_tries=2, job_id=501, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 09:08:45.268171+00:00, queued_by_job_id=276, pid=100470[0m
[[34m2024-05-22T09:15:10.887+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T09:15:10.910+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-24 00:00:00+00:00, run_after=2023-08-25 00:00:00+00:00[0m
[[34m2024-05-22T09:15:11.032+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-16 00:00:00+00:00: scheduled__2023-08-16T00:00:00+00:00, state:running, queued_at: 2024-05-22 08:23:12.920415+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T09:15:11.033+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-16 00:00:00+00:00, run_id=scheduled__2023-08-16T00:00:00+00:00, run_start_date=2024-05-22 08:23:12.940306+00:00, run_end_date=2024-05-22 09:15:11.033126+00:00, run_duration=3118.09282, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-16 00:00:00+00:00, data_interval_end=2023-08-17 00:00:00+00:00, dag_hash=884badf5833e83c37eb2b5ddd85efd5a[0m
[[34m2024-05-22T09:15:11.037+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-17 00:00:00+00:00, run_after=2023-08-18 00:00:00+00:00[0m
[[34m2024-05-22T09:15:11.048+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:15:11.048+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T09:15:11.049+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T09:15:11.049+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T09:15:11.050+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:15:11.053+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T09:15:11.053+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:15:11.054+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T09:15:11.054+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:15:11.054+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T09:15:11.055+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:15:11.056+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:15:12.586+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:15:13.245+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:16:12.401+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:16:13.996+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:16:14.394+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:16:16.195+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:16:17.456+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:16:17.955+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:21:21.125+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:21:21.126+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:21:21.126+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:21:21.163+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:15:13.375052+00:00, run_end_date=2024-05-22 09:16:11.754035+00:00, run_duration=58.378983, state=success, executor_state=success, try_number=1, max_tries=2, job_id=503, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 09:15:11.051065+00:00, queued_by_job_id=276, pid=100697[0m
[[34m2024-05-22T09:21:21.163+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:16:14.495178+00:00, run_end_date=2024-05-22 09:16:15.622789+00:00, run_duration=1.127611, state=success, executor_state=success, try_number=1, max_tries=2, job_id=504, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 09:15:11.051065+00:00, queued_by_job_id=276, pid=100750[0m
[[34m2024-05-22T09:21:21.164+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:16:18.101852+00:00, run_end_date=2024-05-22 09:21:20.440648+00:00, run_duration=302.338796, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=505, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 09:15:11.051065+00:00, queued_by_job_id=276, pid=100774[0m
[[34m2024-05-22T09:21:21.209+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T09:21:21.235+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-25 00:00:00+00:00, run_after=2023-08-26 00:00:00+00:00[0m
[[34m2024-05-22T09:21:21.356+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-17 00:00:00+00:00: scheduled__2023-08-17T00:00:00+00:00, state:running, queued_at: 2024-05-22 08:31:16.075837+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T09:21:21.357+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-17 00:00:00+00:00, run_id=scheduled__2023-08-17T00:00:00+00:00, run_start_date=2024-05-22 08:31:16.098881+00:00, run_end_date=2024-05-22 09:21:21.357231+00:00, run_duration=3005.25835, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-17 00:00:00+00:00, data_interval_end=2023-08-18 00:00:00+00:00, dag_hash=afe8935f0ff9501233879720e88b0aab[0m
[[34m2024-05-22T09:21:21.362+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-18 00:00:00+00:00, run_after=2023-08-19 00:00:00+00:00[0m
[[34m2024-05-22T09:21:21.377+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-22T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:21:21.377+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T09:21:21.378+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T09:21:21.378+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T09:21:21.379+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-22T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:21:21.384+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T09:21:21.384+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:21:21.385+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T09:21:21.385+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:21:21.386+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T09:21:21.386+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:21:21.388+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:21:22.997+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:21:23.420+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:27:15.101+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:27:16.748+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:27:17.178+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:27:19.334+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:27:20.777+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:27:21.217+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:32:24.119+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:32:24.119+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:32:24.119+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:32:24.126+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:21:23.533322+00:00, run_end_date=2024-05-22 09:27:14.391377+00:00, run_duration=350.858055, state=success, executor_state=success, try_number=1, max_tries=2, job_id=506, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 09:21:21.380136+00:00, queued_by_job_id=276, pid=100976[0m
[[34m2024-05-22T09:32:24.127+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:27:17.306629+00:00, run_end_date=2024-05-22 09:27:18.665769+00:00, run_duration=1.35914, state=success, executor_state=success, try_number=1, max_tries=2, job_id=507, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 09:21:21.380136+00:00, queued_by_job_id=276, pid=101214[0m
[[34m2024-05-22T09:32:24.127+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:27:21.321642+00:00, run_end_date=2024-05-22 09:32:23.459678+00:00, run_duration=302.138036, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=508, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 09:21:21.380136+00:00, queued_by_job_id=276, pid=101238[0m
[[34m2024-05-22T09:32:24.162+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T09:32:24.184+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-26 00:00:00+00:00, run_after=2023-08-27 00:00:00+00:00[0m
[[34m2024-05-22T09:32:24.390+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-18 00:00:00+00:00: scheduled__2023-08-18T00:00:00+00:00, state:running, queued_at: 2024-05-22 08:38:30.230086+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T09:32:24.390+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-18 00:00:00+00:00, run_id=scheduled__2023-08-18T00:00:00+00:00, run_start_date=2024-05-22 08:38:30.243319+00:00, run_end_date=2024-05-22 09:32:24.390547+00:00, run_duration=3234.147228, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-18 00:00:00+00:00, data_interval_end=2023-08-19 00:00:00+00:00, dag_hash=0d166da97c686772bc8315f9a492f669[0m
[[34m2024-05-22T09:32:24.394+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-19 00:00:00+00:00, run_after=2023-08-20 00:00:00+00:00[0m
[[34m2024-05-22T09:32:24.405+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-23T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:32:24.406+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T09:32:24.406+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T09:32:24.406+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T09:32:24.406+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-23T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:32:24.409+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T09:32:24.409+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:32:24.410+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T09:32:24.410+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:32:24.410+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T09:32:24.411+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:32:24.413+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:32:25.994+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:32:26.470+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:37:32.654+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:37:34.016+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:37:34.504+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:37:36.326+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:37:37.612+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:37:38.025+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:42:40.899+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:42:40.899+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:42:40.899+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:42:40.908+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:37:38.129915+00:00, run_end_date=2024-05-22 09:42:40.168701+00:00, run_duration=302.038786, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=511, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 09:32:24.407617+00:00, queued_by_job_id=276, pid=101671[0m
[[34m2024-05-22T09:42:40.909+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:32:26.567191+00:00, run_end_date=2024-05-22 09:37:31.567530+00:00, run_duration=305.000339, state=success, executor_state=success, try_number=1, max_tries=2, job_id=509, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 09:32:24.407617+00:00, queued_by_job_id=276, pid=101442[0m
[[34m2024-05-22T09:42:40.909+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:37:34.604213+00:00, run_end_date=2024-05-22 09:37:35.716557+00:00, run_duration=1.112344, state=success, executor_state=success, try_number=1, max_tries=2, job_id=510, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 09:32:24.407617+00:00, queued_by_job_id=276, pid=101647[0m
[[34m2024-05-22T09:42:40.945+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T09:42:40.966+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-27 00:00:00+00:00, run_after=2023-08-28 00:00:00+00:00[0m
[[34m2024-05-22T09:42:41.081+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-19 00:00:00+00:00: scheduled__2023-08-19T00:00:00+00:00, state:running, queued_at: 2024-05-22 08:46:44.186970+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T09:42:41.081+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-19 00:00:00+00:00, run_id=scheduled__2023-08-19T00:00:00+00:00, run_start_date=2024-05-22 08:46:44.201921+00:00, run_end_date=2024-05-22 09:42:41.081910+00:00, run_duration=3356.879989, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-19 00:00:00+00:00, data_interval_end=2023-08-20 00:00:00+00:00, dag_hash=453c7691f433146dcb5963386f2637ca[0m
[[34m2024-05-22T09:42:41.086+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-20 00:00:00+00:00, run_after=2023-08-21 00:00:00+00:00[0m
[[34m2024-05-22T09:42:41.097+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-24T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:42:41.098+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T09:42:41.098+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T09:42:41.098+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T09:42:41.098+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-24T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:42:41.101+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T09:42:41.101+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:42:41.102+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T09:42:41.103+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:42:41.103+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T09:42:41.103+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:42:41.105+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:42:42.789+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:42:43.224+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:43:46.098+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:43:47.452+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:43:47.856+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:43:49.847+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:43:51.491+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:43:52.077+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:48:54.781+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:48:54.781+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:48:54.781+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:48:54.794+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:42:43.339183+00:00, run_end_date=2024-05-22 09:43:45.452210+00:00, run_duration=62.113027, state=success, executor_state=success, try_number=1, max_tries=2, job_id=512, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 09:42:41.099648+00:00, queued_by_job_id=276, pid=101870[0m
[[34m2024-05-22T09:48:54.795+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:43:47.986947+00:00, run_end_date=2024-05-22 09:43:49.252108+00:00, run_duration=1.265161, state=success, executor_state=success, try_number=1, max_tries=2, job_id=513, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 09:42:41.099648+00:00, queued_by_job_id=276, pid=101925[0m
[[34m2024-05-22T09:48:54.796+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:43:52.182693+00:00, run_end_date=2024-05-22 09:48:54.193800+00:00, run_duration=302.011107, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=514, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 09:42:41.099648+00:00, queued_by_job_id=276, pid=101950[0m
[[34m2024-05-22T09:48:54.828+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T09:48:54.847+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-28 00:00:00+00:00, run_after=2023-08-29 00:00:00+00:00[0m
[[34m2024-05-22T09:48:54.935+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-20 00:00:00+00:00: scheduled__2023-08-20T00:00:00+00:00, state:running, queued_at: 2024-05-22 08:56:43.711895+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T09:48:54.935+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-20 00:00:00+00:00, run_id=scheduled__2023-08-20T00:00:00+00:00, run_start_date=2024-05-22 08:56:43.729125+00:00, run_end_date=2024-05-22 09:48:54.935378+00:00, run_duration=3131.206253, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-20 00:00:00+00:00, data_interval_end=2023-08-21 00:00:00+00:00, dag_hash=74d614cf229274f43f0ce8dccb8ad938[0m
[[34m2024-05-22T09:48:54.938+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-21 00:00:00+00:00, run_after=2023-08-22 00:00:00+00:00[0m
[[34m2024-05-22T09:48:54.947+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-25T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:48:54.948+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T09:48:54.948+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T09:48:54.948+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T09:48:54.948+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-25T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:48:54.950+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T09:48:54.951+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:48:54.951+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T09:48:54.951+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:48:54.952+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T09:48:54.952+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:48:54.953+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:48:56.106+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:48:56.455+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:51:17.628+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:51:18.836+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:51:19.190+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:51:21.416+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:51:22.936+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:51:23.316+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T09:56:25.934+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:56:25.935+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:56:25.935+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T09:56:25.956+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:48:56.552671+00:00, run_end_date=2024-05-22 09:51:16.947450+00:00, run_duration=140.394779, state=success, executor_state=success, try_number=1, max_tries=2, job_id=515, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 09:48:54.949489+00:00, queued_by_job_id=276, pid=102146[0m
[[34m2024-05-22T09:56:25.956+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:51:19.287206+00:00, run_end_date=2024-05-22 09:51:20.520275+00:00, run_duration=1.233069, state=success, executor_state=success, try_number=1, max_tries=2, job_id=516, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 09:48:54.949489+00:00, queued_by_job_id=276, pid=102250[0m
[[34m2024-05-22T09:56:25.957+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:51:23.416420+00:00, run_end_date=2024-05-22 09:56:25.327947+00:00, run_duration=301.911527, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=517, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 09:48:54.949489+00:00, queued_by_job_id=276, pid=102274[0m
[[34m2024-05-22T09:56:25.993+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T09:56:26.017+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-29 00:00:00+00:00, run_after=2023-08-30 00:00:00+00:00[0m
[[34m2024-05-22T09:56:26.108+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-21 00:00:00+00:00: scheduled__2023-08-21T00:00:00+00:00, state:running, queued_at: 2024-05-22 09:02:44.305711+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T09:56:26.109+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-21 00:00:00+00:00, run_id=scheduled__2023-08-21T00:00:00+00:00, run_start_date=2024-05-22 09:02:44.319760+00:00, run_end_date=2024-05-22 09:56:26.109303+00:00, run_duration=3221.789543, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-21 00:00:00+00:00, data_interval_end=2023-08-22 00:00:00+00:00, dag_hash=ebe8c17db90692204d04d304372beb6b[0m
[[34m2024-05-22T09:56:26.112+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-22 00:00:00+00:00, run_after=2023-08-23 00:00:00+00:00[0m
[[34m2024-05-22T09:56:26.123+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-26T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:56:26.123+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T09:56:26.123+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T09:56:26.123+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T09:56:26.124+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-26T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T09:56:26.127+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T09:56:26.128+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:56:26.128+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T09:56:26.129+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:56:26.129+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T09:56:26.130+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:56:26.131+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T09:56:27.428+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T09:56:27.844+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:03:18.400+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:03:19.717+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:03:20.103+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:03:22.215+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:03:23.465+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:03:23.828+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:08:27.238+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:08:27.239+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:08:27.239+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:08:27.244+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:03:20.210488+00:00, run_end_date=2024-05-22 10:03:21.433110+00:00, run_duration=1.222622, state=success, executor_state=success, try_number=1, max_tries=2, job_id=519, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 09:56:26.125168+00:00, queued_by_job_id=276, pid=102732[0m
[[34m2024-05-22T10:08:27.244+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:03:23.958202+00:00, run_end_date=2024-05-22 10:08:26.495007+00:00, run_duration=302.536805, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=520, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 09:56:26.125168+00:00, queued_by_job_id=276, pid=102756[0m
[[34m2024-05-22T10:08:27.245+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 09:56:27.987208+00:00, run_end_date=2024-05-22 10:03:17.785542+00:00, run_duration=409.798334, state=success, executor_state=success, try_number=1, max_tries=2, job_id=518, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 09:56:26.125168+00:00, queued_by_job_id=276, pid=102473[0m
[[34m2024-05-22T10:08:27.282+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T10:08:27.306+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-30 00:00:00+00:00, run_after=2023-08-31 00:00:00+00:00[0m
[[34m2024-05-22T10:08:27.420+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-22 00:00:00+00:00: scheduled__2023-08-22T00:00:00+00:00, state:running, queued_at: 2024-05-22 09:08:45.131431+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T10:08:27.421+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-22 00:00:00+00:00, run_id=scheduled__2023-08-22T00:00:00+00:00, run_start_date=2024-05-22 09:08:45.150113+00:00, run_end_date=2024-05-22 10:08:27.421425+00:00, run_duration=3582.271312, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-22 00:00:00+00:00, data_interval_end=2023-08-23 00:00:00+00:00, dag_hash=fc53900c079a19add7325cf00241de71[0m
[[34m2024-05-22T10:08:27.425+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-23 00:00:00+00:00, run_after=2023-08-24 00:00:00+00:00[0m
[[34m2024-05-22T10:08:27.436+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-27T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:08:27.437+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T10:08:27.437+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T10:08:27.437+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T10:08:27.438+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-27T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:08:27.441+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T10:08:27.441+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:08:27.442+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T10:08:27.442+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:08:27.442+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T10:08:27.442+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:08:27.444+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:08:28.856+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:08:29.524+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:10:51.535+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:10:52.956+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:10:53.327+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:10:55.262+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:10:56.711+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:10:57.130+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:16:00.067+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:16:00.068+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:16:00.068+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:16:00.083+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:10:57.238302+00:00, run_end_date=2024-05-22 10:15:59.316402+00:00, run_duration=302.0781, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=523, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 10:08:27.439585+00:00, queued_by_job_id=276, pid=103086[0m
[[34m2024-05-22T10:16:00.084+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:08:29.650843+00:00, run_end_date=2024-05-22 10:10:50.907897+00:00, run_duration=141.257054, state=success, executor_state=success, try_number=1, max_tries=2, job_id=521, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 10:08:27.439585+00:00, queued_by_job_id=276, pid=102957[0m
[[34m2024-05-22T10:16:00.084+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:10:53.432114+00:00, run_end_date=2024-05-22 10:10:54.599104+00:00, run_duration=1.16699, state=success, executor_state=success, try_number=1, max_tries=2, job_id=522, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 10:08:27.439585+00:00, queued_by_job_id=276, pid=103062[0m
[[34m2024-05-22T10:16:00.137+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T10:16:00.162+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-31 00:00:00+00:00, run_after=2023-09-01 00:00:00+00:00[0m
[[34m2024-05-22T10:16:00.274+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-23 00:00:00+00:00: scheduled__2023-08-23T00:00:00+00:00, state:running, queued_at: 2024-05-22 09:15:10.902011+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T10:16:00.275+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-23 00:00:00+00:00, run_id=scheduled__2023-08-23T00:00:00+00:00, run_start_date=2024-05-22 09:15:10.918248+00:00, run_end_date=2024-05-22 10:16:00.275409+00:00, run_duration=3649.357161, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-23 00:00:00+00:00, data_interval_end=2023-08-24 00:00:00+00:00, dag_hash=eb2581b0a96f1d130dccc6420ef77ae4[0m
[[34m2024-05-22T10:16:00.279+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-24 00:00:00+00:00, run_after=2023-08-25 00:00:00+00:00[0m
[[34m2024-05-22T10:16:00.290+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-28T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:16:00.291+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T10:16:00.291+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T10:16:00.291+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T10:16:00.291+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-28T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:16:00.294+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T10:16:00.295+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:16:00.295+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T10:16:00.295+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:16:00.296+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T10:16:00.296+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:16:00.298+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:16:02.092+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:16:02.568+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:19:46.725+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:19:48.050+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:19:48.431+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:19:50.219+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:19:51.392+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:19:51.813+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:24:54.596+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:24:54.597+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:24:54.597+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:24:54.602+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:16:02.704483+00:00, run_end_date=2024-05-22 10:19:46.175252+00:00, run_duration=223.470769, state=success, executor_state=success, try_number=1, max_tries=2, job_id=524, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 10:16:00.292809+00:00, queued_by_job_id=276, pid=103283[0m
[[34m2024-05-22T10:24:54.603+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:19:48.537789+00:00, run_end_date=2024-05-22 10:19:49.674052+00:00, run_duration=1.136263, state=success, executor_state=success, try_number=1, max_tries=2, job_id=525, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 10:16:00.292809+00:00, queued_by_job_id=276, pid=103443[0m
[[34m2024-05-22T10:24:54.603+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:19:52.023387+00:00, run_end_date=2024-05-22 10:24:53.976949+00:00, run_duration=301.953562, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=526, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 10:16:00.292809+00:00, queued_by_job_id=276, pid=103468[0m
[[34m2024-05-22T10:24:54.636+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T10:24:54.659+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-01 00:00:00+00:00, run_after=2023-09-02 00:00:00+00:00[0m
[[34m2024-05-22T10:24:54.750+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-24 00:00:00+00:00: scheduled__2023-08-24T00:00:00+00:00, state:running, queued_at: 2024-05-22 09:21:21.226422+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T10:24:54.750+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-24 00:00:00+00:00, run_id=scheduled__2023-08-24T00:00:00+00:00, run_start_date=2024-05-22 09:21:21.244045+00:00, run_end_date=2024-05-22 10:24:54.750452+00:00, run_duration=3813.506407, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-24 00:00:00+00:00, data_interval_end=2023-08-25 00:00:00+00:00, dag_hash=ad4f1a9c8ccf882dfa9487289c14b24b[0m
[[34m2024-05-22T10:24:54.753+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-25 00:00:00+00:00, run_after=2023-08-26 00:00:00+00:00[0m
[[34m2024-05-22T10:24:54.765+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-31T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-29T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:24:54.765+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T10:24:54.766+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T10:24:54.766+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T10:24:54.766+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-31T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-29T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:24:54.768+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T10:24:54.769+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:24:54.769+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T10:24:54.769+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:24:54.770+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T10:24:54.770+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:24:54.772+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:24:56.355+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:24:56.848+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-08-31T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:26:49.209+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:26:50.866+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:26:51.278+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:26:53.476+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:26:54.764+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:26:55.154+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:31:57.995+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:31:57.995+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:31:57.996+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:31:58.020+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-08-31T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:24:56.994926+00:00, run_end_date=2024-05-22 10:26:48.611478+00:00, run_duration=111.616552, state=success, executor_state=success, try_number=1, max_tries=2, job_id=527, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 10:24:54.767465+00:00, queued_by_job_id=276, pid=104074[0m
[[34m2024-05-22T10:31:58.021+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:26:51.374085+00:00, run_end_date=2024-05-22 10:26:52.787086+00:00, run_duration=1.413001, state=success, executor_state=success, try_number=1, max_tries=2, job_id=528, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 10:24:54.767465+00:00, queued_by_job_id=276, pid=104163[0m
[[34m2024-05-22T10:31:58.021+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:26:55.283119+00:00, run_end_date=2024-05-22 10:31:57.314122+00:00, run_duration=302.031003, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=529, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 10:24:54.767465+00:00, queued_by_job_id=276, pid=104189[0m
[[34m2024-05-22T10:31:58.059+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T10:31:58.082+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-02 00:00:00+00:00, run_after=2023-09-03 00:00:00+00:00[0m
[[34m2024-05-22T10:31:58.184+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-25 00:00:00+00:00: scheduled__2023-08-25T00:00:00+00:00, state:running, queued_at: 2024-05-22 09:32:24.177847+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T10:31:58.185+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-25 00:00:00+00:00, run_id=scheduled__2023-08-25T00:00:00+00:00, run_start_date=2024-05-22 09:32:24.196536+00:00, run_end_date=2024-05-22 10:31:58.184999+00:00, run_duration=3573.988463, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-25 00:00:00+00:00, data_interval_end=2023-08-26 00:00:00+00:00, dag_hash=f398354b15ebe35dd239a0aacfe1a061[0m
[[34m2024-05-22T10:31:58.189+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-26 00:00:00+00:00, run_after=2023-08-27 00:00:00+00:00[0m
[[34m2024-05-22T10:31:58.200+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-31T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-30T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:31:58.200+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T10:31:58.200+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T10:31:58.201+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T10:31:58.201+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-31T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-30T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:31:58.203+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T10:31:58.204+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:31:58.204+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T10:31:58.204+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:31:58.205+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T10:31:58.205+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:31:58.206+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:31:59.469+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:31:59.840+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:34:06.379+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:34:07.747+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:34:08.234+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-08-31T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:34:10.421+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:34:11.760+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:34:12.312+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:39:14.981+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:39:14.982+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:39:14.982+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:39:14.987+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-08-31T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:34:08.385283+00:00, run_end_date=2024-05-22 10:34:09.733074+00:00, run_duration=1.347791, state=success, executor_state=success, try_number=1, max_tries=2, job_id=531, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 10:31:58.202227+00:00, queued_by_job_id=276, pid=104480[0m
[[34m2024-05-22T10:39:14.987+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:34:12.422608+00:00, run_end_date=2024-05-22 10:39:14.334292+00:00, run_duration=301.911684, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=532, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 10:31:58.202227+00:00, queued_by_job_id=276, pid=104505[0m
[[34m2024-05-22T10:39:14.988+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:32:00.047999+00:00, run_end_date=2024-05-22 10:34:05.779759+00:00, run_duration=125.73176, state=success, executor_state=success, try_number=1, max_tries=2, job_id=530, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 10:31:58.202227+00:00, queued_by_job_id=276, pid=104390[0m
[[34m2024-05-22T10:39:15.027+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T10:39:15.047+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-03 00:00:00+00:00, run_after=2023-09-04 00:00:00+00:00[0m
[[34m2024-05-22T10:39:15.137+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-26 00:00:00+00:00: scheduled__2023-08-26T00:00:00+00:00, state:running, queued_at: 2024-05-22 09:42:40.958859+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T10:39:15.138+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-26 00:00:00+00:00, run_id=scheduled__2023-08-26T00:00:00+00:00, run_start_date=2024-05-22 09:42:40.975144+00:00, run_end_date=2024-05-22 10:39:15.138137+00:00, run_duration=3394.162993, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-26 00:00:00+00:00, data_interval_end=2023-08-27 00:00:00+00:00, dag_hash=eabbc2f0ea337ca7389858eaaf2a7314[0m
[[34m2024-05-22T10:39:15.141+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-27 00:00:00+00:00, run_after=2023-08-28 00:00:00+00:00[0m
[[34m2024-05-22T10:39:15.150+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-31T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:39:15.151+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T10:39:15.151+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T10:39:15.151+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T10:39:15.151+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-31T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:39:15.154+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T10:39:15.154+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:39:15.154+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T10:39:15.154+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:39:15.155+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T10:39:15.155+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:39:15.156+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:39:16.354+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:39:16.719+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:42:26.601+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:42:27.871+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:42:28.559+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:42:30.674+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:42:32.131+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:42:32.508+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-08-31T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:47:35.115+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:47:35.115+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:47:35.115+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-08-31T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:47:35.121+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-08-31T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:42:32.610004+00:00, run_end_date=2024-05-22 10:47:34.510189+00:00, run_duration=301.900185, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=535, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 10:39:15.152599+00:00, queued_by_job_id=276, pid=104878[0m
[[34m2024-05-22T10:47:35.121+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:39:16.814990+00:00, run_end_date=2024-05-22 10:42:25.884868+00:00, run_duration=189.069878, state=success, executor_state=success, try_number=1, max_tries=2, job_id=533, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 10:39:15.152599+00:00, queued_by_job_id=276, pid=104704[0m
[[34m2024-05-22T10:47:35.122+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:42:28.682737+00:00, run_end_date=2024-05-22 10:42:30.027525+00:00, run_duration=1.344788, state=success, executor_state=success, try_number=1, max_tries=2, job_id=534, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 10:39:15.152599+00:00, queued_by_job_id=276, pid=104853[0m
[[34m2024-05-22T10:47:35.161+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T10:47:35.183+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-04 00:00:00+00:00, run_after=2023-09-05 00:00:00+00:00[0m
[[34m2024-05-22T10:47:35.275+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-27 00:00:00+00:00: scheduled__2023-08-27T00:00:00+00:00, state:running, queued_at: 2024-05-22 09:48:54.840651+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T10:47:35.276+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-27 00:00:00+00:00, run_id=scheduled__2023-08-27T00:00:00+00:00, run_start_date=2024-05-22 09:48:54.854434+00:00, run_end_date=2024-05-22 10:47:35.276188+00:00, run_duration=3520.421754, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-27 00:00:00+00:00, data_interval_end=2023-08-28 00:00:00+00:00, dag_hash=eb4eb73cf9af4fcd9a439bd3f318df21[0m
[[34m2024-05-22T10:47:35.279+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-28 00:00:00+00:00, run_after=2023-08-29 00:00:00+00:00[0m
[[34m2024-05-22T10:47:35.290+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:47:35.290+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T10:47:35.290+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T10:47:35.291+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T10:47:35.291+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:47:35.294+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T10:47:35.294+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:47:35.295+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T10:47:35.295+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:47:35.295+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T10:47:35.295+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:47:35.297+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:47:36.763+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:47:37.192+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:50:05.959+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:50:07.472+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:50:07.903+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:50:09.763+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:50:10.988+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:50:11.377+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:55:14.274+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:55:14.275+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:55:14.275+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T10:55:14.282+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:47:37.300537+00:00, run_end_date=2024-05-22 10:50:05.379715+00:00, run_duration=148.079178, state=success, executor_state=success, try_number=1, max_tries=2, job_id=536, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 10:47:35.292294+00:00, queued_by_job_id=276, pid=105079[0m
[[34m2024-05-22T10:55:14.282+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:50:08.012734+00:00, run_end_date=2024-05-22 10:50:09.166670+00:00, run_duration=1.153936, state=success, executor_state=success, try_number=1, max_tries=2, job_id=537, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 10:47:35.292294+00:00, queued_by_job_id=276, pid=105186[0m
[[34m2024-05-22T10:55:14.283+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:50:11.473956+00:00, run_end_date=2024-05-22 10:55:13.619582+00:00, run_duration=302.145626, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=538, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 10:47:35.292294+00:00, queued_by_job_id=276, pid=105211[0m
[[34m2024-05-22T10:55:14.318+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T10:55:14.338+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-05 00:00:00+00:00, run_after=2023-09-06 00:00:00+00:00[0m
[[34m2024-05-22T10:55:14.460+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-28 00:00:00+00:00: scheduled__2023-08-28T00:00:00+00:00, state:running, queued_at: 2024-05-22 09:56:26.010045+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T10:55:14.461+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-28 00:00:00+00:00, run_id=scheduled__2023-08-28T00:00:00+00:00, run_start_date=2024-05-22 09:56:26.024836+00:00, run_end_date=2024-05-22 10:55:14.461161+00:00, run_duration=3528.436325, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-28 00:00:00+00:00, data_interval_end=2023-08-29 00:00:00+00:00, dag_hash=f00eab9bf8c99d150a05b123fb6ea71a[0m
[[34m2024-05-22T10:55:14.465+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-29 00:00:00+00:00, run_after=2023-08-30 00:00:00+00:00[0m
[[34m2024-05-22T10:55:14.478+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-02T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:55:14.478+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T10:55:14.478+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T10:55:14.479+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T10:55:14.480+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-02T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T10:55:14.482+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T10:55:14.482+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:55:14.483+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T10:55:14.483+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:55:14.484+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T10:55:14.484+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:55:14.486+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:55:16.121+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:55:16.495+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:57:34.942+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:57:36.317+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:57:36.759+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T10:57:38.988+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T10:57:40.226+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T10:57:40.579+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:02:43.419+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:02:43.419+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:02:43.419+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:02:43.441+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:55:16.603574+00:00, run_end_date=2024-05-22 10:57:34.251384+00:00, run_duration=137.64781, state=success, executor_state=success, try_number=1, max_tries=2, job_id=539, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 10:55:14.480774+00:00, queued_by_job_id=276, pid=105409[0m
[[34m2024-05-22T11:02:43.442+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:57:36.868218+00:00, run_end_date=2024-05-22 10:57:38.239760+00:00, run_duration=1.371542, state=success, executor_state=success, try_number=1, max_tries=2, job_id=540, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 10:55:14.480774+00:00, queued_by_job_id=276, pid=105505[0m
[[34m2024-05-22T11:02:43.443+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 10:57:40.676709+00:00, run_end_date=2024-05-22 11:02:42.679485+00:00, run_duration=302.002776, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=541, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 10:55:14.480774+00:00, queued_by_job_id=276, pid=105530[0m
[[34m2024-05-22T11:02:43.485+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T11:02:43.505+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-06 00:00:00+00:00, run_after=2023-09-07 00:00:00+00:00[0m
[[34m2024-05-22T11:02:43.603+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-29 00:00:00+00:00: scheduled__2023-08-29T00:00:00+00:00, state:running, queued_at: 2024-05-22 10:08:27.298414+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T11:02:43.603+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-29 00:00:00+00:00, run_id=scheduled__2023-08-29T00:00:00+00:00, run_start_date=2024-05-22 10:08:27.314697+00:00, run_end_date=2024-05-22 11:02:43.603467+00:00, run_duration=3256.28877, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-29 00:00:00+00:00, data_interval_end=2023-08-30 00:00:00+00:00, dag_hash=f1d34d345402af2af5662439cf7af075[0m
[[34m2024-05-22T11:02:43.606+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-30 00:00:00+00:00, run_after=2023-08-31 00:00:00+00:00[0m
[[34m2024-05-22T11:02:43.620+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:02:43.620+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T11:02:43.620+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T11:02:43.620+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T11:02:43.621+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:02:43.629+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T11:02:43.629+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:02:43.631+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T11:02:43.632+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:02:43.635+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T11:02:43.635+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:02:43.638+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:02:45.242+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:02:45.676+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:06:34.321+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:06:35.605+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:06:35.973+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:06:38.006+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:06:39.444+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:06:39.868+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:11:42.988+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:11:42.989+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:11:42.989+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:11:42.996+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:02:45.787235+00:00, run_end_date=2024-05-22 11:06:33.760236+00:00, run_duration=227.973001, state=success, executor_state=success, try_number=1, max_tries=2, job_id=542, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 11:02:43.624506+00:00, queued_by_job_id=276, pid=105730[0m
[[34m2024-05-22T11:11:42.996+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:06:36.079793+00:00, run_end_date=2024-05-22 11:06:37.333060+00:00, run_duration=1.253267, state=success, executor_state=success, try_number=1, max_tries=2, job_id=543, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 11:02:43.624506+00:00, queued_by_job_id=276, pid=105900[0m
[[34m2024-05-22T11:11:42.996+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:06:39.972142+00:00, run_end_date=2024-05-22 11:11:42.038988+00:00, run_duration=302.066846, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=544, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 11:02:43.624506+00:00, queued_by_job_id=276, pid=105924[0m
[[34m2024-05-22T11:11:43.034+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T11:11:43.055+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-07 00:00:00+00:00, run_after=2023-09-08 00:00:00+00:00[0m
[[34m2024-05-22T11:11:43.163+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-30 00:00:00+00:00: scheduled__2023-08-30T00:00:00+00:00, state:running, queued_at: 2024-05-22 10:16:00.153887+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T11:11:43.164+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-30 00:00:00+00:00, run_id=scheduled__2023-08-30T00:00:00+00:00, run_start_date=2024-05-22 10:16:00.171399+00:00, run_end_date=2024-05-22 11:11:43.164151+00:00, run_duration=3342.992752, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-30 00:00:00+00:00, data_interval_end=2023-08-31 00:00:00+00:00, dag_hash=5da7e4b8b73d828d414a8e76eba37643[0m
[[34m2024-05-22T11:11:43.168+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-08-31 00:00:00+00:00, run_after=2023-09-01 00:00:00+00:00[0m
[[34m2024-05-22T11:11:43.179+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:11:43.179+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T11:11:43.179+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T11:11:43.180+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T11:11:43.180+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:11:43.183+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T11:11:43.183+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:11:43.184+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T11:11:43.184+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:11:43.184+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T11:11:43.185+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:11:43.186+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:11:44.579+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:11:45.041+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:13:29.732+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:13:30.982+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:13:31.349+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:13:33.467+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:13:34.824+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:13:35.200+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:18:38.335+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:18:38.335+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:18:38.336+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:18:38.341+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:13:31.448293+00:00, run_end_date=2024-05-22 11:13:32.813343+00:00, run_duration=1.36505, state=success, executor_state=success, try_number=1, max_tries=2, job_id=546, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 11:11:43.181408+00:00, queued_by_job_id=276, pid=106208[0m
[[34m2024-05-22T11:18:38.342+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:11:45.171984+00:00, run_end_date=2024-05-22 11:13:29.073580+00:00, run_duration=103.901596, state=success, executor_state=success, try_number=1, max_tries=2, job_id=545, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 11:11:43.181408+00:00, queued_by_job_id=276, pid=106125[0m
[[34m2024-05-22T11:18:38.342+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:13:35.308215+00:00, run_end_date=2024-05-22 11:18:37.527621+00:00, run_duration=302.219406, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=547, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 11:11:43.181408+00:00, queued_by_job_id=276, pid=106232[0m
[[34m2024-05-22T11:18:38.382+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T11:18:38.404+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-08 00:00:00+00:00, run_after=2023-09-09 00:00:00+00:00[0m
[[34m2024-05-22T11:18:38.537+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-08-31 00:00:00+00:00: scheduled__2023-08-31T00:00:00+00:00, state:running, queued_at: 2024-05-22 10:24:54.652207+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T11:18:38.538+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-08-31 00:00:00+00:00, run_id=scheduled__2023-08-31T00:00:00+00:00, run_start_date=2024-05-22 10:24:54.666107+00:00, run_end_date=2024-05-22 11:18:38.537910+00:00, run_duration=3223.871803, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-08-31 00:00:00+00:00, data_interval_end=2023-09-01 00:00:00+00:00, dag_hash=ae3d699801db35fc5880a2ea4c4a045c[0m
[[34m2024-05-22T11:18:38.544+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-01 00:00:00+00:00, run_after=2023-09-02 00:00:00+00:00[0m
[[34m2024-05-22T11:18:38.561+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:18:38.561+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T11:18:38.561+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T11:18:38.561+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T11:18:38.563+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:18:38.566+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T11:18:38.567+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:18:38.568+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T11:18:38.569+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:18:38.569+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T11:18:38.570+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:18:38.571+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:18:40.167+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:18:40.590+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:19:33.421+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:19:34.787+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:19:35.238+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:19:37.202+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:19:38.392+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:19:38.740+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:24:41.547+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:24:41.548+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:24:41.548+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:24:41.570+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:18:40.762383+00:00, run_end_date=2024-05-22 11:19:32.714942+00:00, run_duration=51.952559, state=success, executor_state=success, try_number=1, max_tries=2, job_id=548, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 11:18:38.564663+00:00, queued_by_job_id=276, pid=106432[0m
[[34m2024-05-22T11:24:41.571+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:19:38.887939+00:00, run_end_date=2024-05-22 11:24:40.912124+00:00, run_duration=302.024185, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=550, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 11:18:38.564663+00:00, queued_by_job_id=276, pid=106506[0m
[[34m2024-05-22T11:24:41.571+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:19:35.343843+00:00, run_end_date=2024-05-22 11:19:36.592186+00:00, run_duration=1.248343, state=success, executor_state=success, try_number=1, max_tries=2, job_id=549, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 11:18:38.564663+00:00, queued_by_job_id=276, pid=106482[0m
[[34m2024-05-22T11:24:41.608+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T11:24:41.629+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-09 00:00:00+00:00, run_after=2023-09-10 00:00:00+00:00[0m
[[34m2024-05-22T11:24:41.725+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-01 00:00:00+00:00: scheduled__2023-09-01T00:00:00+00:00, state:running, queued_at: 2024-05-22 10:31:58.074741+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T11:24:41.726+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-01 00:00:00+00:00, run_id=scheduled__2023-09-01T00:00:00+00:00, run_start_date=2024-05-22 10:31:58.090838+00:00, run_end_date=2024-05-22 11:24:41.725957+00:00, run_duration=3163.635119, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-01 00:00:00+00:00, data_interval_end=2023-09-02 00:00:00+00:00, dag_hash=418a5fb0d3a32e56c95c16121ea7acf8[0m
[[34m2024-05-22T11:24:41.729+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-02 00:00:00+00:00, run_after=2023-09-03 00:00:00+00:00[0m
[[34m2024-05-22T11:24:41.738+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-06T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:24:41.739+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T11:24:41.739+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T11:24:41.739+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T11:24:41.740+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-06T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:24:41.742+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T11:24:41.742+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:24:41.742+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T11:24:41.743+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:24:41.743+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T11:24:41.743+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:24:41.744+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:24:43.261+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:24:43.729+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:27:12.146+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:27:13.612+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:27:14.009+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:27:16.196+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:27:17.610+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:27:18.112+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:32:20.900+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:32:20.901+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:32:20.901+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:32:20.906+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:27:14.111393+00:00, run_end_date=2024-05-22 11:27:15.517075+00:00, run_duration=1.405682, state=success, executor_state=success, try_number=1, max_tries=2, job_id=552, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 11:24:41.740785+00:00, queued_by_job_id=276, pid=106813[0m
[[34m2024-05-22T11:32:20.906+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:27:18.231703+00:00, run_end_date=2024-05-22 11:32:20.183489+00:00, run_duration=301.951786, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=553, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 11:24:41.740785+00:00, queued_by_job_id=276, pid=106837[0m
[[34m2024-05-22T11:32:20.907+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:24:43.826754+00:00, run_end_date=2024-05-22 11:27:11.502103+00:00, run_duration=147.675349, state=success, executor_state=success, try_number=1, max_tries=2, job_id=551, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 11:24:41.740785+00:00, queued_by_job_id=276, pid=106704[0m
[[34m2024-05-22T11:32:20.945+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T11:32:20.966+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-10 00:00:00+00:00, run_after=2023-09-11 00:00:00+00:00[0m
[[34m2024-05-22T11:32:21.068+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-02 00:00:00+00:00: scheduled__2023-09-02T00:00:00+00:00, state:running, queued_at: 2024-05-22 10:39:15.040398+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T11:32:21.069+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-02 00:00:00+00:00, run_id=scheduled__2023-09-02T00:00:00+00:00, run_start_date=2024-05-22 10:39:15.054823+00:00, run_end_date=2024-05-22 11:32:21.069155+00:00, run_duration=3186.014332, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-02 00:00:00+00:00, data_interval_end=2023-09-03 00:00:00+00:00, dag_hash=b2ddb983faa1ce09f2725670d7209a77[0m
[[34m2024-05-22T11:32:21.072+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-03 00:00:00+00:00, run_after=2023-09-04 00:00:00+00:00[0m
[[34m2024-05-22T11:32:21.087+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-07T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:32:21.088+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T11:32:21.088+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T11:32:21.088+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T11:32:21.088+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-07T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:32:21.091+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T11:32:21.091+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:32:21.091+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T11:32:21.092+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:32:21.092+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T11:32:21.092+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:32:21.094+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:32:22.374+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:32:22.923+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:34:01.424+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:34:02.945+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:34:03.324+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:34:05.222+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:34:06.624+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:34:07.037+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:39:10.008+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:39:10.009+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:39:10.010+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:39:10.034+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:34:07.149313+00:00, run_end_date=2024-05-22 11:39:09.248168+00:00, run_duration=302.098855, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=556, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 11:32:21.089601+00:00, queued_by_job_id=276, pid=107135[0m
[[34m2024-05-22T11:39:10.035+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:32:23.031905+00:00, run_end_date=2024-05-22 11:34:00.703217+00:00, run_duration=97.671312, state=success, executor_state=success, try_number=1, max_tries=2, job_id=554, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 11:32:21.089601+00:00, queued_by_job_id=276, pid=107036[0m
[[34m2024-05-22T11:39:10.035+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:34:03.427127+00:00, run_end_date=2024-05-22 11:34:04.561692+00:00, run_duration=1.134565, state=success, executor_state=success, try_number=1, max_tries=2, job_id=555, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 11:32:21.089601+00:00, queued_by_job_id=276, pid=107111[0m
[[34m2024-05-22T11:39:10.075+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T11:39:10.107+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-11 00:00:00+00:00, run_after=2023-09-12 00:00:00+00:00[0m
[[34m2024-05-22T11:39:10.266+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-03 00:00:00+00:00: scheduled__2023-09-03T00:00:00+00:00, state:running, queued_at: 2024-05-22 10:47:35.176433+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T11:39:10.267+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-03 00:00:00+00:00, run_id=scheduled__2023-09-03T00:00:00+00:00, run_start_date=2024-05-22 10:47:35.190921+00:00, run_end_date=2024-05-22 11:39:10.267077+00:00, run_duration=3095.076156, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-03 00:00:00+00:00, data_interval_end=2023-09-04 00:00:00+00:00, dag_hash=e004ff30fd4d50ea32ed59b9623b5501[0m
[[34m2024-05-22T11:39:10.270+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-04 00:00:00+00:00, run_after=2023-09-05 00:00:00+00:00[0m
[[34m2024-05-22T11:39:10.282+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-08T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:39:10.283+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T11:39:10.283+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T11:39:10.284+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T11:39:10.284+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-08T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:39:10.288+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T11:39:10.288+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:39:10.289+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T11:39:10.289+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:39:10.290+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T11:39:10.290+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:39:10.292+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:39:11.934+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:39:12.450+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:41:10.994+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:41:12.309+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:41:12.920+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:41:14.950+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:41:16.279+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:41:16.666+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:46:19.426+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:46:19.426+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:46:19.427+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:46:19.432+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:39:12.621613+00:00, run_end_date=2024-05-22 11:41:10.372873+00:00, run_duration=117.75126, state=success, executor_state=success, try_number=1, max_tries=2, job_id=557, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 11:39:10.285883+00:00, queued_by_job_id=276, pid=107335[0m
[[34m2024-05-22T11:46:19.432+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:41:13.042687+00:00, run_end_date=2024-05-22 11:41:14.350437+00:00, run_duration=1.30775, state=success, executor_state=success, try_number=1, max_tries=2, job_id=558, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 11:39:10.285883+00:00, queued_by_job_id=276, pid=107426[0m
[[34m2024-05-22T11:46:19.432+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:41:16.767371+00:00, run_end_date=2024-05-22 11:46:18.784115+00:00, run_duration=302.016744, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=559, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 11:39:10.285883+00:00, queued_by_job_id=276, pid=107450[0m
[[34m2024-05-22T11:46:19.466+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T11:46:19.485+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-12 00:00:00+00:00, run_after=2023-09-13 00:00:00+00:00[0m
[[34m2024-05-22T11:46:19.573+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-04 00:00:00+00:00: scheduled__2023-09-04T00:00:00+00:00, state:running, queued_at: 2024-05-22 10:55:14.331051+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T11:46:19.573+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-04 00:00:00+00:00, run_id=scheduled__2023-09-04T00:00:00+00:00, run_start_date=2024-05-22 10:55:14.344956+00:00, run_end_date=2024-05-22 11:46:19.573428+00:00, run_duration=3065.228472, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-04 00:00:00+00:00, data_interval_end=2023-09-05 00:00:00+00:00, dag_hash=92dfa6813d372d5d529934f17d725325[0m
[[34m2024-05-22T11:46:19.577+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-05 00:00:00+00:00, run_after=2023-09-06 00:00:00+00:00[0m
[[34m2024-05-22T11:46:19.587+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-09T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:46:19.588+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T11:46:19.588+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T11:46:19.588+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T11:46:19.589+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-09T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:46:19.592+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T11:46:19.592+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:46:19.593+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T11:46:19.593+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:46:19.593+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T11:46:19.594+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:46:19.595+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:46:20.830+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:46:21.182+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:49:01.415+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:49:02.854+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:49:03.251+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:49:05.426+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:49:06.927+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:49:07.391+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:54:10.135+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:54:10.135+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:54:10.135+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T11:54:10.140+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:46:21.275255+00:00, run_end_date=2024-05-22 11:49:00.801956+00:00, run_duration=159.526701, state=success, executor_state=success, try_number=1, max_tries=2, job_id=560, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 11:46:19.590497+00:00, queued_by_job_id=276, pid=107648[0m
[[34m2024-05-22T11:54:10.141+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:49:03.392331+00:00, run_end_date=2024-05-22 11:49:04.762267+00:00, run_duration=1.369936, state=success, executor_state=success, try_number=1, max_tries=2, job_id=561, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 11:46:19.590497+00:00, queued_by_job_id=276, pid=107761[0m
[[34m2024-05-22T11:54:10.141+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:49:07.500557+00:00, run_end_date=2024-05-22 11:54:09.465604+00:00, run_duration=301.965047, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=562, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 11:46:19.590497+00:00, queued_by_job_id=276, pid=107785[0m
[[34m2024-05-22T11:54:10.175+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T11:54:10.194+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-13 00:00:00+00:00, run_after=2023-09-14 00:00:00+00:00[0m
[[34m2024-05-22T11:54:10.295+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-05 00:00:00+00:00: scheduled__2023-09-05T00:00:00+00:00, state:running, queued_at: 2024-05-22 11:02:43.498472+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T11:54:10.296+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-05 00:00:00+00:00, run_id=scheduled__2023-09-05T00:00:00+00:00, run_start_date=2024-05-22 11:02:43.513483+00:00, run_end_date=2024-05-22 11:54:10.296063+00:00, run_duration=3086.78258, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-05 00:00:00+00:00, data_interval_end=2023-09-06 00:00:00+00:00, dag_hash=d9bb340dc288fe82a7832c31faed478f[0m
[[34m2024-05-22T11:54:10.299+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-06 00:00:00+00:00, run_after=2023-09-07 00:00:00+00:00[0m
[[34m2024-05-22T11:54:10.312+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-10T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:54:10.312+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T11:54:10.312+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T11:54:10.312+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T11:54:10.313+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-10T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T11:54:10.315+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T11:54:10.315+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:54:10.316+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T11:54:10.316+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:54:10.316+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T11:54:10.316+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:54:10.317+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:54:11.631+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:54:12.081+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:55:26.028+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:55:27.496+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:55:27.884+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T11:55:29.891+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T11:55:31.251+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T11:55:31.662+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:00:34.617+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:00:34.618+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:00:34.618+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:00:34.623+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:54:12.189306+00:00, run_end_date=2024-05-22 11:55:25.353783+00:00, run_duration=73.164477, state=success, executor_state=success, try_number=1, max_tries=2, job_id=563, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 11:54:10.313697+00:00, queued_by_job_id=276, pid=107986[0m
[[34m2024-05-22T12:00:34.624+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:55:27.987999+00:00, run_end_date=2024-05-22 11:55:29.261811+00:00, run_duration=1.273812, state=success, executor_state=success, try_number=1, max_tries=2, job_id=564, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 11:54:10.313697+00:00, queued_by_job_id=276, pid=108048[0m
[[34m2024-05-22T12:00:34.624+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 11:55:31.767048+00:00, run_end_date=2024-05-22 12:00:33.845792+00:00, run_duration=302.078744, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=565, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 11:54:10.313697+00:00, queued_by_job_id=276, pid=108073[0m
[[34m2024-05-22T12:00:34.657+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T12:00:34.676+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-14 00:00:00+00:00, run_after=2023-09-15 00:00:00+00:00[0m
[[34m2024-05-22T12:00:34.835+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-06 00:00:00+00:00: scheduled__2023-09-06T00:00:00+00:00, state:running, queued_at: 2024-05-22 11:11:43.047574+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T12:00:34.835+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-06 00:00:00+00:00, run_id=scheduled__2023-09-06T00:00:00+00:00, run_start_date=2024-05-22 11:11:43.062658+00:00, run_end_date=2024-05-22 12:00:34.835443+00:00, run_duration=2931.772785, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-06 00:00:00+00:00, data_interval_end=2023-09-07 00:00:00+00:00, dag_hash=929945f3f85556d52f82077cd1140651[0m
[[34m2024-05-22T12:00:34.840+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-07 00:00:00+00:00, run_after=2023-09-08 00:00:00+00:00[0m
[[34m2024-05-22T12:00:34.852+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-11T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:00:34.853+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T12:00:34.853+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T12:00:34.853+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T12:00:34.853+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-11T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:00:34.856+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T12:00:34.856+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:00:34.856+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T12:00:34.856+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:00:34.857+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T12:00:34.857+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:00:34.858+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:00:36.232+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:00:36.663+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:02:04.186+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:02:05.629+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:02:05.993+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:02:07.782+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:02:08.979+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:02:09.435+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:07:12.589+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:07:12.589+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:07:12.589+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:07:12.596+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:00:36.774030+00:00, run_end_date=2024-05-22 12:02:03.443181+00:00, run_duration=86.669151, state=success, executor_state=success, try_number=1, max_tries=2, job_id=566, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 12:00:34.854461+00:00, queued_by_job_id=276, pid=108271[0m
[[34m2024-05-22T12:07:12.597+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:02:06.090368+00:00, run_end_date=2024-05-22 12:02:07.196648+00:00, run_duration=1.10628, state=success, executor_state=success, try_number=1, max_tries=2, job_id=567, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 12:00:34.854461+00:00, queued_by_job_id=276, pid=108341[0m
[[34m2024-05-22T12:07:12.597+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:02:09.557507+00:00, run_end_date=2024-05-22 12:07:11.798131+00:00, run_duration=302.240624, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=568, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 12:00:34.854461+00:00, queued_by_job_id=276, pid=108365[0m
[[34m2024-05-22T12:07:12.644+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T12:07:12.679+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-15 00:00:00+00:00, run_after=2023-09-16 00:00:00+00:00[0m
[[34m2024-05-22T12:07:12.914+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-07 00:00:00+00:00: scheduled__2023-09-07T00:00:00+00:00, state:running, queued_at: 2024-05-22 11:18:38.397045+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T12:07:12.915+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-07 00:00:00+00:00, run_id=scheduled__2023-09-07T00:00:00+00:00, run_start_date=2024-05-22 11:18:38.414334+00:00, run_end_date=2024-05-22 12:07:12.915211+00:00, run_duration=2914.500877, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-07 00:00:00+00:00, data_interval_end=2023-09-08 00:00:00+00:00, dag_hash=2d845ef0177d1fa683b125eb1093c0a2[0m
[[34m2024-05-22T12:07:12.927+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-08 00:00:00+00:00, run_after=2023-09-09 00:00:00+00:00[0m
[[34m2024-05-22T12:07:12.942+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-12T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:07:12.942+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T12:07:12.942+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T12:07:12.942+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T12:07:12.943+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-12T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:07:12.945+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T12:07:12.946+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:07:12.946+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T12:07:12.946+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:07:12.946+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T12:07:12.947+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:07:12.948+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:07:14.554+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:07:15.305+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:08:59.053+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:09:00.697+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:09:01.173+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:09:03.503+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:09:04.889+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:09:05.298+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:14:08.198+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:14:08.199+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:14:08.199+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:14:08.219+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:07:15.435607+00:00, run_end_date=2024-05-22 12:08:58.430255+00:00, run_duration=102.994648, state=success, executor_state=success, try_number=1, max_tries=2, job_id=569, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 12:07:12.943803+00:00, queued_by_job_id=276, pid=108568[0m
[[34m2024-05-22T12:14:08.220+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:09:01.307273+00:00, run_end_date=2024-05-22 12:09:02.863838+00:00, run_duration=1.556565, state=success, executor_state=success, try_number=1, max_tries=2, job_id=570, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 12:07:12.943803+00:00, queued_by_job_id=276, pid=108645[0m
[[34m2024-05-22T12:14:08.220+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:09:05.409695+00:00, run_end_date=2024-05-22 12:14:07.419319+00:00, run_duration=302.009624, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=571, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 12:07:12.943803+00:00, queued_by_job_id=276, pid=108669[0m
[[34m2024-05-22T12:14:08.256+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T12:14:08.286+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-16 00:00:00+00:00, run_after=2023-09-17 00:00:00+00:00[0m
[[34m2024-05-22T12:14:08.391+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-08 00:00:00+00:00: scheduled__2023-09-08T00:00:00+00:00, state:running, queued_at: 2024-05-22 11:24:41.621852+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T12:14:08.392+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-08 00:00:00+00:00, run_id=scheduled__2023-09-08T00:00:00+00:00, run_start_date=2024-05-22 11:24:41.635485+00:00, run_end_date=2024-05-22 12:14:08.392264+00:00, run_duration=2966.756779, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-08 00:00:00+00:00, data_interval_end=2023-09-09 00:00:00+00:00, dag_hash=329de3e2a0f757d6609ae0ccf03abd1d[0m
[[34m2024-05-22T12:14:08.395+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-09 00:00:00+00:00, run_after=2023-09-10 00:00:00+00:00[0m
[[34m2024-05-22T12:14:08.406+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:14:08.407+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T12:14:08.407+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T12:14:08.407+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T12:14:08.407+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:14:08.410+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T12:14:08.410+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:14:08.411+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T12:14:08.411+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:14:08.411+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T12:14:08.411+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:14:08.413+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:14:09.846+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:14:10.253+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:18:23.236+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:18:24.501+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:18:24.886+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:18:27.104+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:18:28.379+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:18:28.727+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:23:31.328+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:23:31.329+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:23:31.329+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:23:31.334+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:18:24.990263+00:00, run_end_date=2024-05-22 12:18:26.214601+00:00, run_duration=1.224338, state=success, executor_state=success, try_number=1, max_tries=2, job_id=573, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 12:14:08.408260+00:00, queued_by_job_id=276, pid=109038[0m
[[34m2024-05-22T12:23:31.335+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:14:10.357528+00:00, run_end_date=2024-05-22 12:18:22.440629+00:00, run_duration=252.083101, state=success, executor_state=success, try_number=1, max_tries=2, job_id=572, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 12:14:08.408260+00:00, queued_by_job_id=276, pid=108868[0m
[[34m2024-05-22T12:23:31.335+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:18:28.820775+00:00, run_end_date=2024-05-22 12:23:30.703757+00:00, run_duration=301.882982, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=574, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 12:14:08.408260+00:00, queued_by_job_id=276, pid=109066[0m
[[34m2024-05-22T12:23:31.367+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T12:23:31.385+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-17 00:00:00+00:00, run_after=2023-09-18 00:00:00+00:00[0m
[[34m2024-05-22T12:23:31.473+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-09 00:00:00+00:00: scheduled__2023-09-09T00:00:00+00:00, state:running, queued_at: 2024-05-22 11:32:20.959296+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T12:23:31.474+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-09 00:00:00+00:00, run_id=scheduled__2023-09-09T00:00:00+00:00, run_start_date=2024-05-22 11:32:20.974373+00:00, run_end_date=2024-05-22 12:23:31.474308+00:00, run_duration=3070.499935, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-09 00:00:00+00:00, data_interval_end=2023-09-10 00:00:00+00:00, dag_hash=3ac6dc76cee9daae7ecc6581f96b24f4[0m
[[34m2024-05-22T12:23:31.477+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-10 00:00:00+00:00, run_after=2023-09-11 00:00:00+00:00[0m
[[34m2024-05-22T12:23:31.488+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-14T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:23:31.488+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T12:23:31.489+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T12:23:31.489+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T12:23:31.489+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-14T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:23:31.492+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T12:23:31.493+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:23:31.493+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T12:23:31.493+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:23:31.493+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T12:23:31.494+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:23:31.495+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:23:33.386+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:23:33.728+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:24:41.002+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:24:42.274+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:24:42.626+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:24:44.935+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:24:46.317+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:24:46.737+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:29:49.486+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:29:49.486+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:29:49.486+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:29:49.491+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:24:46.869328+00:00, run_end_date=2024-05-22 12:29:48.728599+00:00, run_duration=301.859271, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=577, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 12:23:31.491293+00:00, queued_by_job_id=276, pid=109354[0m
[[34m2024-05-22T12:29:49.492+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:23:33.821609+00:00, run_end_date=2024-05-22 12:24:40.397714+00:00, run_duration=66.576105, state=success, executor_state=success, try_number=1, max_tries=2, job_id=575, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 12:23:31.491293+00:00, queued_by_job_id=276, pid=109274[0m
[[34m2024-05-22T12:29:49.492+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:24:42.718972+00:00, run_end_date=2024-05-22 12:24:44.269825+00:00, run_duration=1.550853, state=success, executor_state=success, try_number=1, max_tries=2, job_id=576, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 12:23:31.491293+00:00, queued_by_job_id=276, pid=109330[0m
[[34m2024-05-22T12:29:49.525+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T12:29:49.547+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-18 00:00:00+00:00, run_after=2023-09-19 00:00:00+00:00[0m
[[34m2024-05-22T12:29:49.636+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-10 00:00:00+00:00: scheduled__2023-09-10T00:00:00+00:00, state:running, queued_at: 2024-05-22 11:39:10.096008+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T12:29:49.637+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-10 00:00:00+00:00, run_id=scheduled__2023-09-10T00:00:00+00:00, run_start_date=2024-05-22 11:39:10.117766+00:00, run_end_date=2024-05-22 12:29:49.637013+00:00, run_duration=3039.519247, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-10 00:00:00+00:00, data_interval_end=2023-09-11 00:00:00+00:00, dag_hash=12f5c464e86fcdd02047b5abc7d73b4c[0m
[[34m2024-05-22T12:29:49.640+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-11 00:00:00+00:00, run_after=2023-09-12 00:00:00+00:00[0m
[[34m2024-05-22T12:29:49.660+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:29:49.660+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T12:29:49.661+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T12:29:49.661+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T12:29:49.661+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:29:49.663+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T12:29:49.663+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:29:49.664+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T12:29:49.664+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:29:49.664+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T12:29:49.664+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:29:49.666+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:29:51.006+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:29:51.427+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:32:53.349+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:32:54.539+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:32:54.886+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:32:56.766+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:32:58.436+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:32:58.915+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:38:01.883+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:38:01.884+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:38:01.884+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:38:01.890+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:29:51.557125+00:00, run_end_date=2024-05-22 12:32:52.560416+00:00, run_duration=181.003291, state=success, executor_state=success, try_number=1, max_tries=2, job_id=578, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 12:29:49.662074+00:00, queued_by_job_id=276, pid=109554[0m
[[34m2024-05-22T12:38:01.891+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:32:54.981818+00:00, run_end_date=2024-05-22 12:32:56.118115+00:00, run_duration=1.136297, state=success, executor_state=success, try_number=1, max_tries=2, job_id=579, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 12:29:49.662074+00:00, queued_by_job_id=276, pid=109681[0m
[[34m2024-05-22T12:38:01.891+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:32:59.044427+00:00, run_end_date=2024-05-22 12:38:01.136171+00:00, run_duration=302.091744, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=580, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 12:29:49.662074+00:00, queued_by_job_id=276, pid=109705[0m
[[34m2024-05-22T12:38:01.930+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T12:38:01.952+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-19 00:00:00+00:00, run_after=2023-09-20 00:00:00+00:00[0m
[[34m2024-05-22T12:38:02.064+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-11 00:00:00+00:00: scheduled__2023-09-11T00:00:00+00:00, state:running, queued_at: 2024-05-22 11:46:19.479178+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T12:38:02.064+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-11 00:00:00+00:00, run_id=scheduled__2023-09-11T00:00:00+00:00, run_start_date=2024-05-22 11:46:19.492767+00:00, run_end_date=2024-05-22 12:38:02.064479+00:00, run_duration=3102.571712, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-11 00:00:00+00:00, data_interval_end=2023-09-12 00:00:00+00:00, dag_hash=079079925bf9e903b9a45752ad1fe320[0m
[[34m2024-05-22T12:38:02.068+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-12 00:00:00+00:00, run_after=2023-09-13 00:00:00+00:00[0m
[[34m2024-05-22T12:38:02.079+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:38:02.079+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T12:38:02.079+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T12:38:02.079+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T12:38:02.080+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:38:02.082+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T12:38:02.082+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:38:02.083+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T12:38:02.083+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:38:02.083+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T12:38:02.083+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:38:02.085+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:38:03.678+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:38:04.064+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:39:17.264+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:39:18.814+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:39:19.542+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:39:21.627+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:39:23.011+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:39:23.612+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:44:26.400+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:44:26.400+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:44:26.401+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:44:26.420+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:39:23.715209+00:00, run_end_date=2024-05-22 12:44:25.769894+00:00, run_duration=302.054685, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=583, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 12:38:02.081074+00:00, queued_by_job_id=276, pid=109990[0m
[[34m2024-05-22T12:44:26.420+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:38:04.166754+00:00, run_end_date=2024-05-22 12:39:16.586826+00:00, run_duration=72.420072, state=success, executor_state=success, try_number=1, max_tries=2, job_id=581, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 12:38:02.081074+00:00, queued_by_job_id=276, pid=109903[0m
[[34m2024-05-22T12:44:26.421+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:39:19.750794+00:00, run_end_date=2024-05-22 12:39:20.979053+00:00, run_duration=1.228259, state=success, executor_state=success, try_number=1, max_tries=2, job_id=582, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 12:38:02.081074+00:00, queued_by_job_id=276, pid=109966[0m
[[34m2024-05-22T12:44:26.454+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T12:44:26.474+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-20 00:00:00+00:00, run_after=2023-09-21 00:00:00+00:00[0m
[[34m2024-05-22T12:44:26.560+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-12 00:00:00+00:00: scheduled__2023-09-12T00:00:00+00:00, state:running, queued_at: 2024-05-22 11:54:10.187873+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T12:44:26.561+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-12 00:00:00+00:00, run_id=scheduled__2023-09-12T00:00:00+00:00, run_start_date=2024-05-22 11:54:10.201489+00:00, run_end_date=2024-05-22 12:44:26.560966+00:00, run_duration=3016.359477, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-12 00:00:00+00:00, data_interval_end=2023-09-13 00:00:00+00:00, dag_hash=02cf826eed0b9abd36bd42ed95ace21c[0m
[[34m2024-05-22T12:44:26.564+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-13 00:00:00+00:00, run_after=2023-09-14 00:00:00+00:00[0m
[[34m2024-05-22T12:44:26.573+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:44:26.574+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T12:44:26.574+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T12:44:26.574+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T12:44:26.574+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:44:26.576+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T12:44:26.576+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:44:26.577+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T12:44:26.577+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:44:26.577+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T12:44:26.577+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:44:26.579+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:44:27.816+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:44:28.208+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:45:23.817+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:45:24.975+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:45:25.325+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:45:27.500+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:45:28.814+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:45:29.208+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:50:31.868+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:50:31.869+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:50:31.869+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:50:31.874+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:44:28.336230+00:00, run_end_date=2024-05-22 12:45:23.250221+00:00, run_duration=54.913991, state=success, executor_state=success, try_number=1, max_tries=2, job_id=584, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 12:44:26.575210+00:00, queued_by_job_id=276, pid=110190[0m
[[34m2024-05-22T12:50:31.874+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:45:25.419261+00:00, run_end_date=2024-05-22 12:45:26.536221+00:00, run_duration=1.11696, state=success, executor_state=success, try_number=1, max_tries=2, job_id=585, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 12:44:26.575210+00:00, queued_by_job_id=276, pid=110239[0m
[[34m2024-05-22T12:50:31.874+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:45:29.316160+00:00, run_end_date=2024-05-22 12:50:31.254137+00:00, run_duration=301.937977, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=586, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 12:44:26.575210+00:00, queued_by_job_id=276, pid=110267[0m
[[34m2024-05-22T12:50:31.906+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T12:50:31.925+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-21 00:00:00+00:00, run_after=2023-09-22 00:00:00+00:00[0m
[[34m2024-05-22T12:50:32.022+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-13 00:00:00+00:00: scheduled__2023-09-13T00:00:00+00:00, state:running, queued_at: 2024-05-22 12:00:34.670148+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T12:50:32.023+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-13 00:00:00+00:00, run_id=scheduled__2023-09-13T00:00:00+00:00, run_start_date=2024-05-22 12:00:34.683638+00:00, run_end_date=2024-05-22 12:50:32.023158+00:00, run_duration=2997.33952, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-13 00:00:00+00:00, data_interval_end=2023-09-14 00:00:00+00:00, dag_hash=a0c5bbbce386aeda47734a31d2086184[0m
[[34m2024-05-22T12:50:32.027+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-14 00:00:00+00:00, run_after=2023-09-15 00:00:00+00:00[0m
[[34m2024-05-22T12:50:32.037+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-18T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:50:32.038+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T12:50:32.038+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T12:50:32.038+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T12:50:32.038+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-18T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:50:32.040+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T12:50:32.041+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:50:32.041+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T12:50:32.041+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:50:32.041+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T12:50:32.042+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:50:32.043+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:50:33.867+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:50:34.217+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:51:28.533+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:51:30.006+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:51:30.428+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:51:32.456+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:51:34.365+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:51:34.926+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:56:37.942+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:56:37.942+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:56:37.943+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T12:56:37.948+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:50:34.312408+00:00, run_end_date=2024-05-22 12:51:27.855367+00:00, run_duration=53.542959, state=success, executor_state=success, try_number=1, max_tries=2, job_id=587, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 12:50:32.039266+00:00, queued_by_job_id=276, pid=110479[0m
[[34m2024-05-22T12:56:37.948+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:51:30.548834+00:00, run_end_date=2024-05-22 12:51:31.816818+00:00, run_duration=1.267984, state=success, executor_state=success, try_number=1, max_tries=2, job_id=588, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 12:50:32.039266+00:00, queued_by_job_id=276, pid=110529[0m
[[34m2024-05-22T12:56:37.949+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:51:35.023876+00:00, run_end_date=2024-05-22 12:56:37.025061+00:00, run_duration=302.001185, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=589, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 12:50:32.039266+00:00, queued_by_job_id=276, pid=110558[0m
[[34m2024-05-22T12:56:37.990+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T12:56:38.019+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-22 00:00:00+00:00, run_after=2023-09-23 00:00:00+00:00[0m
[[34m2024-05-22T12:56:38.208+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-14 00:00:00+00:00: scheduled__2023-09-14T00:00:00+00:00, state:running, queued_at: 2024-05-22 12:07:12.667114+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T12:56:38.208+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-14 00:00:00+00:00, run_id=scheduled__2023-09-14T00:00:00+00:00, run_start_date=2024-05-22 12:07:12.693953+00:00, run_end_date=2024-05-22 12:56:38.208584+00:00, run_duration=2965.514631, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-14 00:00:00+00:00, data_interval_end=2023-09-15 00:00:00+00:00, dag_hash=cc23396c13931e7d4de29541aa394e09[0m
[[34m2024-05-22T12:56:38.212+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-15 00:00:00+00:00, run_after=2023-09-16 00:00:00+00:00[0m
[[34m2024-05-22T12:56:38.224+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-19T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:56:38.224+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T12:56:38.224+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T12:56:38.224+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T12:56:38.225+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-19T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T12:56:38.227+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T12:56:38.228+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:56:38.228+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T12:56:38.228+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:56:38.228+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T12:56:38.228+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:56:38.230+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:56:39.650+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:56:40.022+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:57:22.120+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:57:23.631+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:57:24.084+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T12:57:26.660+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T12:57:28.283+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T12:57:28.815+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:02:31.811+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:02:31.811+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:02:31.811+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:02:31.817+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:56:40.155462+00:00, run_end_date=2024-05-22 12:57:21.537935+00:00, run_duration=41.382473, state=success, executor_state=success, try_number=1, max_tries=2, job_id=590, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 12:56:38.225838+00:00, queued_by_job_id=276, pid=110752[0m
[[34m2024-05-22T13:02:31.818+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:57:24.241051+00:00, run_end_date=2024-05-22 12:57:25.798229+00:00, run_duration=1.557178, state=success, executor_state=success, try_number=1, max_tries=2, job_id=591, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 12:56:38.225838+00:00, queued_by_job_id=276, pid=110797[0m
[[34m2024-05-22T13:02:31.818+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 12:57:28.981889+00:00, run_end_date=2024-05-22 13:02:31.066878+00:00, run_duration=302.084989, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=592, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 12:56:38.225838+00:00, queued_by_job_id=276, pid=110821[0m
[[34m2024-05-22T13:02:31.861+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T13:02:31.884+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-23 00:00:00+00:00, run_after=2023-09-24 00:00:00+00:00[0m
[[34m2024-05-22T13:02:31.992+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-15 00:00:00+00:00: scheduled__2023-09-15T00:00:00+00:00, state:running, queued_at: 2024-05-22 12:14:08.276918+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T13:02:31.992+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-15 00:00:00+00:00, run_id=scheduled__2023-09-15T00:00:00+00:00, run_start_date=2024-05-22 12:14:08.296365+00:00, run_end_date=2024-05-22 13:02:31.992499+00:00, run_duration=2903.696134, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-15 00:00:00+00:00, data_interval_end=2023-09-16 00:00:00+00:00, dag_hash=f19230a645997f9492453e60fd7ed96b[0m
[[34m2024-05-22T13:02:31.996+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-16 00:00:00+00:00, run_after=2023-09-17 00:00:00+00:00[0m
[[34m2024-05-22T13:02:32.009+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-20T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:02:32.010+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T13:02:32.010+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T13:02:32.010+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T13:02:32.010+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-20T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:02:32.013+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T13:02:32.014+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:02:32.014+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T13:02:32.014+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:02:32.014+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T13:02:32.014+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:02:32.016+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:02:33.652+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:02:34.202+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:03:56.547+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:03:57.929+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:03:58.335+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:04:00.372+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:04:01.670+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:04:02.080+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:09:04.981+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:09:04.981+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:09:04.981+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:09:04.987+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:03:58.439665+00:00, run_end_date=2024-05-22 13:03:59.627470+00:00, run_duration=1.187805, state=success, executor_state=success, try_number=1, max_tries=2, job_id=594, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 13:02:32.011158+00:00, queued_by_job_id=276, pid=111089[0m
[[34m2024-05-22T13:09:04.987+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:02:34.348587+00:00, run_end_date=2024-05-22 13:03:55.891569+00:00, run_duration=81.542982, state=success, executor_state=success, try_number=1, max_tries=2, job_id=593, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 13:02:32.011158+00:00, queued_by_job_id=276, pid=111021[0m
[[34m2024-05-22T13:09:04.987+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:04:02.182122+00:00, run_end_date=2024-05-22 13:09:04.314157+00:00, run_duration=302.132035, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=595, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 13:02:32.011158+00:00, queued_by_job_id=276, pid=111113[0m
[[34m2024-05-22T13:09:05.022+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T13:09:05.042+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-24 00:00:00+00:00, run_after=2023-09-25 00:00:00+00:00[0m
[[34m2024-05-22T13:09:05.134+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-16 00:00:00+00:00: scheduled__2023-09-16T00:00:00+00:00, state:running, queued_at: 2024-05-22 12:23:31.379414+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T13:09:05.135+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-16 00:00:00+00:00, run_id=scheduled__2023-09-16T00:00:00+00:00, run_start_date=2024-05-22 12:23:31.392383+00:00, run_end_date=2024-05-22 13:09:05.135275+00:00, run_duration=2733.742892, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-16 00:00:00+00:00, data_interval_end=2023-09-17 00:00:00+00:00, dag_hash=33606ca2c16e4b4381cda0dcc2ab53ac[0m
[[34m2024-05-22T13:09:05.138+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-17 00:00:00+00:00, run_after=2023-09-18 00:00:00+00:00[0m
[[34m2024-05-22T13:09:05.148+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:09:05.148+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T13:09:05.149+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T13:09:05.149+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T13:09:05.149+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:09:05.151+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T13:09:05.151+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:09:05.152+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T13:09:05.152+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:09:05.152+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T13:09:05.152+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:09:05.153+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:09:06.747+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:09:07.146+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:13:08.746+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:13:10.004+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:13:10.417+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:13:12.496+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:13:14.154+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:13:14.633+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:18:17.544+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:18:17.544+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:18:17.544+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:18:17.564+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:13:14.763051+00:00, run_end_date=2024-05-22 13:18:16.938279+00:00, run_duration=302.175228, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=598, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 13:09:05.150044+00:00, queued_by_job_id=276, pid=111495[0m
[[34m2024-05-22T13:18:17.565+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:13:10.513537+00:00, run_end_date=2024-05-22 13:13:11.789390+00:00, run_duration=1.275853, state=success, executor_state=success, try_number=1, max_tries=2, job_id=597, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 13:09:05.150044+00:00, queued_by_job_id=276, pid=111470[0m
[[34m2024-05-22T13:18:17.565+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:09:07.254992+00:00, run_end_date=2024-05-22 13:13:08.114914+00:00, run_duration=240.859922, state=success, executor_state=success, try_number=1, max_tries=2, job_id=596, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 13:09:05.150044+00:00, queued_by_job_id=276, pid=111310[0m
[[34m2024-05-22T13:18:17.600+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T13:18:17.622+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-25 00:00:00+00:00, run_after=2023-09-26 00:00:00+00:00[0m
[[34m2024-05-22T13:18:17.715+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-17 00:00:00+00:00: scheduled__2023-09-17T00:00:00+00:00, state:running, queued_at: 2024-05-22 12:29:49.541129+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T13:18:17.715+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-17 00:00:00+00:00, run_id=scheduled__2023-09-17T00:00:00+00:00, run_start_date=2024-05-22 12:29:49.554155+00:00, run_end_date=2024-05-22 13:18:17.715893+00:00, run_duration=2908.161738, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-17 00:00:00+00:00, data_interval_end=2023-09-18 00:00:00+00:00, dag_hash=b06bfde1100655dbe35e2009efb07444[0m
[[34m2024-05-22T13:18:17.719+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-18 00:00:00+00:00, run_after=2023-09-19 00:00:00+00:00[0m
[[34m2024-05-22T13:18:17.731+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-22T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:18:17.731+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T13:18:17.732+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T13:18:17.732+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T13:18:17.732+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-22T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:18:17.735+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T13:18:17.735+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:18:17.736+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T13:18:17.736+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:18:17.736+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T13:18:17.736+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:18:17.738+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:18:19.092+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:18:19.473+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:19:38.912+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:19:40.613+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:19:41.027+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:19:43.075+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:19:44.605+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:19:44.987+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:24:47.638+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:24:47.638+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:24:47.638+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:24:47.643+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:19:45.119513+00:00, run_end_date=2024-05-22 13:24:47.069386+00:00, run_duration=301.949873, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=601, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 13:18:17.733317+00:00, queued_by_job_id=276, pid=111787[0m
[[34m2024-05-22T13:24:47.644+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:19:41.167458+00:00, run_end_date=2024-05-22 13:19:42.417584+00:00, run_duration=1.250126, state=success, executor_state=success, try_number=1, max_tries=2, job_id=600, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 13:18:17.733317+00:00, queued_by_job_id=276, pid=111763[0m
[[34m2024-05-22T13:24:47.644+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:18:19.605406+00:00, run_end_date=2024-05-22 13:19:38.189324+00:00, run_duration=78.583918, state=success, executor_state=success, try_number=1, max_tries=2, job_id=599, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 13:18:17.733317+00:00, queued_by_job_id=276, pid=111698[0m
[[34m2024-05-22T13:24:47.694+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T13:24:47.716+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-26 00:00:00+00:00, run_after=2023-09-27 00:00:00+00:00[0m
[[34m2024-05-22T13:24:47.805+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-18 00:00:00+00:00: scheduled__2023-09-18T00:00:00+00:00, state:running, queued_at: 2024-05-22 12:38:01.944908+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T13:24:47.806+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-18 00:00:00+00:00, run_id=scheduled__2023-09-18T00:00:00+00:00, run_start_date=2024-05-22 12:38:01.961471+00:00, run_end_date=2024-05-22 13:24:47.806081+00:00, run_duration=2805.84461, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-18 00:00:00+00:00, data_interval_end=2023-09-19 00:00:00+00:00, dag_hash=727c911ce68fbf73992da0902f703d53[0m
[[34m2024-05-22T13:24:47.809+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-19 00:00:00+00:00, run_after=2023-09-20 00:00:00+00:00[0m
[[34m2024-05-22T13:24:47.819+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-23T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:24:47.819+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T13:24:47.820+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T13:24:47.820+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T13:24:47.820+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-23T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:24:47.822+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T13:24:47.822+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:24:47.822+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T13:24:47.823+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:24:47.823+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T13:24:47.823+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:24:47.824+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:24:48.996+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:24:49.374+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:25:59.900+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:26:01.342+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:26:01.861+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:26:04.085+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:26:05.436+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:26:05.849+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:31:08.841+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:31:08.841+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:31:08.841+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:31:08.846+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:26:06.011555+00:00, run_end_date=2024-05-22 13:31:08.137375+00:00, run_duration=302.12582, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=604, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 13:24:47.820980+00:00, queued_by_job_id=276, pid=112069[0m
[[34m2024-05-22T13:31:08.847+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:24:49.471065+00:00, run_end_date=2024-05-22 13:25:59.208167+00:00, run_duration=69.737102, state=success, executor_state=success, try_number=1, max_tries=2, job_id=602, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 13:24:47.820980+00:00, queued_by_job_id=276, pid=111985[0m
[[34m2024-05-22T13:31:08.847+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:26:01.976941+00:00, run_end_date=2024-05-22 13:26:03.362555+00:00, run_duration=1.385614, state=success, executor_state=success, try_number=1, max_tries=2, job_id=603, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 13:24:47.820980+00:00, queued_by_job_id=276, pid=112045[0m
[[34m2024-05-22T13:31:08.881+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T13:31:08.903+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-27 00:00:00+00:00, run_after=2023-09-28 00:00:00+00:00[0m
[[34m2024-05-22T13:31:09.002+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-19 00:00:00+00:00: scheduled__2023-09-19T00:00:00+00:00, state:running, queued_at: 2024-05-22 12:44:26.467589+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T13:31:09.002+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-19 00:00:00+00:00, run_id=scheduled__2023-09-19T00:00:00+00:00, run_start_date=2024-05-22 12:44:26.480690+00:00, run_end_date=2024-05-22 13:31:09.002944+00:00, run_duration=2802.522254, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-19 00:00:00+00:00, data_interval_end=2023-09-20 00:00:00+00:00, dag_hash=e0f191207bd3da36c38ee801c36884b8[0m
[[34m2024-05-22T13:31:09.006+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-20 00:00:00+00:00, run_after=2023-09-21 00:00:00+00:00[0m
[[34m2024-05-22T13:31:09.017+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-24T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:31:09.018+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T13:31:09.018+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T13:31:09.018+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T13:31:09.018+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-24T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:31:09.021+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T13:31:09.021+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:31:09.022+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T13:31:09.022+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:31:09.022+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T13:31:09.022+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:31:09.023+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:31:10.344+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:31:10.789+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:33:01.296+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:33:02.810+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:33:03.271+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:33:05.566+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:33:06.991+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:33:07.427+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:38:10.304+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:38:10.304+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:38:10.304+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:38:10.310+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:31:10.900579+00:00, run_end_date=2024-05-22 13:33:00.671859+00:00, run_duration=109.77128, state=success, executor_state=success, try_number=1, max_tries=2, job_id=605, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 13:31:09.019516+00:00, queued_by_job_id=276, pid=112268[0m
[[34m2024-05-22T13:38:10.310+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:33:03.436218+00:00, run_end_date=2024-05-22 13:33:04.897358+00:00, run_duration=1.46114, state=success, executor_state=success, try_number=1, max_tries=2, job_id=606, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 13:31:09.019516+00:00, queued_by_job_id=276, pid=112352[0m
[[34m2024-05-22T13:38:10.310+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:33:07.555142+00:00, run_end_date=2024-05-22 13:38:09.635366+00:00, run_duration=302.080224, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=607, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 13:31:09.019516+00:00, queued_by_job_id=276, pid=112376[0m
[[34m2024-05-22T13:38:10.343+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T13:38:10.361+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-28 00:00:00+00:00, run_after=2023-09-29 00:00:00+00:00[0m
[[34m2024-05-22T13:38:10.447+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-20 00:00:00+00:00: scheduled__2023-09-20T00:00:00+00:00, state:running, queued_at: 2024-05-22 12:50:31.919009+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T13:38:10.448+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-20 00:00:00+00:00, run_id=scheduled__2023-09-20T00:00:00+00:00, run_start_date=2024-05-22 12:50:31.931653+00:00, run_end_date=2024-05-22 13:38:10.447968+00:00, run_duration=2858.516315, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-20 00:00:00+00:00, data_interval_end=2023-09-21 00:00:00+00:00, dag_hash=708bdba5ee18994dd1ce71dc0e15122a[0m
[[34m2024-05-22T13:38:10.451+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-21 00:00:00+00:00, run_after=2023-09-22 00:00:00+00:00[0m
[[34m2024-05-22T13:38:10.460+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-25T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:38:10.461+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T13:38:10.461+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T13:38:10.461+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T13:38:10.461+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-25T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:38:10.463+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T13:38:10.463+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:38:10.464+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T13:38:10.464+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:38:10.464+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T13:38:10.464+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:38:10.465+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:38:11.693+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:38:12.109+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:40:55.956+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:40:57.419+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:40:57.816+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:40:59.781+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:41:01.456+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:41:01.992+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:46:04.921+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:46:04.922+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:46:04.922+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:46:04.927+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:38:12.235345+00:00, run_end_date=2024-05-22 13:40:55.298528+00:00, run_duration=163.063183, state=success, executor_state=success, try_number=1, max_tries=2, job_id=608, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 13:38:10.462141+00:00, queued_by_job_id=276, pid=112574[0m
[[34m2024-05-22T13:46:04.928+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:40:57.922288+00:00, run_end_date=2024-05-22 13:40:59.159594+00:00, run_duration=1.237306, state=success, executor_state=success, try_number=1, max_tries=2, job_id=609, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 13:38:10.462141+00:00, queued_by_job_id=276, pid=112688[0m
[[34m2024-05-22T13:46:04.928+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:41:02.109188+00:00, run_end_date=2024-05-22 13:46:04.301311+00:00, run_duration=302.192123, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=610, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 13:38:10.462141+00:00, queued_by_job_id=276, pid=112712[0m
[[34m2024-05-22T13:46:04.964+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T13:46:04.983+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-29 00:00:00+00:00, run_after=2023-09-30 00:00:00+00:00[0m
[[34m2024-05-22T13:46:05.076+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-21 00:00:00+00:00: scheduled__2023-09-21T00:00:00+00:00, state:running, queued_at: 2024-05-22 12:56:38.011714+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T13:46:05.077+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-21 00:00:00+00:00, run_id=scheduled__2023-09-21T00:00:00+00:00, run_start_date=2024-05-22 12:56:38.028662+00:00, run_end_date=2024-05-22 13:46:05.077297+00:00, run_duration=2967.048635, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-21 00:00:00+00:00, data_interval_end=2023-09-22 00:00:00+00:00, dag_hash=793334edf7b60c1283ec0d188fbaaebc[0m
[[34m2024-05-22T13:46:05.080+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-22 00:00:00+00:00, run_after=2023-09-23 00:00:00+00:00[0m
[[34m2024-05-22T13:46:05.090+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-26T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:46:05.091+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T13:46:05.091+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T13:46:05.091+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T13:46:05.091+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-26T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:46:05.095+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T13:46:05.095+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:46:05.095+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T13:46:05.096+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:46:05.096+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T13:46:05.096+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:46:05.097+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:46:06.405+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:46:06.846+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:47:36.826+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:47:38.161+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:47:38.593+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:47:40.598+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:47:42.215+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:47:42.638+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:52:45.931+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:52:45.931+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:52:45.931+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:52:45.950+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:46:07.012770+00:00, run_end_date=2024-05-22 13:47:36.198851+00:00, run_duration=89.186081, state=success, executor_state=success, try_number=1, max_tries=2, job_id=611, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 13:46:05.093478+00:00, queued_by_job_id=276, pid=112910[0m
[[34m2024-05-22T13:52:45.951+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:47:38.701587+00:00, run_end_date=2024-05-22 13:47:39.964063+00:00, run_duration=1.262476, state=success, executor_state=success, try_number=1, max_tries=2, job_id=612, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 13:46:05.093478+00:00, queued_by_job_id=276, pid=112982[0m
[[34m2024-05-22T13:52:45.951+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:47:42.752285+00:00, run_end_date=2024-05-22 13:52:45.270082+00:00, run_duration=302.517797, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=613, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 13:46:05.093478+00:00, queued_by_job_id=276, pid=113006[0m
[[34m2024-05-22T13:52:45.986+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T13:52:46.008+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-30 00:00:00+00:00, run_after=2023-10-01 00:00:00+00:00[0m
[[34m2024-05-22T13:52:46.107+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-22 00:00:00+00:00: scheduled__2023-09-22T00:00:00+00:00, state:running, queued_at: 2024-05-22 13:02:31.875771+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T13:52:46.107+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-22 00:00:00+00:00, run_id=scheduled__2023-09-22T00:00:00+00:00, run_start_date=2024-05-22 13:02:31.891557+00:00, run_end_date=2024-05-22 13:52:46.107422+00:00, run_duration=3014.215865, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-22 00:00:00+00:00, data_interval_end=2023-09-23 00:00:00+00:00, dag_hash=7f08504d40275d7b637c69a0a1e2f2f9[0m
[[34m2024-05-22T13:52:46.110+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-23 00:00:00+00:00, run_after=2023-09-24 00:00:00+00:00[0m
[[34m2024-05-22T13:52:46.121+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-27T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:52:46.121+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T13:52:46.121+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T13:52:46.121+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T13:52:46.121+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-27T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:52:46.125+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T13:52:46.125+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:52:46.126+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T13:52:46.126+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:52:46.126+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T13:52:46.126+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:52:46.128+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:52:47.513+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:52:47.937+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:54:00.468+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:54:01.723+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:54:02.194+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:54:04.413+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:54:05.663+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:54:06.022+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T13:59:08.749+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:59:08.749+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:59:08.749+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T13:59:08.761+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:52:48.037646+00:00, run_end_date=2024-05-22 13:53:59.747939+00:00, run_duration=71.710293, state=success, executor_state=success, try_number=1, max_tries=2, job_id=614, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 13:52:46.122600+00:00, queued_by_job_id=276, pid=113204[0m
[[34m2024-05-22T13:59:08.761+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:54:02.323608+00:00, run_end_date=2024-05-22 13:54:03.720564+00:00, run_duration=1.396956, state=success, executor_state=success, try_number=1, max_tries=2, job_id=615, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 13:52:46.122600+00:00, queued_by_job_id=276, pid=113269[0m
[[34m2024-05-22T13:59:08.761+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:54:06.118886+00:00, run_end_date=2024-05-22 13:59:08.156822+00:00, run_duration=302.037936, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=616, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 13:52:46.122600+00:00, queued_by_job_id=276, pid=113293[0m
[[34m2024-05-22T13:59:08.826+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T13:59:08.844+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-01 00:00:00+00:00, run_after=2023-10-02 00:00:00+00:00[0m
[[34m2024-05-22T13:59:08.931+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-23 00:00:00+00:00: scheduled__2023-09-23T00:00:00+00:00, state:running, queued_at: 2024-05-22 13:09:05.035613+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T13:59:08.931+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-23 00:00:00+00:00, run_id=scheduled__2023-09-23T00:00:00+00:00, run_start_date=2024-05-22 13:09:05.050948+00:00, run_end_date=2024-05-22 13:59:08.931875+00:00, run_duration=3003.880927, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-23 00:00:00+00:00, data_interval_end=2023-09-24 00:00:00+00:00, dag_hash=42cc5ab6d6fa56d46c42a98542071148[0m
[[34m2024-05-22T13:59:08.935+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-24 00:00:00+00:00, run_after=2023-09-25 00:00:00+00:00[0m
[[34m2024-05-22T13:59:08.944+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-28T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:59:08.945+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T13:59:08.945+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T13:59:08.945+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T13:59:08.945+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-28T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T13:59:08.948+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T13:59:08.948+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:59:08.948+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T13:59:08.948+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:59:08.949+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T13:59:08.949+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:59:08.950+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-09-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T13:59:10.167+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T13:59:10.611+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-09-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:00:37.080+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:00:38.493+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:00:38.914+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:00:41.098+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:00:42.415+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:00:42.771+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:05:45.544+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-09-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:05:45.544+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:05:45.545+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:05:45.551+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-09-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 13:59:10.724193+00:00, run_end_date=2024-05-22 14:00:36.469918+00:00, run_duration=85.745725, state=success, executor_state=success, try_number=1, max_tries=2, job_id=617, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 13:59:08.946291+00:00, queued_by_job_id=276, pid=113505[0m
[[34m2024-05-22T14:05:45.552+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:00:39.023165+00:00, run_end_date=2024-05-22 14:00:40.424394+00:00, run_duration=1.401229, state=success, executor_state=success, try_number=1, max_tries=2, job_id=618, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 13:59:08.946291+00:00, queued_by_job_id=276, pid=113582[0m
[[34m2024-05-22T14:05:45.552+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:00:42.871989+00:00, run_end_date=2024-05-22 14:05:44.764224+00:00, run_duration=301.892235, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=619, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 13:59:08.946291+00:00, queued_by_job_id=276, pid=113606[0m
[[34m2024-05-22T14:05:45.584+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T14:05:45.603+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-02 00:00:00+00:00, run_after=2023-10-03 00:00:00+00:00[0m
[[34m2024-05-22T14:05:45.693+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-24 00:00:00+00:00: scheduled__2023-09-24T00:00:00+00:00, state:running, queued_at: 2024-05-22 13:18:17.614557+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T14:05:45.693+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-24 00:00:00+00:00, run_id=scheduled__2023-09-24T00:00:00+00:00, run_start_date=2024-05-22 13:18:17.630389+00:00, run_end_date=2024-05-22 14:05:45.693572+00:00, run_duration=2848.063183, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-24 00:00:00+00:00, data_interval_end=2023-09-25 00:00:00+00:00, dag_hash=3bdb9ac73dccf72b76bd228bf9e5fe12[0m
[[34m2024-05-22T14:05:45.697+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-25 00:00:00+00:00, run_after=2023-09-26 00:00:00+00:00[0m
[[34m2024-05-22T14:05:45.706+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-29T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:05:45.707+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T14:05:45.707+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T14:05:45.707+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T14:05:45.707+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-29T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:05:45.709+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T14:05:45.709+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:05:45.710+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T14:05:45.710+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:05:45.710+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T14:05:45.710+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:05:45.711+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:05:47.047+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:05:47.510+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:07:11.086+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-09-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:07:12.347+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:07:12.703+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-09-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:07:14.778+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:07:16.047+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:07:16.436+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:12:18.972+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:12:18.972+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-09-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:12:18.972+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:12:18.977+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-09-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:07:12.800968+00:00, run_end_date=2024-05-22 14:07:14.140324+00:00, run_duration=1.339356, state=success, executor_state=success, try_number=1, max_tries=2, job_id=621, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 14:05:45.708203+00:00, queued_by_job_id=276, pid=113873[0m
[[34m2024-05-22T14:12:18.978+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:07:16.534518+00:00, run_end_date=2024-05-22 14:12:18.361738+00:00, run_duration=301.82722, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=622, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 14:05:45.708203+00:00, queued_by_job_id=276, pid=113898[0m
[[34m2024-05-22T14:12:18.978+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:05:47.630237+00:00, run_end_date=2024-05-22 14:07:10.522484+00:00, run_duration=82.892247, state=success, executor_state=success, try_number=1, max_tries=2, job_id=620, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 14:05:45.708203+00:00, queued_by_job_id=276, pid=113806[0m
[[34m2024-05-22T14:12:19.011+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T14:12:19.032+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-03 00:00:00+00:00, run_after=2023-10-04 00:00:00+00:00[0m
[[34m2024-05-22T14:12:19.119+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-25 00:00:00+00:00: scheduled__2023-09-25T00:00:00+00:00, state:running, queued_at: 2024-05-22 13:24:47.709525+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T14:12:19.119+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-25 00:00:00+00:00, run_id=scheduled__2023-09-25T00:00:00+00:00, run_start_date=2024-05-22 13:24:47.722994+00:00, run_end_date=2024-05-22 14:12:19.119943+00:00, run_duration=2851.396949, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-25 00:00:00+00:00, data_interval_end=2023-09-26 00:00:00+00:00, dag_hash=db34a9cf94c71fa43c1a10f0b146ee21[0m
[[34m2024-05-22T14:12:19.123+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-26 00:00:00+00:00, run_after=2023-09-27 00:00:00+00:00[0m
[[34m2024-05-22T14:12:19.132+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-30T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:12:19.133+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T14:12:19.133+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T14:12:19.133+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T14:12:19.133+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-30T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:12:19.135+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T14:12:19.135+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:12:19.136+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T14:12:19.136+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:12:19.136+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T14:12:19.136+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:12:19.138+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:12:20.529+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:12:20.953+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:13:10.549+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:13:11.946+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:13:12.372+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:13:15.042+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-09-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:13:16.225+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:13:16.572+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-09-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:18:19.123+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:18:19.123+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:18:19.124+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-09-30T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:18:19.130+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-09-30T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:13:16.668483+00:00, run_end_date=2024-05-22 14:18:18.579854+00:00, run_duration=301.911371, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=625, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 14:12:19.134216+00:00, queued_by_job_id=276, pid=114170[0m
[[34m2024-05-22T14:18:19.130+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:12:21.056004+00:00, run_end_date=2024-05-22 14:13:09.951066+00:00, run_duration=48.895062, state=success, executor_state=success, try_number=1, max_tries=2, job_id=623, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 14:12:19.134216+00:00, queued_by_job_id=276, pid=114097[0m
[[34m2024-05-22T14:18:19.130+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:13:12.486831+00:00, run_end_date=2024-05-22 14:13:14.242591+00:00, run_duration=1.75576, state=success, executor_state=success, try_number=1, max_tries=2, job_id=624, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 14:12:19.134216+00:00, queued_by_job_id=276, pid=114140[0m
[[34m2024-05-22T14:18:19.173+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T14:18:19.193+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-04 00:00:00+00:00, run_after=2023-10-05 00:00:00+00:00[0m
[[34m2024-05-22T14:18:19.286+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-26 00:00:00+00:00: scheduled__2023-09-26T00:00:00+00:00, state:running, queued_at: 2024-05-22 13:31:08.897010+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T14:18:19.287+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-26 00:00:00+00:00, run_id=scheduled__2023-09-26T00:00:00+00:00, run_start_date=2024-05-22 13:31:08.910986+00:00, run_end_date=2024-05-22 14:18:19.287070+00:00, run_duration=2830.376084, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-26 00:00:00+00:00, data_interval_end=2023-09-27 00:00:00+00:00, dag_hash=b47cb76ca1ab60316146ffb4eb520dd6[0m
[[34m2024-05-22T14:18:19.290+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-27 00:00:00+00:00, run_after=2023-09-28 00:00:00+00:00[0m
[[34m2024-05-22T14:18:19.299+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:18:19.300+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T14:18:19.300+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T14:18:19.300+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T14:18:19.300+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:18:19.302+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T14:18:19.303+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:18:19.303+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T14:18:19.303+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:18:19.303+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T14:18:19.304+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:18:19.305+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:18:20.506+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:18:20.908+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:20:54.154+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:20:55.379+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:20:55.807+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:20:57.834+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:20:59.247+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:20:59.658+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-01T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:26:02.448+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:26:02.450+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:26:02.451+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:26:02.473+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:20:59.764684+00:00, run_end_date=2024-05-22 14:26:01.777545+00:00, run_duration=302.012861, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=628, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 14:18:19.301407+00:00, queued_by_job_id=276, pid=114505[0m
[[34m2024-05-22T14:26:02.473+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:18:21.014996+00:00, run_end_date=2024-05-22 14:20:53.360997+00:00, run_duration=152.346001, state=success, executor_state=success, try_number=1, max_tries=2, job_id=626, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 14:18:19.301407+00:00, queued_by_job_id=276, pid=114371[0m
[[34m2024-05-22T14:26:02.474+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:20:55.922850+00:00, run_end_date=2024-05-22 14:20:57.174987+00:00, run_duration=1.252137, state=success, executor_state=success, try_number=1, max_tries=2, job_id=627, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 14:18:19.301407+00:00, queued_by_job_id=276, pid=114481[0m
[[34m2024-05-22T14:26:02.512+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T14:26:02.537+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-05 00:00:00+00:00, run_after=2023-10-06 00:00:00+00:00[0m
[[34m2024-05-22T14:26:02.644+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-27 00:00:00+00:00: scheduled__2023-09-27T00:00:00+00:00, state:running, queued_at: 2024-05-22 13:38:10.355946+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T14:26:02.645+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-27 00:00:00+00:00, run_id=scheduled__2023-09-27T00:00:00+00:00, run_start_date=2024-05-22 13:38:10.369273+00:00, run_end_date=2024-05-22 14:26:02.645077+00:00, run_duration=2872.275804, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-27 00:00:00+00:00, data_interval_end=2023-09-28 00:00:00+00:00, dag_hash=5013834a4381abe1ea0b40f8abaafedc[0m
[[34m2024-05-22T14:26:02.648+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-28 00:00:00+00:00, run_after=2023-09-29 00:00:00+00:00[0m
[[34m2024-05-22T14:26:02.660+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-02T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:26:02.660+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T14:26:02.660+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T14:26:02.661+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T14:26:02.661+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-02T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:26:02.663+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T14:26:02.663+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:26:02.664+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T14:26:02.664+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:26:02.664+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T14:26:02.664+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:26:02.666+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:26:04.073+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:26:04.553+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:26:55.002+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:26:56.456+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:26:56.880+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:26:58.849+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:27:00.089+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:27:00.468+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-02T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:32:03.501+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:32:03.501+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:32:03.501+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:32:03.508+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:26:04.662412+00:00, run_end_date=2024-05-22 14:26:54.356643+00:00, run_duration=49.694231, state=success, executor_state=success, try_number=1, max_tries=2, job_id=629, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 14:26:02.661881+00:00, queued_by_job_id=276, pid=114704[0m
[[34m2024-05-22T14:32:03.509+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:26:57.017369+00:00, run_end_date=2024-05-22 14:26:58.221906+00:00, run_duration=1.204537, state=success, executor_state=success, try_number=1, max_tries=2, job_id=630, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 14:26:02.661881+00:00, queued_by_job_id=276, pid=114752[0m
[[34m2024-05-22T14:32:03.509+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-02T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:27:00.578415+00:00, run_end_date=2024-05-22 14:32:02.693457+00:00, run_duration=302.115042, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=631, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 14:26:02.661881+00:00, queued_by_job_id=276, pid=114776[0m
[[34m2024-05-22T14:32:03.548+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T14:32:03.585+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-06 00:00:00+00:00, run_after=2023-10-07 00:00:00+00:00[0m
[[34m2024-05-22T14:32:03.805+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-28 00:00:00+00:00: scheduled__2023-09-28T00:00:00+00:00, state:running, queued_at: 2024-05-22 13:46:04.977194+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T14:32:03.806+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-28 00:00:00+00:00, run_id=scheduled__2023-09-28T00:00:00+00:00, run_start_date=2024-05-22 13:46:04.990949+00:00, run_end_date=2024-05-22 14:32:03.806025+00:00, run_duration=2758.815076, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-28 00:00:00+00:00, data_interval_end=2023-09-29 00:00:00+00:00, dag_hash=de273602226fa001d9f3797e9fbd91c9[0m
[[34m2024-05-22T14:32:03.810+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-29 00:00:00+00:00, run_after=2023-09-30 00:00:00+00:00[0m
[[34m2024-05-22T14:32:03.824+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:32:03.825+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T14:32:03.826+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T14:32:03.826+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T14:32:03.826+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-03T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:32:03.829+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T14:32:03.829+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:32:03.829+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T14:32:03.830+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:32:03.830+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T14:32:03.830+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:32:03.832+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:32:05.342+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:32:05.762+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:32:56.507+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:32:57.751+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:32:58.140+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:33:00.085+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:33:01.669+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:33:02.067+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-03T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:38:05.305+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:38:05.305+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:38:05.305+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:38:05.310+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:32:05.859241+00:00, run_end_date=2024-05-22 14:32:55.910953+00:00, run_duration=50.051712, state=success, executor_state=success, try_number=1, max_tries=2, job_id=632, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 14:32:03.827615+00:00, queued_by_job_id=276, pid=114975[0m
[[34m2024-05-22T14:38:05.310+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:32:58.242628+00:00, run_end_date=2024-05-22 14:32:59.464533+00:00, run_duration=1.221905, state=success, executor_state=success, try_number=1, max_tries=2, job_id=633, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 14:32:03.827615+00:00, queued_by_job_id=276, pid=115019[0m
[[34m2024-05-22T14:38:05.311+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-03T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:33:02.174201+00:00, run_end_date=2024-05-22 14:38:04.383090+00:00, run_duration=302.208889, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=634, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 14:32:03.827615+00:00, queued_by_job_id=276, pid=115047[0m
[[34m2024-05-22T14:38:05.343+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T14:38:05.361+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-07 00:00:00+00:00, run_after=2023-10-08 00:00:00+00:00[0m
[[34m2024-05-22T14:38:05.454+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-29 00:00:00+00:00: scheduled__2023-09-29T00:00:00+00:00, state:running, queued_at: 2024-05-22 13:52:45.999910+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T14:38:05.455+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-29 00:00:00+00:00, run_id=scheduled__2023-09-29T00:00:00+00:00, run_start_date=2024-05-22 13:52:46.017064+00:00, run_end_date=2024-05-22 14:38:05.455352+00:00, run_duration=2719.438288, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-29 00:00:00+00:00, data_interval_end=2023-09-30 00:00:00+00:00, dag_hash=ac1aa2c0778b52e398f176e6e14cbcd7[0m
[[34m2024-05-22T14:38:05.459+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-09-30 00:00:00+00:00, run_after=2023-10-01 00:00:00+00:00[0m
[[34m2024-05-22T14:38:05.483+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:38:05.484+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T14:38:05.485+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T14:38:05.485+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T14:38:05.485+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-04T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:38:05.493+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T14:38:05.494+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:38:05.494+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T14:38:05.494+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:38:05.496+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T14:38:05.496+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:38:05.498+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:38:07.044+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:38:07.521+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:39:50.602+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:39:52.156+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:39:52.624+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:39:54.862+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:39:56.186+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:39:56.740+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-04T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:44:59.724+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:44:59.725+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:44:59.725+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:44:59.730+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:38:07.630651+00:00, run_end_date=2024-05-22 14:39:49.928554+00:00, run_duration=102.297903, state=success, executor_state=success, try_number=1, max_tries=2, job_id=635, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 14:38:05.490400+00:00, queued_by_job_id=276, pid=115246[0m
[[34m2024-05-22T14:44:59.730+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:39:52.741439+00:00, run_end_date=2024-05-22 14:39:54.157269+00:00, run_duration=1.41583, state=success, executor_state=success, try_number=1, max_tries=2, job_id=636, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 14:38:05.490400+00:00, queued_by_job_id=276, pid=115322[0m
[[34m2024-05-22T14:44:59.731+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-04T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:39:56.872039+00:00, run_end_date=2024-05-22 14:44:59.069044+00:00, run_duration=302.197005, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=637, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 14:38:05.490400+00:00, queued_by_job_id=276, pid=115347[0m
[[34m2024-05-22T14:44:59.764+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T14:44:59.783+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-08 00:00:00+00:00, run_after=2023-10-09 00:00:00+00:00[0m
[[34m2024-05-22T14:44:59.881+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-09-30 00:00:00+00:00: scheduled__2023-09-30T00:00:00+00:00, state:running, queued_at: 2024-05-22 13:59:08.838343+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T14:44:59.881+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-09-30 00:00:00+00:00, run_id=scheduled__2023-09-30T00:00:00+00:00, run_start_date=2024-05-22 13:59:08.851916+00:00, run_end_date=2024-05-22 14:44:59.881560+00:00, run_duration=2751.029644, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-09-30 00:00:00+00:00, data_interval_end=2023-10-01 00:00:00+00:00, dag_hash=1b8110d30be17615382ea5b8c015b219[0m
[[34m2024-05-22T14:44:59.884+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-01 00:00:00+00:00, run_after=2023-10-02 00:00:00+00:00[0m
[[34m2024-05-22T14:44:59.895+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:44:59.895+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T14:44:59.895+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T14:44:59.896+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T14:44:59.896+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-05T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:44:59.898+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T14:44:59.898+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:44:59.899+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T14:44:59.899+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:44:59.899+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T14:44:59.899+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:44:59.901+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:45:01.353+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:45:01.788+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:46:16.782+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:46:18.232+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:46:18.685+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:46:20.824+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:46:22.339+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:46:22.739+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-05T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:51:25.725+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:51:25.726+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:51:25.726+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:51:25.731+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:45:01.895835+00:00, run_end_date=2024-05-22 14:46:16.092245+00:00, run_duration=74.19641, state=success, executor_state=success, try_number=1, max_tries=2, job_id=638, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 14:44:59.897097+00:00, queued_by_job_id=276, pid=115549[0m
[[34m2024-05-22T14:51:25.731+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:46:18.803805+00:00, run_end_date=2024-05-22 14:46:20.129799+00:00, run_duration=1.325994, state=success, executor_state=success, try_number=1, max_tries=2, job_id=639, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 14:44:59.897097+00:00, queued_by_job_id=276, pid=115612[0m
[[34m2024-05-22T14:51:25.731+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-05T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:46:22.860706+00:00, run_end_date=2024-05-22 14:51:25.106888+00:00, run_duration=302.246182, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=640, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 14:44:59.897097+00:00, queued_by_job_id=276, pid=115636[0m
[[34m2024-05-22T14:51:25.763+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T14:51:25.781+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-09 00:00:00+00:00, run_after=2023-10-10 00:00:00+00:00[0m
[[34m2024-05-22T14:51:25.865+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-01 00:00:00+00:00: scheduled__2023-10-01T00:00:00+00:00, state:running, queued_at: 2024-05-22 14:05:45.597232+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T14:51:25.866+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-01 00:00:00+00:00, run_id=scheduled__2023-10-01T00:00:00+00:00, run_start_date=2024-05-22 14:05:45.610302+00:00, run_end_date=2024-05-22 14:51:25.866045+00:00, run_duration=2740.255743, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-01 00:00:00+00:00, data_interval_end=2023-10-02 00:00:00+00:00, dag_hash=2f2ecacaa3fe65ca0fd85da27fcdc732[0m
[[34m2024-05-22T14:51:25.869+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-02 00:00:00+00:00, run_after=2023-10-03 00:00:00+00:00[0m
[[34m2024-05-22T14:51:25.879+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-06T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:51:25.879+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T14:51:25.879+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T14:51:25.879+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T14:51:25.880+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-06T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:51:25.882+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T14:51:25.882+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:51:25.882+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T14:51:25.882+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:51:25.883+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T14:51:25.883+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:51:25.884+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:51:27.031+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:51:27.372+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:52:34.427+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:52:35.705+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:52:36.061+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:52:37.835+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:52:39.063+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:52:39.424+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-06T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:57:42.000+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:57:42.000+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:57:42.001+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T14:57:42.021+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:52:36.158217+00:00, run_end_date=2024-05-22 14:52:37.268755+00:00, run_duration=1.110538, state=success, executor_state=success, try_number=1, max_tries=2, job_id=642, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 14:51:25.880655+00:00, queued_by_job_id=276, pid=115890[0m
[[34m2024-05-22T14:57:42.022+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:51:27.466566+00:00, run_end_date=2024-05-22 14:52:33.708465+00:00, run_duration=66.241899, state=success, executor_state=success, try_number=1, max_tries=2, job_id=641, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 14:51:25.880655+00:00, queued_by_job_id=276, pid=115833[0m
[[34m2024-05-22T14:57:42.022+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-06T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:52:39.522379+00:00, run_end_date=2024-05-22 14:57:41.401794+00:00, run_duration=301.879415, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=643, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 14:51:25.880655+00:00, queued_by_job_id=276, pid=115914[0m
[[34m2024-05-22T14:57:42.056+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T14:57:42.076+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-10 00:00:00+00:00, run_after=2023-10-11 00:00:00+00:00[0m
[[34m2024-05-22T14:57:42.173+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-02 00:00:00+00:00: scheduled__2023-10-02T00:00:00+00:00, state:running, queued_at: 2024-05-22 14:12:19.025694+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T14:57:42.173+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-02 00:00:00+00:00, run_id=scheduled__2023-10-02T00:00:00+00:00, run_start_date=2024-05-22 14:12:19.039509+00:00, run_end_date=2024-05-22 14:57:42.173368+00:00, run_duration=2723.133859, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-02 00:00:00+00:00, data_interval_end=2023-10-03 00:00:00+00:00, dag_hash=268d9e7943f25188f8e016dfeae8f53b[0m
[[34m2024-05-22T14:57:42.176+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-03 00:00:00+00:00, run_after=2023-10-04 00:00:00+00:00[0m
[[34m2024-05-22T14:57:42.190+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-07T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:57:42.190+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T14:57:42.190+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T14:57:42.190+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T14:57:42.191+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-07T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T14:57:42.193+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T14:57:42.193+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:57:42.193+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T14:57:42.194+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:57:42.194+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T14:57:42.194+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:57:42.195+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:57:43.598+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:57:44.125+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:59:02.550+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:59:04.066+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:59:04.668+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T14:59:06.565+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T14:59:07.855+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T14:59:08.272+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-07T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:04:11.156+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:04:11.156+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:04:11.157+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:04:11.181+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-07T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:59:08.396495+00:00, run_end_date=2024-05-22 15:04:10.491747+00:00, run_duration=302.095252, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=646, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 14:57:42.191757+00:00, queued_by_job_id=276, pid=116218[0m
[[34m2024-05-22T15:04:11.185+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:59:04.851063+00:00, run_end_date=2024-05-22 14:59:05.926821+00:00, run_duration=1.075758, state=success, executor_state=success, try_number=1, max_tries=2, job_id=645, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 14:57:42.191757+00:00, queued_by_job_id=276, pid=116194[0m
[[34m2024-05-22T15:04:11.186+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 14:57:44.271090+00:00, run_end_date=2024-05-22 14:59:01.862470+00:00, run_duration=77.59138, state=success, executor_state=success, try_number=1, max_tries=2, job_id=644, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 14:57:42.191757+00:00, queued_by_job_id=276, pid=116128[0m
[[34m2024-05-22T15:04:11.226+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T15:04:11.247+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-11 00:00:00+00:00, run_after=2023-10-12 00:00:00+00:00[0m
[[34m2024-05-22T15:04:11.340+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-03 00:00:00+00:00: scheduled__2023-10-03T00:00:00+00:00, state:running, queued_at: 2024-05-22 14:18:19.186149+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T15:04:11.340+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-03 00:00:00+00:00, run_id=scheduled__2023-10-03T00:00:00+00:00, run_start_date=2024-05-22 14:18:19.206139+00:00, run_end_date=2024-05-22 15:04:11.340520+00:00, run_duration=2752.134381, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-03 00:00:00+00:00, data_interval_end=2023-10-04 00:00:00+00:00, dag_hash=2544f1dce0f608aec441bf6faaa54bbb[0m
[[34m2024-05-22T15:04:11.343+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-04 00:00:00+00:00, run_after=2023-10-05 00:00:00+00:00[0m
[[34m2024-05-22T15:04:11.354+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-08T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:04:11.355+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T15:04:11.355+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T15:04:11.355+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T15:04:11.355+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-08T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:04:11.358+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T15:04:11.358+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:04:11.358+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T15:04:11.358+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:04:11.359+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T15:04:11.359+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:04:11.360+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:04:12.770+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:04:13.147+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:05:07.544+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:05:08.827+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:05:09.220+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:05:11.111+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:05:12.452+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:05:12.831+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-08T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:10:15.773+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:10:15.773+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:10:15.774+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:10:15.779+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-08T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:05:12.948138+00:00, run_end_date=2024-05-22 15:10:15.073917+00:00, run_duration=302.125779, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=649, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 15:04:11.356406+00:00, queued_by_job_id=276, pid=116498[0m
[[34m2024-05-22T15:10:15.779+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:04:13.260329+00:00, run_end_date=2024-05-22 15:05:06.878560+00:00, run_duration=53.618231, state=success, executor_state=success, try_number=1, max_tries=2, job_id=647, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 15:04:11.356406+00:00, queued_by_job_id=276, pid=116421[0m
[[34m2024-05-22T15:10:15.780+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:05:09.325585+00:00, run_end_date=2024-05-22 15:05:10.510988+00:00, run_duration=1.185403, state=success, executor_state=success, try_number=1, max_tries=2, job_id=648, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 15:04:11.356406+00:00, queued_by_job_id=276, pid=116474[0m
[[34m2024-05-22T15:10:15.817+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T15:10:15.840+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-12 00:00:00+00:00, run_after=2023-10-13 00:00:00+00:00[0m
[[34m2024-05-22T15:10:15.997+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-04 00:00:00+00:00: scheduled__2023-10-04T00:00:00+00:00, state:running, queued_at: 2024-05-22 14:26:02.528239+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T15:10:15.998+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-04 00:00:00+00:00, run_id=scheduled__2023-10-04T00:00:00+00:00, run_start_date=2024-05-22 14:26:02.546355+00:00, run_end_date=2024-05-22 15:10:15.998222+00:00, run_duration=2653.451867, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-04 00:00:00+00:00, data_interval_end=2023-10-05 00:00:00+00:00, dag_hash=077f6fcd18b2b7ea4c0356a34d479ac3[0m
[[34m2024-05-22T15:10:16.002+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-05 00:00:00+00:00, run_after=2023-10-06 00:00:00+00:00[0m
[[34m2024-05-22T15:10:16.016+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-09T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:10:16.017+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T15:10:16.017+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T15:10:16.017+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T15:10:16.017+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-09T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:10:16.020+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T15:10:16.020+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:10:16.021+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T15:10:16.021+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:10:16.021+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T15:10:16.022+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:10:16.023+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:10:17.456+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:10:17.829+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:11:44.281+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:11:45.648+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:11:46.016+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:11:47.872+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:11:49.241+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:11:49.713+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-09T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:16:52.753+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:16:52.753+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:16:52.753+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:16:52.760+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:11:46.117326+00:00, run_end_date=2024-05-22 15:11:47.226418+00:00, run_duration=1.109092, state=success, executor_state=success, try_number=1, max_tries=2, job_id=651, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 15:10:16.018606+00:00, queued_by_job_id=276, pid=116766[0m
[[34m2024-05-22T15:16:52.760+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-09T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:11:49.843429+00:00, run_end_date=2024-05-22 15:16:51.974448+00:00, run_duration=302.131019, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=652, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 15:10:16.018606+00:00, queued_by_job_id=276, pid=116790[0m
[[34m2024-05-22T15:16:52.760+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:10:17.929085+00:00, run_end_date=2024-05-22 15:11:43.484702+00:00, run_duration=85.555617, state=success, executor_state=success, try_number=1, max_tries=2, job_id=650, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 15:10:16.018606+00:00, queued_by_job_id=276, pid=116699[0m
[[34m2024-05-22T15:16:52.805+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T15:16:52.830+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-13 00:00:00+00:00, run_after=2023-10-14 00:00:00+00:00[0m
[[34m2024-05-22T15:16:52.943+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-05 00:00:00+00:00: scheduled__2023-10-05T00:00:00+00:00, state:running, queued_at: 2024-05-22 14:32:03.571896+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T15:16:52.943+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-05 00:00:00+00:00, run_id=scheduled__2023-10-05T00:00:00+00:00, run_start_date=2024-05-22 14:32:03.611699+00:00, run_end_date=2024-05-22 15:16:52.943839+00:00, run_duration=2689.33214, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-05 00:00:00+00:00, data_interval_end=2023-10-06 00:00:00+00:00, dag_hash=1a0ebebc8261ac5be1f39d170dd1bcec[0m
[[34m2024-05-22T15:16:52.948+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-06 00:00:00+00:00, run_after=2023-10-07 00:00:00+00:00[0m
[[34m2024-05-22T15:16:52.961+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-10T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:16:52.961+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T15:16:52.961+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T15:16:52.962+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T15:16:52.962+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-10T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:16:52.964+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T15:16:52.965+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:16:52.965+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T15:16:52.965+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:16:52.966+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T15:16:52.966+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:16:52.967+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:16:54.596+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:16:55.067+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:17:39.783+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:17:41.254+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:17:41.715+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:17:43.781+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:17:45.510+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:17:46.012+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-10T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:22:48.929+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:22:48.929+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:22:48.929+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:22:48.935+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:17:41.815859+00:00, run_end_date=2024-05-22 15:17:43.101210+00:00, run_duration=1.285351, state=success, executor_state=success, try_number=1, max_tries=2, job_id=654, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 15:16:52.962870+00:00, queued_by_job_id=276, pid=117037[0m
[[34m2024-05-22T15:22:48.935+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:16:55.206506+00:00, run_end_date=2024-05-22 15:17:39.136283+00:00, run_duration=43.929777, state=success, executor_state=success, try_number=1, max_tries=2, job_id=653, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 15:16:52.962870+00:00, queued_by_job_id=276, pid=116989[0m
[[34m2024-05-22T15:22:48.936+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-10T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:17:46.151177+00:00, run_end_date=2024-05-22 15:22:48.212660+00:00, run_duration=302.061483, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=655, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 15:16:52.962870+00:00, queued_by_job_id=276, pid=117061[0m
[[34m2024-05-22T15:22:48.971+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T15:22:48.995+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-14 00:00:00+00:00, run_after=2023-10-15 00:00:00+00:00[0m
[[34m2024-05-22T15:22:49.094+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-06 00:00:00+00:00: scheduled__2023-10-06T00:00:00+00:00, state:running, queued_at: 2024-05-22 14:38:05.355032+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T15:22:49.095+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-06 00:00:00+00:00, run_id=scheduled__2023-10-06T00:00:00+00:00, run_start_date=2024-05-22 14:38:05.368575+00:00, run_end_date=2024-05-22 15:22:49.095112+00:00, run_duration=2683.726537, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-06 00:00:00+00:00, data_interval_end=2023-10-07 00:00:00+00:00, dag_hash=c63b40171d32cb80a5b2bdc08d3559d9[0m
[[34m2024-05-22T15:22:49.098+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-07 00:00:00+00:00, run_after=2023-10-08 00:00:00+00:00[0m
[[34m2024-05-22T15:22:49.109+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-11T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:22:49.109+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T15:22:49.109+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T15:22:49.110+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T15:22:49.110+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-11T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:22:49.112+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T15:22:49.112+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:22:49.113+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T15:22:49.113+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:22:49.113+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T15:22:49.113+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:22:49.115+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:22:50.410+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:22:50.843+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:23:43.445+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:23:45.272+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:23:45.667+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:23:47.677+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:23:49.158+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:23:49.519+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-11T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:28:52.095+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:28:52.096+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:28:52.096+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:28:52.118+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:22:50.945733+00:00, run_end_date=2024-05-22 15:23:42.862413+00:00, run_duration=51.91668, state=success, executor_state=success, try_number=1, max_tries=2, job_id=656, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 15:22:49.110917+00:00, queued_by_job_id=276, pid=117259[0m
[[34m2024-05-22T15:28:52.119+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:23:45.828466+00:00, run_end_date=2024-05-22 15:23:47.062052+00:00, run_duration=1.233586, state=success, executor_state=success, try_number=1, max_tries=2, job_id=657, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 15:22:49.110917+00:00, queued_by_job_id=276, pid=117310[0m
[[34m2024-05-22T15:28:52.119+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-11T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:23:49.617494+00:00, run_end_date=2024-05-22 15:28:51.406009+00:00, run_duration=301.788515, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=658, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 15:22:49.110917+00:00, queued_by_job_id=276, pid=117334[0m
[[34m2024-05-22T15:28:52.157+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T15:28:52.182+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-15 00:00:00+00:00, run_after=2023-10-16 00:00:00+00:00[0m
[[34m2024-05-22T15:28:52.305+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-07 00:00:00+00:00: scheduled__2023-10-07T00:00:00+00:00, state:running, queued_at: 2024-05-22 14:44:59.777588+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T15:28:52.306+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-07 00:00:00+00:00, run_id=scheduled__2023-10-07T00:00:00+00:00, run_start_date=2024-05-22 14:44:59.791481+00:00, run_end_date=2024-05-22 15:28:52.306304+00:00, run_duration=2632.514823, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-07 00:00:00+00:00, data_interval_end=2023-10-08 00:00:00+00:00, dag_hash=62af1c81d81c7cd5fb4a5e62b4fb97f2[0m
[[34m2024-05-22T15:28:52.311+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-08 00:00:00+00:00, run_after=2023-10-09 00:00:00+00:00[0m
[[34m2024-05-22T15:28:52.324+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-12T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:28:52.325+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T15:28:52.325+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T15:28:52.325+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T15:28:52.325+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-12T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:28:52.328+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T15:28:52.328+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:28:52.328+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T15:28:52.329+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:28:52.329+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T15:28:52.329+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:28:52.331+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:28:53.889+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:28:54.382+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:30:07.957+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:30:09.249+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:30:09.677+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:30:11.870+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:30:13.297+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:30:13.687+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-12T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:35:16.286+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:35:16.286+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:35:16.287+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:35:16.291+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:28:54.525794+00:00, run_end_date=2024-05-22 15:30:07.334786+00:00, run_duration=72.808992, state=success, executor_state=success, try_number=1, max_tries=2, job_id=659, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 15:28:52.326557+00:00, queued_by_job_id=276, pid=117532[0m
[[34m2024-05-22T15:35:16.292+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:30:09.790270+00:00, run_end_date=2024-05-22 15:30:11.169217+00:00, run_duration=1.378947, state=success, executor_state=success, try_number=1, max_tries=2, job_id=660, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 15:28:52.326557+00:00, queued_by_job_id=276, pid=117595[0m
[[34m2024-05-22T15:35:16.292+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-12T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:30:13.786309+00:00, run_end_date=2024-05-22 15:35:15.694814+00:00, run_duration=301.908505, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=661, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 15:28:52.326557+00:00, queued_by_job_id=276, pid=117619[0m
[[34m2024-05-22T15:35:16.324+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T15:35:16.342+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-16 00:00:00+00:00, run_after=2023-10-17 00:00:00+00:00[0m
[[34m2024-05-22T15:35:16.432+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-08 00:00:00+00:00: scheduled__2023-10-08T00:00:00+00:00, state:running, queued_at: 2024-05-22 14:51:25.774819+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T15:35:16.432+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-08 00:00:00+00:00, run_id=scheduled__2023-10-08T00:00:00+00:00, run_start_date=2024-05-22 14:51:25.788406+00:00, run_end_date=2024-05-22 15:35:16.432613+00:00, run_duration=2630.644207, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-08 00:00:00+00:00, data_interval_end=2023-10-09 00:00:00+00:00, dag_hash=937b83c7685b228397a020cdfca9016f[0m
[[34m2024-05-22T15:35:16.435+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-09 00:00:00+00:00, run_after=2023-10-10 00:00:00+00:00[0m
[[34m2024-05-22T15:35:16.444+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:35:16.445+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T15:35:16.445+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T15:35:16.445+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T15:35:16.445+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-13T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:35:16.447+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T15:35:16.448+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:35:16.448+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T15:35:16.448+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:35:16.448+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T15:35:16.448+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:35:16.450+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:35:17.657+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:35:18.012+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:36:44.554+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:36:45.846+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:36:46.265+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:36:48.275+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:36:49.767+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:36:50.154+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-13T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:41:53.018+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:41:53.018+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:41:53.018+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:41:53.025+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:35:18.108414+00:00, run_end_date=2024-05-22 15:36:43.830000+00:00, run_duration=85.721586, state=success, executor_state=success, try_number=1, max_tries=2, job_id=662, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 15:35:16.446294+00:00, queued_by_job_id=276, pid=117819[0m
[[34m2024-05-22T15:41:53.025+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:36:46.364925+00:00, run_end_date=2024-05-22 15:36:47.586702+00:00, run_duration=1.221777, state=success, executor_state=success, try_number=1, max_tries=2, job_id=663, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 15:35:16.446294+00:00, queued_by_job_id=276, pid=117886[0m
[[34m2024-05-22T15:41:53.026+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-13T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:36:50.270420+00:00, run_end_date=2024-05-22 15:41:52.329938+00:00, run_duration=302.059518, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=664, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 15:35:16.446294+00:00, queued_by_job_id=276, pid=117910[0m
[[34m2024-05-22T15:41:53.066+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T15:41:53.084+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-17 00:00:00+00:00, run_after=2023-10-18 00:00:00+00:00[0m
[[34m2024-05-22T15:41:53.191+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-09 00:00:00+00:00: scheduled__2023-10-09T00:00:00+00:00, state:running, queued_at: 2024-05-22 14:57:42.069820+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T15:41:53.191+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-09 00:00:00+00:00, run_id=scheduled__2023-10-09T00:00:00+00:00, run_start_date=2024-05-22 14:57:42.084373+00:00, run_end_date=2024-05-22 15:41:53.191628+00:00, run_duration=2651.107255, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-09 00:00:00+00:00, data_interval_end=2023-10-10 00:00:00+00:00, dag_hash=3e58c6c074b3e453b6f6f5a4d2a1ab49[0m
[[34m2024-05-22T15:41:53.194+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-10 00:00:00+00:00, run_after=2023-10-11 00:00:00+00:00[0m
[[34m2024-05-22T15:41:53.204+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-14T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:41:53.205+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T15:41:53.205+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T15:41:53.205+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T15:41:53.205+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-14T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:41:53.207+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T15:41:53.208+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:41:53.208+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T15:41:53.208+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:41:53.208+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T15:41:53.208+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:41:53.210+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:41:54.644+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:41:55.044+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:42:41.171+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:42:42.745+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:42:43.217+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:42:45.611+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:42:47.144+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:42:47.576+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-14T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:47:50.307+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:47:50.307+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:47:50.307+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:47:50.313+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:42:43.370473+00:00, run_end_date=2024-05-22 15:42:44.880591+00:00, run_duration=1.510118, state=success, executor_state=success, try_number=1, max_tries=2, job_id=666, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 15:41:53.206366+00:00, queued_by_job_id=276, pid=118170[0m
[[34m2024-05-22T15:47:50.313+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:41:55.141352+00:00, run_end_date=2024-05-22 15:42:40.476013+00:00, run_duration=45.334661, state=success, executor_state=success, try_number=1, max_tries=2, job_id=665, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 15:41:53.206366+00:00, queued_by_job_id=276, pid=118123[0m
[[34m2024-05-22T15:47:50.313+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-14T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:42:47.697637+00:00, run_end_date=2024-05-22 15:47:49.646074+00:00, run_duration=301.948437, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=667, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 15:41:53.206366+00:00, queued_by_job_id=276, pid=118194[0m
[[34m2024-05-22T15:47:50.347+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T15:47:50.366+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-18 00:00:00+00:00, run_after=2023-10-19 00:00:00+00:00[0m
[[34m2024-05-22T15:47:50.453+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-10 00:00:00+00:00: scheduled__2023-10-10T00:00:00+00:00, state:running, queued_at: 2024-05-22 15:04:11.240022+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T15:47:50.453+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-10 00:00:00+00:00, run_id=scheduled__2023-10-10T00:00:00+00:00, run_start_date=2024-05-22 15:04:11.256080+00:00, run_end_date=2024-05-22 15:47:50.453568+00:00, run_duration=2619.197488, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-10 00:00:00+00:00, data_interval_end=2023-10-11 00:00:00+00:00, dag_hash=e7b470fda80d8df54d481a8929a047b9[0m
[[34m2024-05-22T15:47:50.456+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-11 00:00:00+00:00, run_after=2023-10-12 00:00:00+00:00[0m
[[34m2024-05-22T15:47:50.466+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:47:50.466+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T15:47:50.466+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T15:47:50.467+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T15:47:50.467+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:47:50.469+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T15:47:50.469+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:47:50.470+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T15:47:50.470+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:47:50.470+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T15:47:50.470+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:47:50.472+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:47:51.651+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:47:52.008+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:52:53.075+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:52:54.350+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:52:54.913+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:52:56.684+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:52:57.965+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:52:58.378+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-15T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:58:01.322+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:58:01.323+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:58:01.323+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T15:58:01.328+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-15T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:52:58.514631+00:00, run_end_date=2024-05-22 15:58:00.691734+00:00, run_duration=302.177103, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=670, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 15:47:50.467899+00:00, queued_by_job_id=276, pid=118618[0m
[[34m2024-05-22T15:58:01.328+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:52:55.028612+00:00, run_end_date=2024-05-22 15:52:56.103883+00:00, run_duration=1.075271, state=success, executor_state=success, try_number=1, max_tries=2, job_id=669, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 15:47:50.467899+00:00, queued_by_job_id=276, pid=118593[0m
[[34m2024-05-22T15:58:01.328+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:47:52.108416+00:00, run_end_date=2024-05-22 15:52:52.455467+00:00, run_duration=300.347051, state=success, executor_state=success, try_number=1, max_tries=2, job_id=668, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 15:47:50.467899+00:00, queued_by_job_id=276, pid=118397[0m
[[34m2024-05-22T15:58:01.373+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T15:58:01.393+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-19 00:00:00+00:00, run_after=2023-10-20 00:00:00+00:00[0m
[[34m2024-05-22T15:58:01.487+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-11 00:00:00+00:00: scheduled__2023-10-11T00:00:00+00:00, state:running, queued_at: 2024-05-22 15:10:15.834094+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T15:58:01.488+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-11 00:00:00+00:00, run_id=scheduled__2023-10-11T00:00:00+00:00, run_start_date=2024-05-22 15:10:15.850135+00:00, run_end_date=2024-05-22 15:58:01.488281+00:00, run_duration=2865.638146, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-11 00:00:00+00:00, data_interval_end=2023-10-12 00:00:00+00:00, dag_hash=cf2ff15287c2601acd52de48c36f7930[0m
[[34m2024-05-22T15:58:01.491+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-12 00:00:00+00:00, run_after=2023-10-13 00:00:00+00:00[0m
[[34m2024-05-22T15:58:01.502+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:58:01.502+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T15:58:01.503+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T15:58:01.503+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T15:58:01.503+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-16T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T15:58:01.505+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T15:58:01.505+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:58:01.506+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T15:58:01.506+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:58:01.506+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T15:58:01.506+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:58:01.508+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:58:03.082+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:58:03.584+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:58:57.841+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:58:59.160+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:58:59.538+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T15:59:01.638+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T15:59:03.189+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T15:59:03.655+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-16T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:04:06.765+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:04:06.765+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:04:06.765+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:04:06.786+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-16T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:59:03.754485+00:00, run_end_date=2024-05-22 16:04:05.981410+00:00, run_duration=302.226925, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=673, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 15:58:01.503979+00:00, queued_by_job_id=276, pid=118890[0m
[[34m2024-05-22T16:04:06.787+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:58:03.695851+00:00, run_end_date=2024-05-22 15:58:57.295344+00:00, run_duration=53.599493, state=success, executor_state=success, try_number=1, max_tries=2, job_id=671, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 15:58:01.503979+00:00, queued_by_job_id=276, pid=118815[0m
[[34m2024-05-22T16:04:06.787+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 15:58:59.639925+00:00, run_end_date=2024-05-22 15:59:00.882845+00:00, run_duration=1.24292, state=success, executor_state=success, try_number=1, max_tries=2, job_id=672, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 15:58:01.503979+00:00, queued_by_job_id=276, pid=118866[0m
[[34m2024-05-22T16:04:06.823+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T16:04:06.845+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-20 00:00:00+00:00, run_after=2023-10-21 00:00:00+00:00[0m
[[34m2024-05-22T16:04:06.947+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-12 00:00:00+00:00: scheduled__2023-10-12T00:00:00+00:00, state:running, queued_at: 2024-05-22 15:16:52.822554+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T16:04:06.947+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-12 00:00:00+00:00, run_id=scheduled__2023-10-12T00:00:00+00:00, run_start_date=2024-05-22 15:16:52.839267+00:00, run_end_date=2024-05-22 16:04:06.947841+00:00, run_duration=2834.108574, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-12 00:00:00+00:00, data_interval_end=2023-10-13 00:00:00+00:00, dag_hash=89a191cdc02d71d2d07450fe47c1fa60[0m
[[34m2024-05-22T16:04:06.952+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-13 00:00:00+00:00, run_after=2023-10-14 00:00:00+00:00[0m
[[34m2024-05-22T16:04:06.965+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:04:06.966+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T16:04:06.966+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T16:04:06.966+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T16:04:06.966+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-17T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:04:06.969+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T16:04:06.970+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:04:06.970+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T16:04:06.970+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:04:06.970+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T16:04:06.971+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:04:06.972+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:04:08.337+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:04:08.739+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:05:29.950+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:05:31.197+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:05:31.587+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:05:33.516+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:05:34.976+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:05:35.597+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-17T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:10:38.306+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:10:38.307+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:10:38.307+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:10:38.312+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:04:08.836365+00:00, run_end_date=2024-05-22 16:05:29.348093+00:00, run_duration=80.511728, state=success, executor_state=success, try_number=1, max_tries=2, job_id=674, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 16:04:06.967415+00:00, queued_by_job_id=276, pid=119092[0m
[[34m2024-05-22T16:10:38.312+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:05:31.685934+00:00, run_end_date=2024-05-22 16:05:32.838318+00:00, run_duration=1.152384, state=success, executor_state=success, try_number=1, max_tries=2, job_id=675, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 16:04:06.967415+00:00, queued_by_job_id=276, pid=119161[0m
[[34m2024-05-22T16:10:38.312+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-17T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:05:35.769986+00:00, run_end_date=2024-05-22 16:10:37.718288+00:00, run_duration=301.948302, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=676, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 16:04:06.967415+00:00, queued_by_job_id=276, pid=119186[0m
[[34m2024-05-22T16:10:38.350+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T16:10:38.369+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-21 00:00:00+00:00, run_after=2023-10-22 00:00:00+00:00[0m
[[34m2024-05-22T16:10:38.465+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-13 00:00:00+00:00: scheduled__2023-10-13T00:00:00+00:00, state:running, queued_at: 2024-05-22 15:22:48.988071+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T16:10:38.465+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-13 00:00:00+00:00, run_id=scheduled__2023-10-13T00:00:00+00:00, run_start_date=2024-05-22 15:22:49.003464+00:00, run_end_date=2024-05-22 16:10:38.465625+00:00, run_duration=2869.462161, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-13 00:00:00+00:00, data_interval_end=2023-10-14 00:00:00+00:00, dag_hash=b93c13c1d13fb0b2790dbdd36a79502a[0m
[[34m2024-05-22T16:10:38.470+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-14 00:00:00+00:00, run_after=2023-10-15 00:00:00+00:00[0m
[[34m2024-05-22T16:10:38.485+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-18T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:10:38.485+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T16:10:38.485+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T16:10:38.486+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T16:10:38.486+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-18T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:10:38.488+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T16:10:38.489+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:10:38.489+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T16:10:38.489+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:10:38.489+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T16:10:38.490+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:10:38.491+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:10:39.827+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:10:40.243+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:12:07.600+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:12:09.182+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:12:09.678+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:12:11.812+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:12:13.124+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:12:13.682+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-18T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:17:16.580+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:17:16.580+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:17:16.581+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:17:16.586+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:10:40.341490+00:00, run_end_date=2024-05-22 16:12:07.010371+00:00, run_duration=86.668881, state=success, executor_state=success, try_number=1, max_tries=2, job_id=677, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 16:10:38.486920+00:00, queued_by_job_id=276, pid=119382[0m
[[34m2024-05-22T16:17:16.586+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:12:09.790864+00:00, run_end_date=2024-05-22 16:12:11.168116+00:00, run_duration=1.377252, state=success, executor_state=success, try_number=1, max_tries=2, job_id=678, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 16:10:38.486920+00:00, queued_by_job_id=276, pid=119466[0m
[[34m2024-05-22T16:17:16.586+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-18T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:12:13.810659+00:00, run_end_date=2024-05-22 16:17:15.877904+00:00, run_duration=302.067245, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=679, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 16:10:38.486920+00:00, queued_by_job_id=276, pid=119490[0m
[[34m2024-05-22T16:17:16.631+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T16:17:16.666+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-22 00:00:00+00:00, run_after=2023-10-23 00:00:00+00:00[0m
[[34m2024-05-22T16:17:16.770+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-14 00:00:00+00:00: scheduled__2023-10-14T00:00:00+00:00, state:running, queued_at: 2024-05-22 15:28:52.172830+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T16:17:16.770+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-14 00:00:00+00:00, run_id=scheduled__2023-10-14T00:00:00+00:00, run_start_date=2024-05-22 15:28:52.191551+00:00, run_end_date=2024-05-22 16:17:16.770513+00:00, run_duration=2904.578962, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-14 00:00:00+00:00, data_interval_end=2023-10-15 00:00:00+00:00, dag_hash=5e85e6e9fe42253ce2040000637b24fb[0m
[[34m2024-05-22T16:17:16.774+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-15 00:00:00+00:00, run_after=2023-10-16 00:00:00+00:00[0m
[[34m2024-05-22T16:17:16.785+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-19T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:17:16.785+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T16:17:16.785+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T16:17:16.785+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T16:17:16.785+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-19T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:17:16.788+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T16:17:16.788+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:17:16.788+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T16:17:16.789+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:17:16.789+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T16:17:16.789+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:17:16.790+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:17:18.283+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:17:18.651+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:18:06.547+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:18:07.869+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:18:08.257+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:18:10.133+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:18:11.488+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:18:11.917+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-19T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:23:14.592+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:23:14.593+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:23:14.593+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:23:14.598+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:18:08.362337+00:00, run_end_date=2024-05-22 16:18:09.568883+00:00, run_duration=1.206546, state=success, executor_state=success, try_number=1, max_tries=2, job_id=681, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 16:17:16.786575+00:00, queued_by_job_id=276, pid=119743[0m
[[34m2024-05-22T16:23:14.599+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-19T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:18:12.036952+00:00, run_end_date=2024-05-22 16:23:13.956771+00:00, run_duration=301.919819, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=682, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 16:17:16.786575+00:00, queued_by_job_id=276, pid=119767[0m
[[34m2024-05-22T16:23:14.599+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:17:18.748008+00:00, run_end_date=2024-05-22 16:18:05.920769+00:00, run_duration=47.172761, state=success, executor_state=success, try_number=1, max_tries=2, job_id=680, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 16:17:16.786575+00:00, queued_by_job_id=276, pid=119696[0m
[[34m2024-05-22T16:23:14.632+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T16:23:14.659+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-23 00:00:00+00:00, run_after=2023-10-24 00:00:00+00:00[0m
[[34m2024-05-22T16:23:14.779+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-15 00:00:00+00:00: scheduled__2023-10-15T00:00:00+00:00, state:running, queued_at: 2024-05-22 15:35:16.336461+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T16:23:14.779+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-15 00:00:00+00:00, run_id=scheduled__2023-10-15T00:00:00+00:00, run_start_date=2024-05-22 15:35:16.348753+00:00, run_end_date=2024-05-22 16:23:14.779494+00:00, run_duration=2878.430741, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-15 00:00:00+00:00, data_interval_end=2023-10-16 00:00:00+00:00, dag_hash=0d723426c21e1b81c57ded593ee05622[0m
[[34m2024-05-22T16:23:14.782+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-16 00:00:00+00:00, run_after=2023-10-17 00:00:00+00:00[0m
[[34m2024-05-22T16:23:14.792+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-20T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:23:14.792+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T16:23:14.793+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T16:23:14.793+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T16:23:14.793+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-21T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-20T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:23:14.795+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T16:23:14.795+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:23:14.796+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T16:23:14.796+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:23:14.796+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T16:23:14.796+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:23:14.798+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:23:16.439+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:23:16.877+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:24:04.454+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:24:06.004+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:24:06.353+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:24:08.339+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:24:09.481+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:24:09.831+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-20T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:29:12.486+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:29:12.487+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:29:12.487+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:29:12.498+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:23:16.985031+00:00, run_end_date=2024-05-22 16:24:03.825410+00:00, run_duration=46.840379, state=success, executor_state=success, try_number=1, max_tries=2, job_id=683, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 16:23:14.794116+00:00, queued_by_job_id=276, pid=119966[0m
[[34m2024-05-22T16:29:12.499+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-20T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:24:09.926187+00:00, run_end_date=2024-05-22 16:29:11.807157+00:00, run_duration=301.88097, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=685, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 16:23:14.794116+00:00, queued_by_job_id=276, pid=120037[0m
[[34m2024-05-22T16:29:12.499+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:24:06.452116+00:00, run_end_date=2024-05-22 16:24:07.686585+00:00, run_duration=1.234469, state=success, executor_state=success, try_number=1, max_tries=2, job_id=684, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 16:23:14.794116+00:00, queued_by_job_id=276, pid=120013[0m
[[34m2024-05-22T16:29:12.532+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T16:29:12.550+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-24 00:00:00+00:00, run_after=2023-10-25 00:00:00+00:00[0m
[[34m2024-05-22T16:29:12.639+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-16 00:00:00+00:00: scheduled__2023-10-16T00:00:00+00:00, state:running, queued_at: 2024-05-22 15:41:53.078654+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T16:29:12.639+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-16 00:00:00+00:00, run_id=scheduled__2023-10-16T00:00:00+00:00, run_start_date=2024-05-22 15:41:53.092361+00:00, run_end_date=2024-05-22 16:29:12.639662+00:00, run_duration=2839.547301, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-16 00:00:00+00:00, data_interval_end=2023-10-17 00:00:00+00:00, dag_hash=2fa294100de168a75e5ef3b2af306ce3[0m
[[34m2024-05-22T16:29:12.650+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-17 00:00:00+00:00, run_after=2023-10-18 00:00:00+00:00[0m
[[34m2024-05-22T16:29:12.661+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:29:12.661+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T16:29:12.661+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T16:29:12.661+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T16:29:12.662+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:29:12.664+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T16:29:12.664+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:29:12.664+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T16:29:12.664+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:29:12.665+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T16:29:12.665+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:29:12.666+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:29:14.050+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:29:14.467+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:31:23.352+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:31:24.688+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:31:25.147+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:31:27.257+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:31:28.699+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:31:29.112+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-21T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:36:31.808+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:36:31.808+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:36:31.808+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:36:31.828+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:29:14.574031+00:00, run_end_date=2024-05-22 16:31:22.708409+00:00, run_duration=128.134378, state=success, executor_state=success, try_number=1, max_tries=2, job_id=686, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 16:29:12.662660+00:00, queued_by_job_id=276, pid=120235[0m
[[34m2024-05-22T16:36:31.829+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:31:25.298674+00:00, run_end_date=2024-05-22 16:31:26.489793+00:00, run_duration=1.191119, state=success, executor_state=success, try_number=1, max_tries=2, job_id=687, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 16:29:12.662660+00:00, queued_by_job_id=276, pid=120331[0m
[[34m2024-05-22T16:36:31.829+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-21T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:31:29.213196+00:00, run_end_date=2024-05-22 16:36:31.184004+00:00, run_duration=301.970808, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=688, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 16:29:12.662660+00:00, queued_by_job_id=276, pid=120356[0m
[[34m2024-05-22T16:36:31.865+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T16:36:31.886+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-25 00:00:00+00:00, run_after=2023-10-26 00:00:00+00:00[0m
[[34m2024-05-22T16:36:31.977+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-17 00:00:00+00:00: scheduled__2023-10-17T00:00:00+00:00, state:running, queued_at: 2024-05-22 15:47:50.359635+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T16:36:31.977+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-17 00:00:00+00:00, run_id=scheduled__2023-10-17T00:00:00+00:00, run_start_date=2024-05-22 15:47:50.373421+00:00, run_end_date=2024-05-22 16:36:31.977838+00:00, run_duration=2921.604417, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-17 00:00:00+00:00, data_interval_end=2023-10-18 00:00:00+00:00, dag_hash=052cdbc0473f80ef0cc89f68d5c7dde6[0m
[[34m2024-05-22T16:36:31.981+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-18 00:00:00+00:00, run_after=2023-10-19 00:00:00+00:00[0m
[[34m2024-05-22T16:36:31.991+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-22T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:36:31.992+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T16:36:31.992+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T16:36:31.992+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T16:36:31.992+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-22T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:36:31.995+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T16:36:31.995+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:36:31.995+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T16:36:31.995+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:36:31.995+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-22T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T16:36:31.996+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:36:31.997+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:36:33.250+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:36:33.599+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:37:41.321+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:37:42.848+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:37:43.217+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:37:45.120+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:37:46.741+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:37:47.080+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-22T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:42:49.884+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:42:49.884+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:42:49.885+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-22T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:42:49.890+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:36:33.693900+00:00, run_end_date=2024-05-22 16:37:40.641779+00:00, run_duration=66.947879, state=success, executor_state=success, try_number=1, max_tries=2, job_id=689, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 16:36:31.993523+00:00, queued_by_job_id=276, pid=120555[0m
[[34m2024-05-22T16:42:49.890+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:37:43.312746+00:00, run_end_date=2024-05-22 16:37:44.440401+00:00, run_duration=1.127655, state=success, executor_state=success, try_number=1, max_tries=2, job_id=690, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 16:36:31.993523+00:00, queued_by_job_id=276, pid=120612[0m
[[34m2024-05-22T16:42:49.891+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-22T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:37:47.178318+00:00, run_end_date=2024-05-22 16:42:49.174919+00:00, run_duration=301.996601, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=691, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 16:36:31.993523+00:00, queued_by_job_id=276, pid=120640[0m
[[34m2024-05-22T16:42:49.925+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T16:42:49.961+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-26 00:00:00+00:00, run_after=2023-10-27 00:00:00+00:00[0m
[[34m2024-05-22T16:42:50.102+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-18 00:00:00+00:00: scheduled__2023-10-18T00:00:00+00:00, state:running, queued_at: 2024-05-22 15:58:01.385781+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T16:42:50.103+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-18 00:00:00+00:00, run_id=scheduled__2023-10-18T00:00:00+00:00, run_start_date=2024-05-22 15:58:01.400877+00:00, run_end_date=2024-05-22 16:42:50.103074+00:00, run_duration=2688.702197, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-18 00:00:00+00:00, data_interval_end=2023-10-19 00:00:00+00:00, dag_hash=5dd616ccc0c7851b81bf3c6d24e563e5[0m
[[34m2024-05-22T16:42:50.106+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-19 00:00:00+00:00, run_after=2023-10-20 00:00:00+00:00[0m
[[34m2024-05-22T16:42:50.116+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-23T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:42:50.117+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T16:42:50.117+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T16:42:50.117+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T16:42:50.117+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-24T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-23T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:42:50.119+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T16:42:50.120+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:42:50.120+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T16:42:50.120+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:42:50.120+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-23T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T16:42:50.120+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:42:50.122+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:42:51.700+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:42:52.190+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:43:48.523+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:43:49.732+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:43:50.097+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:43:52.498+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:43:53.945+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:43:54.370+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-23T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:48:57.139+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:48:57.140+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:48:57.140+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-23T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:48:57.145+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:42:52.316588+00:00, run_end_date=2024-05-22 16:43:47.943323+00:00, run_duration=55.626735, state=success, executor_state=success, try_number=1, max_tries=2, job_id=692, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 16:42:50.118440+00:00, queued_by_job_id=276, pid=120838[0m
[[34m2024-05-22T16:48:57.145+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:43:50.215574+00:00, run_end_date=2024-05-22 16:43:51.848394+00:00, run_duration=1.63282, state=success, executor_state=success, try_number=1, max_tries=2, job_id=693, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 16:42:50.118440+00:00, queued_by_job_id=276, pid=120885[0m
[[34m2024-05-22T16:48:57.146+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-23T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:43:54.480352+00:00, run_end_date=2024-05-22 16:48:56.438323+00:00, run_duration=301.957971, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=694, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 16:42:50.118440+00:00, queued_by_job_id=276, pid=120913[0m
[[34m2024-05-22T16:48:57.182+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T16:48:57.203+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-27 00:00:00+00:00, run_after=2023-10-28 00:00:00+00:00[0m
[[34m2024-05-22T16:48:57.305+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-19 00:00:00+00:00: scheduled__2023-10-19T00:00:00+00:00, state:running, queued_at: 2024-05-22 16:04:06.837775+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T16:48:57.306+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-19 00:00:00+00:00, run_id=scheduled__2023-10-19T00:00:00+00:00, run_start_date=2024-05-22 16:04:06.853255+00:00, run_end_date=2024-05-22 16:48:57.306235+00:00, run_duration=2690.45298, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-19 00:00:00+00:00, data_interval_end=2023-10-20 00:00:00+00:00, dag_hash=5363908c5ccacc8829451f56531a1e3d[0m
[[34m2024-05-22T16:48:57.309+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-20 00:00:00+00:00, run_after=2023-10-21 00:00:00+00:00[0m
[[34m2024-05-22T16:48:57.322+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-24T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:48:57.323+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T16:48:57.323+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T16:48:57.323+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T16:48:57.323+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-24T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:48:57.326+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T16:48:57.326+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:48:57.327+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T16:48:57.327+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:48:57.327+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-24T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T16:48:57.327+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:48:57.329+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:48:58.624+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:48:58.973+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:50:25.249+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:50:26.736+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:50:27.167+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:50:29.398+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:50:30.623+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:50:30.993+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-24T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:55:33.656+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:55:33.656+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:55:33.656+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-24T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T16:55:33.662+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:50:27.315395+00:00, run_end_date=2024-05-22 16:50:28.718176+00:00, run_duration=1.402781, state=success, executor_state=success, try_number=1, max_tries=2, job_id=696, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 16:48:57.324803+00:00, queued_by_job_id=276, pid=121182[0m
[[34m2024-05-22T16:55:33.663+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:48:59.066639+00:00, run_end_date=2024-05-22 16:50:24.566308+00:00, run_duration=85.499669, state=success, executor_state=success, try_number=1, max_tries=2, job_id=695, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 16:48:57.324803+00:00, queued_by_job_id=276, pid=121111[0m
[[34m2024-05-22T16:55:33.663+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-24T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:50:31.090562+00:00, run_end_date=2024-05-22 16:55:32.959571+00:00, run_duration=301.869009, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=697, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 16:48:57.324803+00:00, queued_by_job_id=276, pid=121206[0m
[[34m2024-05-22T16:55:33.697+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T16:55:33.717+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-28 00:00:00+00:00, run_after=2023-10-29 00:00:00+00:00[0m
[[34m2024-05-22T16:55:33.812+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-20 00:00:00+00:00: scheduled__2023-10-20T00:00:00+00:00, state:running, queued_at: 2024-05-22 16:10:38.363018+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T16:55:33.812+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-20 00:00:00+00:00, run_id=scheduled__2023-10-20T00:00:00+00:00, run_start_date=2024-05-22 16:10:38.376434+00:00, run_end_date=2024-05-22 16:55:33.812354+00:00, run_duration=2695.43592, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-20 00:00:00+00:00, data_interval_end=2023-10-21 00:00:00+00:00, dag_hash=0f7a2a8d0d8bf204966f4506e9339e54[0m
[[34m2024-05-22T16:55:33.815+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-21 00:00:00+00:00, run_after=2023-10-22 00:00:00+00:00[0m
[[34m2024-05-22T16:55:33.826+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-25T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:55:33.826+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T16:55:33.827+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T16:55:33.827+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T16:55:33.827+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-26T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-25T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T16:55:33.829+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T16:55:33.829+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:55:33.830+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T16:55:33.830+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:55:33.830+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T16:55:33.831+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:55:33.832+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:55:35.409+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:55:35.807+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:57:10.992+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:57:12.273+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:57:12.670+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T16:57:14.611+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T16:57:16.046+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T16:57:16.419+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-25T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T17:02:19.090+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T17:02:19.090+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T17:02:19.091+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T17:02:19.096+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:57:12.776548+00:00, run_end_date=2024-05-22 16:57:13.988441+00:00, run_duration=1.211893, state=success, executor_state=success, try_number=1, max_tries=2, job_id=699, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 16:55:33.828066+00:00, queued_by_job_id=276, pid=121477[0m
[[34m2024-05-22T17:02:19.096+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-25T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:57:16.521588+00:00, run_end_date=2024-05-22 17:02:18.426885+00:00, run_duration=301.905297, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=700, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 16:55:33.828066+00:00, queued_by_job_id=276, pid=121502[0m
[[34m2024-05-22T17:02:19.097+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 16:55:35.947828+00:00, run_end_date=2024-05-22 16:57:10.404155+00:00, run_duration=94.456327, state=success, executor_state=success, try_number=1, max_tries=2, job_id=698, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 16:55:33.828066+00:00, queued_by_job_id=276, pid=121405[0m
[[34m2024-05-22T17:02:19.130+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T17:02:19.151+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-29 00:00:00+00:00, run_after=2023-10-30 00:00:00+00:00[0m
[[34m2024-05-22T17:02:19.449+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-21 00:00:00+00:00: scheduled__2023-10-21T00:00:00+00:00, state:running, queued_at: 2024-05-22 16:17:16.648191+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T17:02:19.450+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-21 00:00:00+00:00, run_id=scheduled__2023-10-21T00:00:00+00:00, run_start_date=2024-05-22 16:17:16.682590+00:00, run_end_date=2024-05-22 17:02:19.450509+00:00, run_duration=2702.767919, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-21 00:00:00+00:00, data_interval_end=2023-10-22 00:00:00+00:00, dag_hash=1ab34a3d097f3214aaeafa8d5c15277a[0m
[[34m2024-05-22T17:02:19.455+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-22 00:00:00+00:00, run_after=2023-10-23 00:00:00+00:00[0m
[[34m2024-05-22T17:02:19.470+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-26T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T17:02:19.470+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T17:02:19.470+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T17:02:19.470+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T17:02:19.471+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-26T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T17:02:19.473+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T17:02:19.473+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:02:19.474+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T17:02:19.474+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:02:19.474+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-26T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T17:02:19.475+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:02:19.476+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:02:21.011+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T17:02:21.394+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T17:03:52.280+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:03:53.943+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T17:03:54.506+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T17:03:56.669+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:03:58.058+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T17:03:58.487+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-26T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T17:09:01.481+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T17:09:01.481+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T17:09:01.481+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-26T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T17:09:01.501+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-26T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 17:03:58.600891+00:00, run_end_date=2024-05-22 17:09:00.849390+00:00, run_duration=302.248499, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=703, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 17:02:19.471793+00:00, queued_by_job_id=276, pid=121798[0m
[[34m2024-05-22T17:09:01.502+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 17:02:21.500314+00:00, run_end_date=2024-05-22 17:03:51.565952+00:00, run_duration=90.065638, state=success, executor_state=success, try_number=1, max_tries=2, job_id=701, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 17:02:19.471793+00:00, queued_by_job_id=276, pid=121702[0m
[[34m2024-05-22T17:09:01.502+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 17:03:54.697182+00:00, run_end_date=2024-05-22 17:03:56.056525+00:00, run_duration=1.359343, state=success, executor_state=success, try_number=1, max_tries=2, job_id=702, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 17:02:19.471793+00:00, queued_by_job_id=276, pid=121773[0m
[[34m2024-05-22T17:09:01.540+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T17:09:01.561+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-30 00:00:00+00:00, run_after=2023-10-31 00:00:00+00:00[0m
[[34m2024-05-22T17:09:01.670+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-22 00:00:00+00:00: scheduled__2023-10-22T00:00:00+00:00, state:running, queued_at: 2024-05-22 16:23:14.645003+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T17:09:01.671+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-22 00:00:00+00:00, run_id=scheduled__2023-10-22T00:00:00+00:00, run_start_date=2024-05-22 16:23:14.678378+00:00, run_end_date=2024-05-22 17:09:01.671283+00:00, run_duration=2746.992905, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-22 00:00:00+00:00, data_interval_end=2023-10-23 00:00:00+00:00, dag_hash=f6d895ccd7076c4b2ba94d646e1b9bda[0m
[[34m2024-05-22T17:09:01.675+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-23 00:00:00+00:00, run_after=2023-10-24 00:00:00+00:00[0m
[[34m2024-05-22T17:09:01.695+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-27T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T17:09:01.696+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T17:09:01.697+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T17:09:01.697+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T17:09:01.697+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-27T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T17:09:01.704+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T17:09:01.704+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:09:01.704+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T17:09:01.704+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:09:01.705+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-27T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T17:09:01.705+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:09:01.713+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:09:03.108+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T17:09:03.623+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T17:09:58.837+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:10:00.205+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T17:10:00.655+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T17:10:02.913+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:10:04.344+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T17:10:04.721+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-27T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T17:15:07.380+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-29T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T17:15:07.380+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-28T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T17:15:07.380+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-27T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-05-22T17:15:07.386+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=scheduled__2023-10-29T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 17:09:03.742555+00:00, run_end_date=2024-05-22 17:09:58.205231+00:00, run_duration=54.462676, state=success, executor_state=success, try_number=1, max_tries=2, job_id=704, pool=default_pool, queue=default, priority_weight=8, operator=PythonOperator, queued_dttm=2024-05-22 17:09:01.699374+00:00, queued_by_job_id=276, pid=121997[0m
[[34m2024-05-22T17:15:07.386+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=scheduled__2023-10-28T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 17:10:00.758185+00:00, run_end_date=2024-05-22 17:10:02.221764+00:00, run_duration=1.463579, state=success, executor_state=success, try_number=1, max_tries=2, job_id=705, pool=default_pool, queue=default, priority_weight=7, operator=BashOperator, queued_dttm=2024-05-22 17:09:01.699374+00:00, queued_by_job_id=276, pid=122051[0m
[[34m2024-05-22T17:15:07.387+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=scheduled__2023-10-27T00:00:00+00:00, map_index=-1, run_start_date=2024-05-22 17:10:04.819283+00:00, run_end_date=2024-05-22 17:15:06.757119+00:00, run_duration=301.937836, state=failed, executor_state=success, try_number=1, max_tries=2, job_id=706, pool=default_pool, queue=default, priority_weight=6, operator=S3KeySensor, queued_dttm=2024-05-22 17:09:01.699374+00:00, queued_by_job_id=276, pid=122075[0m
[[34m2024-05-22T17:15:07.422+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-05-22T17:15:07.441+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-31 00:00:00+00:00, run_after=2023-11-01 00:00:00+00:00[0m
[[34m2024-05-22T17:15:07.565+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun extract_311_data_dag @ 2023-10-23 00:00:00+00:00: scheduled__2023-10-23T00:00:00+00:00, state:running, queued_at: 2024-05-22 16:29:12.545124+00:00. externally triggered: False> failed[0m
[[34m2024-05-22T17:15:07.565+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2023-10-23 00:00:00+00:00, run_id=scheduled__2023-10-23T00:00:00+00:00, run_start_date=2024-05-22 16:29:12.557912+00:00, run_end_date=2024-05-22 17:15:07.565524+00:00, run_duration=2755.007612, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2023-10-23 00:00:00+00:00, data_interval_end=2023-10-24 00:00:00+00:00, dag_hash=aab7727126d78c14739687d5e97643de[0m
[[34m2024-05-22T17:15:07.568+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_311_data_dag to 2023-10-24 00:00:00+00:00, run_after=2023-10-25 00:00:00+00:00[0m
[[34m2024-05-22T17:15:07.579+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-28T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T17:15:07.579+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-05-22T17:15:07.579+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 1/16 running and queued tasks[0m
[[34m2024-05-22T17:15:07.579+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 2/16 running and queued tasks[0m
[[34m2024-05-22T17:15:07.580+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-30T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-28T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-22T17:15:07.582+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='scheduled__2023-10-30T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-05-22T17:15:07.582+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:15:07.582+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='scheduled__2023-10-29T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-05-22T17:15:07.583+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:15:07.583+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='scheduled__2023-10-28T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-05-22T17:15:07.583+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:15:07.585+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'scheduled__2023-10-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:15:08.943+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T17:15:09.428+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data scheduled__2023-10-30T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T17:16:46.987+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'scheduled__2023-10-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:16:48.466+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T17:16:48.867+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 scheduled__2023-10-29T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T17:16:50.920+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'scheduled__2023-10-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-05-22T17:16:52.504+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-05-22T17:16:52.969+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available scheduled__2023-10-28T00:00:00+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-05-22T17:20:59.954+0000[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 15[0m
