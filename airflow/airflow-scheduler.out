[[34m2024-06-07T04:29:35.722+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-06-07T04:29:35.724+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2024-06-07T04:29:40.799+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-06-07T04:29:40.800+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-06-07T04:29:40.806+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 14721[0m
[[34m2024-06-07T04:29:40.809+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T04:29:40.814+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[[34m2024-06-07T04:34:40.931+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T04:34:43.404+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-06-07T04:34:27.981682+00:00 [scheduled]>[0m
[[34m2024-06-07T04:34:43.407+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:34:43.407+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-06-07T04:34:27.981682+00:00 [scheduled]>[0m
[[34m2024-06-07T04:34:43.417+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='manual__2024-06-07T04:34:27.981682+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-06-07T04:34:43.418+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'manual__2024-06-07T04:34:27.981682+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:34:43.423+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'manual__2024-06-07T04:34:27.981682+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:34:44.852+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:34:45.339+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-06-07T04:34:27.981682+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:34:48.663+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='manual__2024-06-07T04:34:27.981682+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:34:48.673+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=manual__2024-06-07T04:34:27.981682+00:00, map_index=-1, run_start_date=2024-06-07 04:34:45.498293+00:00, run_end_date=2024-06-07 04:34:47.917274+00:00, run_duration=2.418981, state=success, executor_state=success, try_number=1, max_tries=3, job_id=874, pool=default_pool, queue=default, priority_weight=8, operator=BashOperator, queued_dttm=2024-06-07 04:34:43.408577+00:00, queued_by_job_id=872, pid=14935[0m
[[34m2024-06-07T04:35:11.903+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.create_staging_table manual__2024-06-07T04:34:27.981682+00:00 [scheduled]>[0m
[[34m2024-06-07T04:35:11.904+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:35:11.907+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.create_staging_table manual__2024-06-07T04:34:27.981682+00:00 [scheduled]>[0m
[[34m2024-06-07T04:35:11.912+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='create_staging_table', run_id='manual__2024-06-07T04:34:27.981682+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-06-07T04:35:11.913+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'create_staging_table', 'manual__2024-06-07T04:34:27.981682+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:35:11.915+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'create_staging_table', 'manual__2024-06-07T04:34:27.981682+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:35:13.831+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:35:14.259+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.create_staging_table manual__2024-06-07T04:34:27.981682+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:35:15.791+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='create_staging_table', run_id='manual__2024-06-07T04:34:27.981682+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:35:15.796+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=create_staging_table, run_id=manual__2024-06-07T04:34:27.981682+00:00, map_index=-1, run_start_date=2024-06-07 04:35:14.395334+00:00, run_end_date=2024-06-07 04:35:15.048562+00:00, run_duration=0.653228, state=success, executor_state=success, try_number=1, max_tries=3, job_id=876, pool=default_pool, queue=default, priority_weight=6, operator=PostgresOperator, queued_dttm=2024-06-07 04:35:11.908783+00:00, queued_by_job_id=872, pid=15008[0m
[[34m2024-06-07T04:35:15.864+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.transfer_s3_to_redshift_staging manual__2024-06-07T04:34:27.981682+00:00 [scheduled]>[0m
[[34m2024-06-07T04:35:15.864+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:35:15.865+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.transfer_s3_to_redshift_staging manual__2024-06-07T04:34:27.981682+00:00 [scheduled]>[0m
[[34m2024-06-07T04:35:15.868+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='transfer_s3_to_redshift_staging', run_id='manual__2024-06-07T04:34:27.981682+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2024-06-07T04:35:15.869+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'transfer_s3_to_redshift_staging', 'manual__2024-06-07T04:34:27.981682+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:35:15.871+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'transfer_s3_to_redshift_staging', 'manual__2024-06-07T04:34:27.981682+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:35:17.375+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:35:17.878+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.transfer_s3_to_redshift_staging manual__2024-06-07T04:34:27.981682+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:35:20.651+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='transfer_s3_to_redshift_staging', run_id='manual__2024-06-07T04:34:27.981682+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:35:20.663+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=transfer_s3_to_redshift_staging, run_id=manual__2024-06-07T04:34:27.981682+00:00, map_index=-1, run_start_date=2024-06-07 04:35:18.019850+00:00, run_end_date=2024-06-07 04:35:19.749369+00:00, run_duration=1.729519, state=success, executor_state=success, try_number=1, max_tries=3, job_id=877, pool=default_pool, queue=default, priority_weight=5, operator=S3ToRedshiftOperator, queued_dttm=2024-06-07 04:35:15.866674+00:00, queued_by_job_id=872, pid=15029[0m
[[34m2024-06-07T04:35:28.055+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.delete_csv_from_s3 manual__2024-06-07T04:34:27.981682+00:00 [scheduled]>[0m
[[34m2024-06-07T04:35:28.056+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:35:28.057+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.delete_csv_from_s3 manual__2024-06-07T04:34:27.981682+00:00 [scheduled]>[0m
[[34m2024-06-07T04:35:28.061+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='delete_csv_from_s3', run_id='manual__2024-06-07T04:34:27.981682+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-07T04:35:28.061+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'delete_csv_from_s3', 'manual__2024-06-07T04:34:27.981682+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:35:28.063+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'delete_csv_from_s3', 'manual__2024-06-07T04:34:27.981682+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:35:29.819+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:35:30.315+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.delete_csv_from_s3 manual__2024-06-07T04:34:27.981682+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:35:32.526+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='delete_csv_from_s3', run_id='manual__2024-06-07T04:34:27.981682+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:35:32.532+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=delete_csv_from_s3, run_id=manual__2024-06-07T04:34:27.981682+00:00, map_index=-1, run_start_date=2024-06-07 04:35:30.459305+00:00, run_end_date=2024-06-07 04:35:31.725234+00:00, run_duration=1.265929, state=success, executor_state=success, try_number=1, max_tries=3, job_id=880, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-06-07 04:35:28.058851+00:00, queued_by_job_id=872, pid=15095[0m
[[34m2024-06-07T04:35:35.795+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun extract_311_data_dag @ 2024-06-07 04:34:27.981682+00:00: manual__2024-06-07T04:34:27.981682+00:00, state:running, queued_at: 2024-06-07 04:34:28.023629+00:00. externally triggered: True> successful[0m
[[34m2024-06-07T04:35:35.796+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2024-06-07 04:34:27.981682+00:00, run_id=manual__2024-06-07T04:34:27.981682+00:00, run_start_date=2024-06-07 04:34:28.685257+00:00, run_end_date=2024-06-07 04:35:35.796281+00:00, run_duration=67.111024, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-06-06 00:00:00+00:00, data_interval_end=2024-06-07 00:00:00+00:00, dag_hash=19871c71c01ad779ef024e0b00f64370[0m
[[34m2024-06-07T04:39:40.985+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T04:39:40.988+0000[0m] {[34mscheduler_job_runner.py:[0m1618} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2024-06-07T04:42:33.698+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:42:33.698+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:42:33.698+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:42:33.701+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1) to executor with priority 9 and queue default[0m
[[34m2024-06-07T04:42:33.701+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:42:33.703+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:42:35.453+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:42:35.964+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data manual__2024-06-07T04:42:33.146453+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:42:44.616+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:42:44.621+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=manual__2024-06-07T04:42:33.146453+00:00, map_index=-1, run_start_date=2024-06-07 04:42:36.082435+00:00, run_end_date=2024-06-07 04:42:44.012098+00:00, run_duration=7.929663, state=success, executor_state=success, try_number=1, max_tries=3, job_id=882, pool=default_pool, queue=default, priority_weight=9, operator=PythonOperator, queued_dttm=2024-06-07 04:42:33.699946+00:00, queued_by_job_id=872, pid=15759[0m
[[34m2024-06-07T04:42:44.677+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:42:44.677+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:42:44.678+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:42:44.679+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-06-07T04:42:44.680+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:42:44.681+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:42:46.340+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:42:46.820+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-06-07T04:42:33.146453+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:42:50.134+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:42:50.140+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=manual__2024-06-07T04:42:33.146453+00:00, map_index=-1, run_start_date=2024-06-07 04:42:46.969878+00:00, run_end_date=2024-06-07 04:42:49.470169+00:00, run_duration=2.500291, state=success, executor_state=success, try_number=1, max_tries=3, job_id=883, pool=default_pool, queue=default, priority_weight=8, operator=BashOperator, queued_dttm=2024-06-07 04:42:44.678735+00:00, queued_by_job_id=872, pid=15780[0m
[[34m2024-06-07T04:42:50.208+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:42:50.208+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:42:50.209+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:42:50.212+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-06-07T04:42:50.212+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:42:50.214+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:42:52.026+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:42:52.533+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available manual__2024-06-07T04:42:33.146453+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:43:14.371+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:43:14.377+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=manual__2024-06-07T04:42:33.146453+00:00, map_index=-1, run_start_date=2024-06-07 04:42:52.695842+00:00, run_end_date=2024-06-07 04:43:13.607152+00:00, run_duration=20.91131, state=success, executor_state=success, try_number=1, max_tries=3, job_id=884, pool=default_pool, queue=default, priority_weight=7, operator=S3KeySensor, queued_dttm=2024-06-07 04:42:50.209946+00:00, queued_by_job_id=872, pid=15814[0m
[[34m2024-06-07T04:43:14.451+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.create_staging_table manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:14.452+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:43:14.452+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.create_staging_table manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:14.455+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='create_staging_table', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-06-07T04:43:14.456+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'create_staging_table', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:14.457+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'create_staging_table', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:16.096+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:43:16.510+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.create_staging_table manual__2024-06-07T04:42:33.146453+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:43:17.950+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='create_staging_table', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:43:17.958+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=create_staging_table, run_id=manual__2024-06-07T04:42:33.146453+00:00, map_index=-1, run_start_date=2024-06-07 04:43:16.629629+00:00, run_end_date=2024-06-07 04:43:17.187284+00:00, run_duration=0.557655, state=success, executor_state=success, try_number=1, max_tries=3, job_id=885, pool=default_pool, queue=default, priority_weight=6, operator=PostgresOperator, queued_dttm=2024-06-07 04:43:14.453651+00:00, queued_by_job_id=872, pid=15851[0m
[[34m2024-06-07T04:43:18.002+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.transfer_s3_to_redshift_staging manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:18.002+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:43:18.003+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.transfer_s3_to_redshift_staging manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:18.005+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='transfer_s3_to_redshift_staging', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2024-06-07T04:43:18.006+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'transfer_s3_to_redshift_staging', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:18.008+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'transfer_s3_to_redshift_staging', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:19.486+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:43:19.867+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.transfer_s3_to_redshift_staging manual__2024-06-07T04:42:33.146453+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:43:22.195+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='transfer_s3_to_redshift_staging', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:43:22.202+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=transfer_s3_to_redshift_staging, run_id=manual__2024-06-07T04:42:33.146453+00:00, map_index=-1, run_start_date=2024-06-07 04:43:19.977421+00:00, run_end_date=2024-06-07 04:43:21.444097+00:00, run_duration=1.466676, state=success, executor_state=success, try_number=1, max_tries=3, job_id=886, pool=default_pool, queue=default, priority_weight=5, operator=S3ToRedshiftOperator, queued_dttm=2024-06-07 04:43:18.004210+00:00, queued_by_job_id=872, pid=15872[0m
[[34m2024-06-07T04:43:22.264+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.delete_existing_records manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:22.265+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:43:22.265+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.delete_existing_records manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:22.268+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='delete_existing_records', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-06-07T04:43:22.269+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'delete_existing_records', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:22.271+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'delete_existing_records', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:24.188+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:43:24.745+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.delete_existing_records manual__2024-06-07T04:42:33.146453+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:43:26.194+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='delete_existing_records', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:43:26.199+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=delete_existing_records, run_id=manual__2024-06-07T04:42:33.146453+00:00, map_index=-1, run_start_date=2024-06-07 04:43:24.881772+00:00, run_end_date=2024-06-07 04:43:25.631910+00:00, run_duration=0.750138, state=success, executor_state=success, try_number=1, max_tries=3, job_id=887, pool=default_pool, queue=default, priority_weight=4, operator=PostgresOperator, queued_dttm=2024-06-07 04:43:22.267130+00:00, queued_by_job_id=872, pid=15895[0m
[[34m2024-06-07T04:43:26.241+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.insert_new_records manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:26.242+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:43:26.242+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.insert_new_records manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:26.244+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='insert_new_records', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-06-07T04:43:26.244+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'insert_new_records', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:26.245+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'insert_new_records', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:28.047+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:43:28.874+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.insert_new_records manual__2024-06-07T04:42:33.146453+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:43:30.532+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='insert_new_records', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:43:30.538+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=insert_new_records, run_id=manual__2024-06-07T04:42:33.146453+00:00, map_index=-1, run_start_date=2024-06-07 04:43:28.985005+00:00, run_end_date=2024-06-07 04:43:29.713417+00:00, run_duration=0.728412, state=success, executor_state=success, try_number=1, max_tries=3, job_id=888, pool=default_pool, queue=default, priority_weight=3, operator=PostgresOperator, queued_dttm=2024-06-07 04:43:26.242932+00:00, queued_by_job_id=872, pid=15924[0m
[[34m2024-06-07T04:43:30.607+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.truncate_staging_table manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:30.607+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:43:30.608+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.truncate_staging_table manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:30.610+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='truncate_staging_table', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-07T04:43:30.611+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'truncate_staging_table', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:30.613+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'truncate_staging_table', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:32.140+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:43:32.674+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.truncate_staging_table manual__2024-06-07T04:42:33.146453+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:43:34.050+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='truncate_staging_table', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:43:34.055+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=truncate_staging_table, run_id=manual__2024-06-07T04:42:33.146453+00:00, map_index=-1, run_start_date=2024-06-07 04:43:32.790621+00:00, run_end_date=2024-06-07 04:43:33.445238+00:00, run_duration=0.654617, state=success, executor_state=success, try_number=1, max_tries=3, job_id=889, pool=default_pool, queue=default, priority_weight=2, operator=PostgresOperator, queued_dttm=2024-06-07 04:43:30.609244+00:00, queued_by_job_id=872, pid=15946[0m
[[34m2024-06-07T04:43:34.093+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.delete_csv_from_s3 manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:34.094+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T04:43:34.094+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.delete_csv_from_s3 manual__2024-06-07T04:42:33.146453+00:00 [scheduled]>[0m
[[34m2024-06-07T04:43:34.096+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='delete_csv_from_s3', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-06-07T04:43:34.096+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'delete_csv_from_s3', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:34.099+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'delete_csv_from_s3', 'manual__2024-06-07T04:42:33.146453+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T04:43:35.446+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T04:43:35.937+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.delete_csv_from_s3 manual__2024-06-07T04:42:33.146453+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T04:43:37.898+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='delete_csv_from_s3', run_id='manual__2024-06-07T04:42:33.146453+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T04:43:37.902+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=delete_csv_from_s3, run_id=manual__2024-06-07T04:42:33.146453+00:00, map_index=-1, run_start_date=2024-06-07 04:43:36.077982+00:00, run_end_date=2024-06-07 04:43:37.267653+00:00, run_duration=1.189671, state=success, executor_state=success, try_number=1, max_tries=3, job_id=890, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-06-07 04:43:34.095026+00:00, queued_by_job_id=872, pid=15967[0m
[[34m2024-06-07T04:43:37.940+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun extract_311_data_dag @ 2024-06-07 04:42:33.146453+00:00: manual__2024-06-07T04:42:33.146453+00:00, state:running, queued_at: 2024-06-07 04:42:33.206401+00:00. externally triggered: True> successful[0m
[[34m2024-06-07T04:43:37.940+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2024-06-07 04:42:33.146453+00:00, run_id=manual__2024-06-07T04:42:33.146453+00:00, run_start_date=2024-06-07 04:42:33.668709+00:00, run_end_date=2024-06-07 04:43:37.940823+00:00, run_duration=64.272114, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-06-06 00:00:00+00:00, data_interval_end=2024-06-07 00:00:00+00:00, dag_hash=3e42518104a5ed61a6b38011b706df53[0m
[[34m2024-06-07T04:44:41.040+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T04:49:41.094+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T04:54:41.148+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T04:59:41.199+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:04:41.272+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:09:41.313+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:14:41.355+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:19:41.389+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:24:41.424+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:29:41.460+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:34:41.521+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:39:41.563+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:44:41.603+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:49:41.673+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:54:41.719+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T05:59:41.755+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:04:41.792+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:09:41.830+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:14:41.891+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:19:41.950+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:24:42.030+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:29:42.065+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:34:42.115+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:39:42.160+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:44:42.212+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:49:42.260+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:54:42.297+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T06:59:42.333+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:04:42.391+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:09:42.447+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:14:42.492+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:19:42.532+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:24:42.583+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:29:42.617+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:34:42.657+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:39:42.691+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:44:42.745+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:49:42.784+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:54:42.825+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T07:59:42.910+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:04:42.937+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:09:42.976+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:14:43.052+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:19:43.099+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:24:43.133+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:29:43.177+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:34:43.231+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:39:43.276+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:44:43.318+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:49:43.355+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:54:43.394+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T08:59:43.447+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:04:43.483+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:09:43.521+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:14:43.562+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:19:43.596+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:24:43.649+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:29:43.707+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:34:43.740+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:39:43.821+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:44:43.864+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:49:43.899+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:54:43.936+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T09:59:44.022+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:04:44.056+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:09:44.105+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:14:44.161+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:19:44.217+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:24:44.267+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:29:44.324+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:34:44.430+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:39:44.481+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:44:44.518+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:49:44.561+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:54:44.606+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T10:59:44.668+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:04:44.713+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:09:44.749+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:14:44.809+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:19:44.865+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:24:44.994+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:28:06.156+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.extract_311_data manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:28:06.156+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T11:28:06.156+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.extract_311_data manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:28:06.158+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1) to executor with priority 9 and queue default[0m
[[34m2024-06-07T11:28:06.159+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:28:06.160+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'extract_311_data', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:28:07.816+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T11:28:08.191+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.extract_311_data manual__2024-06-07T11:28:06.094709+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T11:28:16.408+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='extract_311_data', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T11:28:16.412+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=extract_311_data, run_id=manual__2024-06-07T11:28:06.094709+00:00, map_index=-1, run_start_date=2024-06-07 11:28:08.292112+00:00, run_end_date=2024-06-07 11:28:15.767936+00:00, run_duration=7.475824, state=success, executor_state=success, try_number=1, max_tries=3, job_id=891, pool=default_pool, queue=default, priority_weight=9, operator=PythonOperator, queued_dttm=2024-06-07 11:28:06.157572+00:00, queued_by_job_id=872, pid=29539[0m
[[34m2024-06-07T11:28:16.465+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:28:16.465+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T11:28:16.465+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:28:16.467+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1) to executor with priority 8 and queue default[0m
[[34m2024-06-07T11:28:16.467+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:28:16.469+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'load_to_s3', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:28:18.299+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T11:28:18.762+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.load_to_s3 manual__2024-06-07T11:28:06.094709+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T11:28:22.593+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='load_to_s3', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T11:28:22.603+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=load_to_s3, run_id=manual__2024-06-07T11:28:06.094709+00:00, map_index=-1, run_start_date=2024-06-07 11:28:18.862549+00:00, run_end_date=2024-06-07 11:28:21.256770+00:00, run_duration=2.394221, state=success, executor_state=success, try_number=1, max_tries=3, job_id=892, pool=default_pool, queue=default, priority_weight=8, operator=BashOperator, queued_dttm=2024-06-07 11:28:16.466375+00:00, queued_by_job_id=872, pid=29561[0m
[[34m2024-06-07T11:28:22.676+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:28:22.676+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T11:28:22.676+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.is_file_in_s3_available manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:28:22.679+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default[0m
[[34m2024-06-07T11:28:22.680+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:28:22.681+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'is_file_in_s3_available', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:28:24.161+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T11:28:24.672+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.is_file_in_s3_available manual__2024-06-07T11:28:06.094709+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T11:28:46.629+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='is_file_in_s3_available', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T11:28:46.639+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=is_file_in_s3_available, run_id=manual__2024-06-07T11:28:06.094709+00:00, map_index=-1, run_start_date=2024-06-07 11:28:24.817109+00:00, run_end_date=2024-06-07 11:28:45.900192+00:00, run_duration=21.083083, state=success, executor_state=success, try_number=1, max_tries=3, job_id=893, pool=default_pool, queue=default, priority_weight=7, operator=S3KeySensor, queued_dttm=2024-06-07 11:28:22.677597+00:00, queued_by_job_id=872, pid=29602[0m
[[34m2024-06-07T11:28:46.701+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.create_staging_table manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:28:46.701+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T11:28:46.701+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.create_staging_table manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:28:46.704+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='create_staging_table', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
[[34m2024-06-07T11:28:46.705+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'create_staging_table', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:28:46.706+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'create_staging_table', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:28:48.221+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T11:28:48.891+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.create_staging_table manual__2024-06-07T11:28:06.094709+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T11:28:50.808+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='create_staging_table', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T11:28:50.814+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=create_staging_table, run_id=manual__2024-06-07T11:28:06.094709+00:00, map_index=-1, run_start_date=2024-06-07 11:28:49.073505+00:00, run_end_date=2024-06-07 11:28:50.037680+00:00, run_duration=0.964175, state=success, executor_state=success, try_number=1, max_tries=3, job_id=894, pool=default_pool, queue=default, priority_weight=6, operator=PostgresOperator, queued_dttm=2024-06-07 11:28:46.702798+00:00, queued_by_job_id=872, pid=29633[0m
[[34m2024-06-07T11:28:50.865+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.transfer_s3_to_redshift_staging manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:28:50.865+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T11:28:50.865+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.transfer_s3_to_redshift_staging manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:28:50.868+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='transfer_s3_to_redshift_staging', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2024-06-07T11:28:50.868+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'transfer_s3_to_redshift_staging', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:28:50.870+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'transfer_s3_to_redshift_staging', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:28:53.150+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T11:28:53.612+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.transfer_s3_to_redshift_staging manual__2024-06-07T11:28:06.094709+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T11:29:24.187+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='transfer_s3_to_redshift_staging', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T11:29:24.192+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=transfer_s3_to_redshift_staging, run_id=manual__2024-06-07T11:28:06.094709+00:00, map_index=-1, run_start_date=2024-06-07 11:28:53.799056+00:00, run_end_date=2024-06-07 11:29:23.600532+00:00, run_duration=29.801476, state=success, executor_state=success, try_number=1, max_tries=3, job_id=895, pool=default_pool, queue=default, priority_weight=5, operator=S3ToRedshiftOperator, queued_dttm=2024-06-07 11:28:50.866796+00:00, queued_by_job_id=872, pid=29663[0m
[[34m2024-06-07T11:29:24.269+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.delete_existing_records manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:29:24.270+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T11:29:24.270+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.delete_existing_records manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:29:24.272+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='delete_existing_records', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-06-07T11:29:24.273+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'delete_existing_records', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:29:24.274+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'delete_existing_records', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:29:25.929+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T11:29:26.351+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.delete_existing_records manual__2024-06-07T11:28:06.094709+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T11:29:37.113+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='delete_existing_records', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T11:29:37.126+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=delete_existing_records, run_id=manual__2024-06-07T11:28:06.094709+00:00, map_index=-1, run_start_date=2024-06-07 11:29:26.460973+00:00, run_end_date=2024-06-07 11:29:35.984018+00:00, run_duration=9.523045, state=success, executor_state=success, try_number=1, max_tries=3, job_id=896, pool=default_pool, queue=default, priority_weight=4, operator=PostgresOperator, queued_dttm=2024-06-07 11:29:24.271291+00:00, queued_by_job_id=872, pid=29705[0m
[[34m2024-06-07T11:29:37.229+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.insert_new_records manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:29:37.229+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T11:29:37.230+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.insert_new_records manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:29:37.231+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='insert_new_records', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-06-07T11:29:37.232+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'insert_new_records', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:29:37.233+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'insert_new_records', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:29:38.634+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T11:29:39.112+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.insert_new_records manual__2024-06-07T11:28:06.094709+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T11:29:49.793+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='insert_new_records', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T11:29:49.812+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=insert_new_records, run_id=manual__2024-06-07T11:28:06.094709+00:00, map_index=-1, run_start_date=2024-06-07 11:29:39.269383+00:00, run_end_date=2024-06-07 11:29:49.111125+00:00, run_duration=9.841742, state=success, executor_state=success, try_number=1, max_tries=3, job_id=897, pool=default_pool, queue=default, priority_weight=3, operator=PostgresOperator, queued_dttm=2024-06-07 11:29:37.230604+00:00, queued_by_job_id=872, pid=29729[0m
[[34m2024-06-07T11:29:49.834+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:29:49.871+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.truncate_staging_table manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:29:49.871+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T11:29:49.872+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.truncate_staging_table manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:29:49.874+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='truncate_staging_table', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-07T11:29:49.874+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'truncate_staging_table', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:29:49.875+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'truncate_staging_table', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:29:51.311+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T11:29:51.798+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.truncate_staging_table manual__2024-06-07T11:28:06.094709+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T11:29:53.963+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='truncate_staging_table', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T11:29:53.971+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=truncate_staging_table, run_id=manual__2024-06-07T11:28:06.094709+00:00, map_index=-1, run_start_date=2024-06-07 11:29:51.916963+00:00, run_end_date=2024-06-07 11:29:52.811547+00:00, run_duration=0.894584, state=success, executor_state=success, try_number=1, max_tries=3, job_id=898, pool=default_pool, queue=default, priority_weight=2, operator=PostgresOperator, queued_dttm=2024-06-07 11:29:49.872806+00:00, queued_by_job_id=872, pid=29755[0m
[[34m2024-06-07T11:29:54.133+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_311_data_dag.delete_csv_from_s3 manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:29:54.133+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_311_data_dag has 0/16 running and queued tasks[0m
[[34m2024-06-07T11:29:54.133+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_311_data_dag.delete_csv_from_s3 manual__2024-06-07T11:28:06.094709+00:00 [scheduled]>[0m
[[34m2024-06-07T11:29:54.138+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_311_data_dag', task_id='delete_csv_from_s3', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-06-07T11:29:54.138+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'delete_csv_from_s3', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:29:54.142+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_311_data_dag', 'delete_csv_from_s3', 'manual__2024-06-07T11:28:06.094709+00:00', '--local', '--subdir', 'DAGS_FOLDER/311-analytics-dag.py'][0m
[[34m2024-06-07T11:29:55.547+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/ubuntu/***/dags/311-analytics-dag.py[0m
[[34m2024-06-07T11:29:55.980+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_311_data_dag.delete_csv_from_s3 manual__2024-06-07T11:28:06.094709+00:00 [queued]> on host ip-172-31-26-59.eu-north-1.compute.internal[0m
[[34m2024-06-07T11:29:57.894+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_311_data_dag', task_id='delete_csv_from_s3', run_id='manual__2024-06-07T11:28:06.094709+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-07T11:29:57.899+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_311_data_dag, task_id=delete_csv_from_s3, run_id=manual__2024-06-07T11:28:06.094709+00:00, map_index=-1, run_start_date=2024-06-07 11:29:56.098286+00:00, run_end_date=2024-06-07 11:29:57.276100+00:00, run_duration=1.177814, state=success, executor_state=success, try_number=1, max_tries=3, job_id=899, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-06-07 11:29:54.136257+00:00, queued_by_job_id=872, pid=29784[0m
[[34m2024-06-07T11:29:57.940+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun extract_311_data_dag @ 2024-06-07 11:28:06.094709+00:00: manual__2024-06-07T11:28:06.094709+00:00, state:running, queued_at: 2024-06-07 11:28:06.107584+00:00. externally triggered: True> successful[0m
[[34m2024-06-07T11:29:57.940+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=extract_311_data_dag, execution_date=2024-06-07 11:28:06.094709+00:00, run_id=manual__2024-06-07T11:28:06.094709+00:00, run_start_date=2024-06-07 11:28:06.132013+00:00, run_end_date=2024-06-07 11:29:57.940923+00:00, run_duration=111.80891, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-06-06 00:00:00+00:00, data_interval_end=2024-06-07 00:00:00+00:00, dag_hash=693e109453d2ac1236e5d6b0047a176d[0m
[[34m2024-06-07T11:34:49.853+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:39:49.897+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:44:49.946+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:49:49.970+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:54:50.020+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T11:59:50.075+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-06-07T12:04:50.136+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
